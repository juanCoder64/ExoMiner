# Preprocessing data pipeline

### Goals

The code in `src_preprocessing` is used for:

1. Preprocessing the light curve data generated by the SPOC pipeline to create TFRecord datasets of transit signal
   observations (e.g., TCE and Objects of Interest datasets).
2. Postprocessing the data preprocessed in step 1) (e.g., split data into training, validation and test sets,
   normalize features using statistics computed from the training set, select subsets of data from the original
   dataset).

### Data Format

This project uses TFRecord format to save the processed data to be ingested by the models. This format stores a sequence
of binary records. Each TFRecord file has a set of examples (data points), each one with a given set of features. Check
[TFRecord tutorial](https://www.tensorflow.org/tutorials/load_data/tfrecord) for more information.

### Data preprocessing

The main scripts involved in preprocessing the data are `src_preprocessing.generate_input_records.py`
and `src_preprocessing.preprocessing.py`.\
The script `src_preprocessing.generate_input_records.py` is designed to preprocess batches of data in parallel using the
MPI framework or Python `multiprocessin.Pool` module, i.e., given a table of transit signals to be preprocessed, the
code splits the table into subsets, preprocessing each batch simultaneously.

Snippet of code to run the preprocessing pipeline with MPI (e.g. on a cluster):

```shell
mpiexec -n $num_batches python src_preprocessing/generate_input_records.py  &>  log_preprocessing_py.txt
```

#### Preprocessing steps

The main preprocessing steps are (check `src_preprocessing.preprocess.py`):

1. Get data from light curve FITS files for target stars of interest.
   1. Timestamps array: timestamps associated to the recorded cadences (a cadence is a single observation, data point).
   2. PDC-SAP flux time series (informally called 'flux'): consists of accumulated brightness for the pixels in the
      optimal aperture for the given target star.
   3. FW (flux-weighted) centroid time series: consists of weighted average location in celestial coordinates (RA and
      Dec) of the of the pixels in the optimal aperture (brightness values are used as weights).
3. Remove stellar variability noise by fitting a spline to the timeseries, since this phenomenon occurs usually at a
   longer time scale than the transits.
4. Phase fold the timeseries over the orbital period for the transit signal of interest.
5. Create a binned version (usually called 'view') of the phase folded timeseries by averaging cadences in each bin.

#### Features

The features set in the TFRecords come from two sources:

1. The different views created in `src_preprocessing.preprocess.generate_example_for_tce()`.
2. Scalar features added to the TFRecords also in `src_preprocessing.preprocess.generate_example_for_tce()` that come
   from the transit signal table used as input (e.g. transit depth, weak secondary MES, other statistics and diagnostics
   computed in the DV module).

### Data postprocessing

The following processes are modular and their application to the preprocessed data depends on the user's goals, but
usually the postprocessing pipeline consists of: 1) splitting the data into training, validation, test and/or prediction
(used for inference) sets; 2) computing normalization statistics based on the training set; use those statistics to
normalize input features in the datasets. Other steps such as updating labels based on different sets of dispositions or
selecting subsets of the data are also common tasks.

#### Splitting data into different datasets

This can be performed in `src_preprocessing.create_new_tfrecords.py`. The source TFRecords are split into new TFRecords
based on dataset tables (e.g., 'train-shard-xxxx-of-xxxx').

#### Updating labels

This can be performed in `src_preprocessing.update_labels_tfrecords.py`

#### Computing normalization statistics

This can be performed in `src_preprocessing.compute_normalization_stats_tfrecords.py` using as configuration file
`src_preprocessing.config_compute_normalization_stats.yaml`.

#### Normalizing datasets

Normalize multiple features based on statistics from a training set. Depending on the goal, this postprocessing step can
include:

- Impute missing values with median of training set or fixed value.
- Truncate values (clip) to a given range (e.g., median +- MAD std * clip_factor).
- Log-transform the data.

Normalization can be performed in `src_preprocessing.normalize_data_tfrecords.py` using as configuration file
`src_preprocessing.config_normalize_data.yaml`.