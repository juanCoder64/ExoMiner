import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
import os
from src_preprocessing.tf_util import example_util
from astropy.stats import mad_std

#%%

# # plot boxplots of centroid local and global views in the training set
# print('Generating boxplots...')
# f, ax = plt.subplots()
# ax.boxplot([centroidMat['global_centr_view'], centroidMat['local_centr_view']], bootstrap=None, meanline=True,
#            showmeans=True)
# ax.set_title('Non-normalized centroid time series')
# ax.set_yscale('log')
# ax.set_ylabel('Value')
# ax.set_xticklabels(['Global view', 'Local view'])
# f.savefig('/home/msaragoc/Downloads/hist_nonnormalized_centroids.png')
# # plt.show()

#%% Checking values for centroid views

# print(len(np.where(centroidMat['global_centr_view'], np.percentile(centroidMat['global_centr_view'], 75))))

badTces = []
thr = 1000  # 5844.65576171875  # 99.9 percentile

label = 'AFP'
# centroidDict = {'global_centr_view': [], 'local_centr_view': []}

for tfrec_i, tfrecFile in enumerate(tfrecTrainFiles):

    print('Getting data from {} ({} %)'.format(tfrecFile.split('/')[-1], tfrec_i / len(tfrecTrainFiles) * 100))

    # iterate through the shard
    record_iterator = tf.python_io.tf_record_iterator(path=tfrecFile)

    for string_i, string_record in enumerate(record_iterator):

        example = tf.train.Example()
        example.ParseFromString(string_record)

        tceLabel = example.features.feature['label'].bytes_list.value[0].decode("utf-8")

        if tceLabel != label:
            continue

        tceIdentifierTfrec = example.features.feature[tceIdentifier].int64_list.value[0]
        targetIdTfrec = example.features.feature['target_id'].int64_list.value[0]

        # get centroid time series data

        timeSeriesTce = np.array(example.features.feature['global_centr_view'].float_list.value)
        # centroidDict['global_centr_view'].append(np.min(timeSeriesTce))

        if np.any(np.array(timeSeriesTce) > thr):
            badTces.append((targetIdTfrec, tceIdentifierTfrec))

        # timeSeriesTce = np.array(example.features.feature['local_centr_view'].float_list.value)
        # centroidDict['local_centr_view'].append(np.min(timeSeriesTce))

print(len(badTces))
print(badTces)

# numOutliers = {}
# for timeSeries in centroidDict:
#     q75 = np.percentile(centroidDict[timeSeries], 75)
#     q25 = np.percentile(centroidDict[timeSeries], 25)
#
#     iqr = q75 - q25
#
#     numOutliers[timeSeries] = len(np.where(centroidDict[timeSeries] > 1.5 * iqr)[0])
#
# # plot boxplots of centroid local and global views in the training set
# print('Generating boxplots...')
# f, ax = plt.subplots()
# ax.boxplot([centroidDict['global_centr_view'], centroidDict['local_centr_view']], bootstrap=None, meanline=True,
#            showmeans=True)
# ax.set_title('Non-normalized centroid time series\n{} Num outliers = {}'.format(label, numOutliers[timeSeries]))
# ax.set_yscale('log')
# ax.set_ylabel('Median value')
# ax.set_xticklabels(['Global view', 'Local view'])
# f.savefig('/home/msaragoc/Downloads/hist_nonnormalized_centroids_{}.png'.format(label))
# plt.show()

#%% Plot minimum value for odd and even local views for a given dataset

# rankingTbl = pd.read_csv('/home/msaragoc/Projects/Kepler-TESS_exoplanet/Kepler_planet_finder/results_ensemble/'
#                          'keplerdr25_g2001-l201_spline_gapped_glfluxbohb_norobovetterkois_starshuffle_glflux-loe/'
#                          'ensemble_ranked_predictions_testset')
tceTbl = pd.read_csv('/data5/tess_project/Data/Ephemeris_tables/Kepler/Q1-Q17_DR25/'
                     'q1_q17_dr25_tce_2020.04.15_23.19.10_cumkoi_2020.02.21_shuffledstar_noroguetces_noRobobvetterKOIs.csv')

tceTbl['odd_min_val'] = np.nan
tceTbl['even_min_val'] = np.nan

for targetId, tceId in oddevenMinTce:
    tceTbl.loc[(tceTbl['target_id'] == targetId) &
               (tceTbl['tce_plnt_num'] == tceId), ['odd_min_val', 'even_min_val']] = oddevenMinTce[targetId, tceId]

dispositions = ['PC', 'AFP', 'NTP']
plotTbls = {disposition: None for disposition in dispositions}
for disposition in dispositions:
    plotTbls[disposition] = tceTbl.loc[tceTbl['label'] == disposition]
    # plotTbls[disposition] = rankingTbl.loc[tceTbl['original_label'] == disposition]

f, ax = plt.subplots()
# ax.scatter(plotTbls['PC']['odd_min_val'].values, plotTbls['PC']['even_min_val'].values, label='PC', s=15, c='g')
# ax.scatter(plotTbls['AFP']['odd_min_val'].values, plotTbls['AFP']['even_min_val'].values, label='AFP', s=15, c='y')
ax.scatter(plotTbls['NTP']['odd_min_val'].values, plotTbls['NTP']['even_min_val'].values, label='NTP', s=15, c='r')
ax.set_ylabel('Even view min value')
ax.set_xlabel('Odd view min value')
ax.set_title('Q1-Q17 DR25 \nNon-rogue TCEs and no Robovetter KOIs dataset')
ax.legend()
# ax.set_yscale('log')
# ax.set_xscale('log')

#%% Plot oot-std value for local weak secondary flux view for a given dataset

rankingTbl = pd.read_csv('/home/msaragoc/Projects/Kepler-TESS_exoplanet/Kepler_planet_finder/results_ensemble/'
                         'keplerdr25_g2001-l201_spline_gapped_glfluxbohb_norobovetterkois_starshuffle_glflux-loe/'
                         'ensemble_ranked_predictions_testset')
# tceTbl = pd.read_csv('/data5/tess_project/Data/Ephemeris_tables/Kepler/Q1-Q17_DR25/'
#                      'q1_q17_dr25_tce_2020.04.15_23.19.10_cumkoi_2020.02.21_shuffledstar_noroguetces_noRobobvetterKOIs.csv')

rankingTbl['lwks_oot-std'] = np.nan

for targetId, tceId in oddevenMinTce:
    rankingTbl.loc[(rankingTbl['target_id'] == targetId) &
                   (rankingTbl['tce_plnt_num'] == tceId), 'lwks_oot'] = lwksViewStdTce[targetId, tceId]

dispositions = ['PC', 'AFP', 'NTP']
plotTbls = {disposition: None for disposition in dispositions}
for disposition in dispositions:
    # plotTbls[disposition] = tceTbl.loc[tceTbl['label'] == disposition]
    plotTbls[disposition] = rankingTbl.loc[rankingTbl['original_label'] == disposition]

f, ax = plt.subplots()
ax.scatter(plotTbls['PC']['lwks_oot'].values, plotTbls['PC']['score'].values, label='PC', s=15, c='g')
ax.scatter(plotTbls['AFP']['lwks_oot'].values, plotTbls['AFP']['score'].values, label='AFP', s=15, c='y')
ax.scatter(plotTbls['NTP']['lwks_oot'].values, plotTbls['NTP']['score'].values, label='NTP', s=15, c='r')
ax.set_ylabel('Score')
ax.set_xlabel('Local weak secondary oot-std')
ax.set_title('Test set')
ax.legend()
# ax.set_xscale('log')
ax.set_ylim([0, 1])
# ax.set_xlim([0, 1])

#%% Iterate over TFRecords to see where we have a Data Loss Error

tfrecDir = '/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/tfrecordskeplerdr25_g2001-l201_spline_nongapped_flux-centroid_selfnormalized-oddeven-wks-scalar_tps_globalbinwidthaslocal_data/tfrecordskeplerdr25_tps_g2001-l201_gbal_spline_nongapped_flux-centroid-oddeven-wks-scalar_starshuffle_experiment-labels-norm'

tfrecFiles = [os.path.join(tfrecDir, file) for file in os.listdir(tfrecDir) if 'shard' in file]

for tfrecFile in tfrecFiles:

    record_iterator = tf.python_io.tf_record_iterator(path=tfrecFile)

    for string_i, string_record in enumerate(record_iterator):

        a = 1

#%% Correcting local centr medcmaxn_dir by dividing by max(abs) since it was dividied by max

tceIdentifier = 'tce_plnt_num'
srcTfrecDir = '/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/tfrecordskeplerdr25_nontces_g2001-l201_gbal_spline_nongapped_flux-centroid-oddeven-wks-scalar_data/tfrecordskeplerdr25_nontces_g2001-l201_gbal_spline_nongapped_flux-centroid-oddeven-wks-scalar'
srcTfrecFiles = [os.path.join(srcTfrecDir, file) for file in os.listdir(srcTfrecDir) if 'shard' in file]
srcTfrecFiles = [file for file in srcTfrecFiles if not file.endswith('.csv')]
destTfrecDir = '/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/tfrecordskeplerdr25_nontces_g2001-l201_gbal_spline_nongapped_flux-centroid-oddeven-wks-scalar_data/tfrecordskeplerdr25_nontces_g2001-l201_gbal_spline_nongapped_flux-centroid-oddeven-wks-scalar_correct'
os.makedirs(destTfrecDir, exist_ok=True)

for srcTfrecFile in srcTfrecFiles:

    with tf.python_io.TFRecordWriter(os.path.join(destTfrecDir, srcTfrecFile.split('/')[-1])) as writer:

        record_iterator = tf.python_io.tf_record_iterator(path=srcTfrecFile)

        for string_i, string_record in enumerate(record_iterator):

            example = tf.train.Example()
            example.ParseFromString(string_record)

            loc_centr = np.array(example.features.feature['local_centr_view'].float_list.value)

            loc_centr -= np.median(loc_centr)
            loc_centr_medc_maxabsn = loc_centr / np.max(np.abs(loc_centr))
            example_util.set_float_feature(example, 'local_centr_view_medcmaxn_dir', loc_centr_medc_maxabsn,
                                           allow_overwrite=True)

            writer.write(example.SerializeToString())

#%% Adding the normalized rolling band diagnostic to the TFRecords

tceIdentifier = 'tce_plnt_num'
srcTfrecDir = '/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/tfrecordskeplerdr25_g2001-l201_spline_nongapped_flux-centroid_selfnormalized-oddeven-wks-scalar_globalbinwidthaslocal_data/tfrecordskeplerdr25_g2001-l201_spline_nongapped_flux-centroid_selfnormalized-oddeven-wks-scalar_globalbinwidthaslocal_starshuffle_experiment-labels-norm_rollingband_newcentrnorm_ghostclip'
srcTfrecFiles = [os.path.join(srcTfrecDir, file) for file in os.listdir(srcTfrecDir) if 'shard' in file]
destTfrecDir = '/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/tfrecordskeplerdr25_g2001-l201_spline_nongapped_flux-centroid_selfnormalized-oddeven-wks-scalar_globalbinwidthaslocal_data/tfrecordskeplerdr25_tce1_g2001-l201_spline_nongapped_flux-centroid_selfnormalized-oddeven-wks-scalar_globalbinwidthaslocal_starshuffle_experiment-labels-norm_rollingband_newcentrnorm_ghostclip'
os.makedirs(destTfrecDir, exist_ok=True)

# training set TCE table
trainTceTbl = pd.read_csv('/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/tfrecordskeplerdr25_g2001-l201_spline_gapped_flux-centroid_selfnormalized-oddeven-wks-scalar_data/'
                          'tfrecordskeplerdr25_g2001-l201_spline_gapped_flux-centroid_selfnormalized-oddeven-wks-scalar_starshuffle_experiment/trainset.csv')

# TCE table
tceTbl = pd.read_csv('/data5/tess_project/Data/Ephemeris_tables/Kepler/Q1-Q17_DR25/'
                     'q1_q17_dr25_tce_2020.04.15_23.19.10_cumkoi_2020.02.21_shuffledstar_noroguetces_noRobovetterKOIs.csv')

# compute training set statistics for the rolling band diagnostic
tce_rb_tcount0Arr = np.zeros(len(trainTceTbl), dtype='float')
for tce_i, tce in trainTceTbl.iterrows():
    tce_rb_tcount0Arr[tce_i] = tceTbl.loc[(tceTbl['target_id'] == tce['target_id']) &
                                          (tceTbl[tceIdentifier] == tce[tceIdentifier])]['tce_rb_tcount0']
normStats = {
    'tce_rb_tcount0': {'median': np.median(tce_rb_tcount0Arr),
                       'std': stats.mad_std(tce_rb_tcount0Arr)}
}

# normalization stats for ghost diagnostic
normStatsDir = '/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/tfrecordskeplerdr25_g2001-l201_spline_nongapped_flux-centroid_selfnormalized-oddeven-wks-scalar_globalbinwidthaslocal_data/tfrecordskeplerdr25_g2001-l201_spline_nongapped_flux-centroid_selfnormalized-oddeven-wks-scalar_globalbinwidthaslocal_starshuffle_experiment/'
# scalar parameters normalization statistics
normStatsScalar = np.load(os.path.join(normStatsDir, 'train_scalarparam_norm_stats.npy'), allow_pickle=True).item()

for srcTfrecFile in srcTfrecFiles:

    with tf.python_io.TFRecordWriter(os.path.join(destTfrecDir, srcTfrecFile.split('/')[-1])) as writer:

        record_iterator = tf.python_io.tf_record_iterator(path=srcTfrecFile)

        for string_i, string_record in enumerate(record_iterator):

            example = tf.train.Example()
            example.ParseFromString(string_record)

            # get rolling band diagnostic from TCE table for this TCE; normalize it using training set stats;
            # add to the example
            tceIdentifierTfrec = example.features.feature[tceIdentifier].int64_list.value[0]
            targetIdTfrec = example.features.feature['target_id'].int64_list.value[0]

            if tceIdentifierTfrec != 1:
                continue

            # tce_rb_tcount0Tfrec = tceTbl.loc[(tceTbl['target_id'] == targetIdTfrec) &
            #                                  (tceTbl[tceIdentifier] == tceIdentifierTfrec)]['tce_rb_tcount0']
            #
            # assert len(tce_rb_tcount0Tfrec) > 0
            #
            # tce_rb_tcount0Tfrec = (tce_rb_tcount0Tfrec - normStats['tce_rb_tcount0']['median']) / \
            #                       normStats['tce_rb_tcount0']['std']
            #
            # tceScalarParams = np.array(example.features.feature['scalar_params'].float_list.value)
            #
            # tceScalarParams = np.append(tceScalarParams, tce_rb_tcount0Tfrec.values[0])
            #
            # # clip ghost diagnostic values to 20 * sigma
            # # cap
            # tceScalarParams[-3] = np.clip(tceScalarParams[-3],
            #                               -20 * normStatsScalar['std'][-2],
            #                               20 * normStatsScalar['std'][-2])
            # # hap
            # tceScalarParams[-2] = np.clip(tceScalarParams[-2],
            #                               -20 * normStatsScalar['std'][-1],
            #                               20 * normStatsScalar['std'][-1])
            #
            # example_util.set_float_feature(example, 'scalar_params', tceScalarParams, allow_overwrite=True)
            #
            # # create new global and local centroid views
            # # 1) absolute and then max normalization
            # # 2) max(abs) normalization
            # glob_centr_medcmaxn = np.array(example.features.feature['global_centr_view_medcmaxn'].float_list.value)
            # loc_centr_medcmaxn = np.array(example.features.feature['local_centr_view_medcmaxn'].float_list.value)
            #
            # # 1)
            # glob_centr_medc_abs_maxn = np.abs(glob_centr_medcmaxn) / np.max(np.abs(glob_centr_medcmaxn))
            # example_util.set_float_feature(example, 'global_centr_view_medcmaxn', glob_centr_medc_abs_maxn,
            #                                allow_overwrite=True)
            # loc_centr_medc_abs_maxn = np.abs(loc_centr_medcmaxn) / np.max(np.abs(loc_centr_medcmaxn))
            # example_util.set_float_feature(example, 'local_centr_view_medcmaxn', loc_centr_medc_abs_maxn,
            #                                allow_overwrite=True)
            #
            # # 2)
            # glob_centr_medc_maxabsn = glob_centr_medcmaxn / np.max(np.abs(glob_centr_medcmaxn))
            # example_util.set_float_feature(example, 'global_centr_view_medcmaxn_dir', glob_centr_medc_maxabsn)
            # loc_centr_medc_maxabsn = loc_centr_medcmaxn / np.max(np.abs(loc_centr_medcmaxn))
            # example_util.set_float_feature(example, 'local_centr_view_medcmaxn_dir', loc_centr_medc_maxabsn)

            writer.write(example.SerializeToString())


#%% Correcting name for local flux odd-even diff dir

tceIdentifier = 'tce_plnt_num'
srcTfrecDir = '/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/tfrecordskeplerdr25_g2001-l201_gbal_spline_nongapped_flux-centroid-oddeven-wks-6stellar-ghost-bfap-rollingband_data/tfrecordskeplerdr25_g2001-l201_gbal_spline_nongapped_flux-centroid-oddeven-wks-6stellar-ghost-bfap-rollingband_starshuffle_experiment-labels-norm'
srcTfrecFiles = [os.path.join(srcTfrecDir, file) for file in os.listdir(srcTfrecDir) if 'shard' in file]
srcTfrecFiles = [file for file in srcTfrecFiles if not file.endswith('.csv')]
destTfrecDir = srcTfrecDir + '_correct'
os.makedirs(destTfrecDir, exist_ok=True)

for srcTfrecFile in srcTfrecFiles:

    with tf.python_io.TFRecordWriter(os.path.join(destTfrecDir, srcTfrecFile.split('/')[-1])) as writer:

        record_iterator = tf.python_io.tf_record_iterator(path=srcTfrecFile)

        for string_i, string_record in enumerate(record_iterator):

            example = tf.train.Example()
            example.ParseFromString(string_record)

            loc_oe_diff_dir = np.array(example.features.feature['local_flux_even_view_diff_dir'].float_list.value)

            example_util.set_float_feature(example, 'local_flux_oddeven_view_diff_dir', loc_oe_diff_dir)

            writer.write(example.SerializeToString())

#%% Getting only TCEs with tce_plnt_num = 1

tceIdentifier = 'tce_plnt_num'
srcTfrecDir = '/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/tfrecordskeplerdr25_g2001-l201_spline_nongapped_flux-centroid-oddeven-wks-6stellar-bfap-ghost-rollingband_data/tfrecordskeplerdr25-dv_g2001-l201_spline_nongapped_flux-centroid-oddeven-wks-6stellar-bfap-ghost-rollingband_starshuffle_experiment-labels-norm'
srcTfrecFiles = [os.path.join(srcTfrecDir, file) for file in os.listdir(srcTfrecDir) if 'shard' in file]
srcTfrecFiles = [file for file in srcTfrecFiles if not file.endswith('.csv')]
destTfrecDir = '/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/tfrecordskeplerdr25_g2001-l201_spline_nongapped_flux-centroid-oddeven-wks-6stellar-bfap-ghost-rollingband_data/tfrecordskeplerdr25-dv_g2001-l201_spline_nongapped_flux-centroid-oddeven-wks-6stellar-bfap-ghost-rollingband_starshuffle_experiment-labels-norm_tce1'
os.makedirs(destTfrecDir, exist_ok=True)

for srcTfrecFile in srcTfrecFiles:

    with tf.io.TFRecordWriter(os.path.join(destTfrecDir, srcTfrecFile.split('/')[-1])) as writer:

        # record_iterator = tf.python_io.tf_record_iterator(path=srcTfrecFile)
        tfrecord_dataset = tf.data.TFRecordDataset(srcTfrecFile)

        for string_i, string_record in enumerate(tfrecord_dataset.as_numpy_iterator()):
        # for string_i, string_record in enumerate(record_iterator):

            example = tf.train.Example()
            example.ParseFromString(string_record)

            tceIdentifierTfrec = example.features.feature[tceIdentifier].int64_list.value[0]

            if tceIdentifierTfrec != 1:
                continue

            writer.write(example.SerializeToString())

#%% Remove Possible Planets from dataset

tceTbl = pd.read_csv('/data5/tess_project/Data/Ephemeris_tables/Kepler/Q1-Q17_DR25/'
                     'q1_q17_dr25_tce_2020.09.28_10.36.22_stellar_koi_cfp_norobovetterlabels_renamedcols_nomissingval_'
                     'rmcandandfpkois_norogues.csv')

tceIdentifier = 'tce_plnt_num'
srcTfrecDir = '/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/tfrecordskeplerdr25-dv_g301-l31_6tr_spline_nongapped_flux-loe-lwks-centroid-centroid_fdl-6stellar-bfap-ghost-rollingband-stdtimeseries_secsymphase_wksnorm_maxflux-wks_correctprimarygapping_data/tfrecordskeplerdr25-dv_g301-l31_6tr_spline_nongapped_flux-loe-lwks-centroid-centroid_fdl-6stellar-bfap-ghost-rollingband-stdtimeseries_secsymphase_wksnorm_maxflux-wks_correctprimarygapping_starshuffle_experiment-labels-norm'
srcTfrecFiles = [os.path.join(srcTfrecDir, file) for file in os.listdir(srcTfrecDir) if 'shard' in file]
srcTfrecFiles = [file for file in srcTfrecFiles if not file.endswith('.csv')]
destTfrecDir = srcTfrecDir + '_nopps'
os.makedirs(destTfrecDir, exist_ok=True)

tcesRemoved = {'train': 0, 'val': 0, 'test': 0}
for srcTfrecFile in srcTfrecFiles:

    with tf.io.TFRecordWriter(os.path.join(destTfrecDir, srcTfrecFile.split('/')[-1])) as writer:

        tfrecord_dataset = tf.data.TFRecordDataset(srcTfrecFile)

        for string_i, string_record in enumerate(tfrecord_dataset.as_numpy_iterator()):

            example = tf.train.Example()
            example.ParseFromString(string_record)

            targetIdTfrec = example.features.feature['target_id'].int64_list.value[0]
            tceIdentifierTfrec = example.features.feature[tceIdentifier].int64_list.value[0]

            tceFound = tceTbl.loc[(tceTbl['target_id'] == targetIdTfrec) &
                                  (tceTbl['tce_plnt_num'] == tceIdentifierTfrec)]

            # possible planet KOI is not a confirmed KOI
            if tceFound['fpwg_disp_status'].values[0] == 'POSSIBLE PLANET' and \
                    tceFound['koi_disposition'].values[0] != 'CONFIRMED':
                tcesRemoved[srcTfrecFile.split('/')[-1].split('-')[0]] += 1
                continue

            writer.write(example.SerializeToString())

print(tcesRemoved)

#%% Count number of TCEs in the TFRecords

#%% Count TCEs from dataset

tceTbl = pd.read_csv('/data5/tess_project/Data/Ephemeris_tables/Kepler/Q1-Q17_DR25/'
                     'q1_q17_dr25_tce_2020.04.15_23.19.10_cumkoi_2020.02.21_shuffledstar_noroguetces_noRobovetterKOIs.csv')

tceIdentifier = 'tce_plnt_num'
srcTfrecDir = '/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/tfrecordskeplerdr25-dv_g2001-l201_spline_nongapped_flux-centroid-6stellar_shalluevand_fdl_data/tfrecordskeplerdr25-dv_g2001-l201_spline_nongapped_flux-centroid-6stellar_shalluevand_fdl'

tcesCount = {'PC': 0, 'NTP': 0, 'AFP': 0}
for srcTfrecFile in srcTfrecFiles:

    tfrecord_dataset = tf.data.TFRecordDataset(srcTfrecFile)

    for string_i, string_record in enumerate(tfrecord_dataset.as_numpy_iterator()):

        example = tf.train.Example()
        example.ParseFromString(string_record)

        targetIdTfrec = example.features.feature['target_id'].int64_list.value[0]
        tceIdentifierTfrec = example.features.feature[tceIdentifier].int64_list.value[0]
        labelTfrec = example.features.feature['label'].bytes_list.value[0].decode("utf-8")

        tcesCount[labelTfrec] += 1

print(tcesCount)
print(np.sum(list(tcesCount.values())))

#%% Add tce_dikco_msky, tce_dicco_msky and tce_max_mult_ev to the TFRecords

features = ['tce_dikco_msky', 'tce_dicco_msky', 'tce_max_mult_ev', 'tce_maxmes']

trainsetTbl = pd.read_csv('/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/train-val-test-sets/trainset.csv')
print('Number of TCEs in the training set: {}'.format(len(trainsetTbl)))

tceTbl = pd.read_csv('/data5/tess_project/Data/Ephemeris_tables/Kepler/Q1-Q17_DR25/'
                     'q1_q17_dr25_tce_2020.09.15_15.12.12_stellar_koi_cfp_norobovetterlabels_renamedcols_'
                     'rmcandandfpkois_norogues.csv')
print('Number of TCEs in the dataset: {}'.format(len(tceTbl)))

tceTbl['UID'] = tceTbl[['target_id', 'tce_plnt_num']].apply(lambda x: '{}-{}'.format(x['target_id'], x['tce_plnt_num']),
                                                            axis=1)
trainsetTbl['UID'] = trainsetTbl[['target_id', 'tce_plnt_num']].apply(lambda x:
                                                                      '{}-{}'.format(x['target_id'], x['tce_plnt_num']),
                                                                      axis=1)

# get only training set TCEs
trainTceTbl = tceTbl[tceTbl['UID'].isin(trainsetTbl['UID'])][features]

assert len(trainTceTbl) == len(trainsetTbl)

# compute training statistics for standardization
statsFeatures = {feature: {'median': np.median(trainTceTbl[feature]),
                           'mad_std': mad_std(trainTceTbl[feature])
                           } for feature in features}


def _standardize_and_clip(row, normStats):
    for feature in row.index:
        row[feature] -= normStats[feature]['median']
        row[feature] /= normStats[feature]['mad_std']
        row[feature] = np.clip(row[feature], a_min=0, a_max=20 * normStats[feature]['mad_std'])

    return row

# standardize for all the data set
tceTbl[features] = tceTbl[features].apply(_standardize_and_clip, args=(statsFeatures,), axis=1)

# add standardized features to the TFRecords
srcTfrecDir = '/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/tfrecordskeplerdr25-dv_g2001-l201_spline_' \
              'nongapped_flux-loe-lwks-centroid-centroid_fdl-6stellar-bfap-ghost-rollingband_data/' \
              'tfrecordskeplerdr25-dv_g2001-l201_spline_nongapped_flux-loe-lwks-centroid-centroid_fdl-6stellar-bfap-' \
              'ghost-rollingband_starshuffle_experiment-labels-norm_withcentrmedind_std'
srcTfrecFiles = [os.path.join(srcTfrecDir, file) for file in os.listdir(srcTfrecDir) if 'shard' in file]
srcTfrecFiles = [file for file in srcTfrecFiles if not file.endswith('.csv')]
destTfrecDir = srcTfrecDir + '_diffimg_koot_coff-mes-wksmaxmes'
os.makedirs(destTfrecDir, exist_ok=True)
tceIdentifier = 'tce_plnt_num'
for srcTfrecFile in srcTfrecFiles:

    with tf.io.TFRecordWriter(os.path.join(destTfrecDir, srcTfrecFile.split('/')[-1])) as writer:

        tfrecord_dataset = tf.data.TFRecordDataset(srcTfrecFile)

        for string_i, string_record in enumerate(tfrecord_dataset.as_numpy_iterator()):

            example = tf.train.Example()
            example.ParseFromString(string_record)

            targetIdTfrec = example.features.feature['target_id'].int64_list.value[0]
            tceIdentifierTfrec = example.features.feature[tceIdentifier].int64_list.value[0]
            tceScalarParams = np.array(example.features.feature['scalar_params'].float_list.value)

            tceFound = tceTbl.loc[(tceTbl['target_id'] == targetIdTfrec) &
                                  (tceTbl['tce_plnt_num'] == tceIdentifierTfrec)]

            for feature in features:
                tceScalarParams = np.append(tceScalarParams, tceFound[feature])

            example_util.set_float_feature(example, 'scalar_params', tceScalarParams, allow_overwrite=True)

            writer.write(example.SerializeToString())

#%% Plot processed data (views, scalar parameters...)

plt.switch_backend('agg')

tfrecDir = '/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/tfrecordskeplerdr25-dv_g301-l31_6tr_spline_nongapped_flux-loe-lwks-centroid-centroid_fdl-6stellar-bfap-ghost-rollingband-stdtimeseries_secsymphase_wksnorm_maxflux-wks_data/tfrecordskeplerdr25-dv_g301-l31_6tr_spline_nongapped_flux-loe-lwks-centroid-centroid_fdl-6stellar-bfap-ghost-rollingband-stdtimeseries_secsymphase_wksnorm_maxflux-wks_starshuffle_experiment-labels-norm_nopps'
plotDir = '/home/msaragoc/Projects/Kepler-TESS_exoplanet/Data/tfrecords/Kepler/Q1-Q17_DR25/' \
          'test_set_misclassified_configK_nopps_CertifiedFPs_10-26-2020'
os.makedirs(plotDir, exist_ok=True)
tfrecFiles = [os.path.join(tfrecDir, file) for file in os.listdir(tfrecDir) if 'shard' in file]

rankingTbl = pd.read_csv('/home/msaragoc/Projects/Kepler-TESS_exoplanet/Analysis/misclassified_analysis/'
                         'configK_nopps_10-26-2020/ranking_Certified FP_testset.csv')[:4]
# tce = (6587796, 1)

tceIdentifier = 'tce_plnt_num'
views = [
    'global_flux_view',
    'local_flux_view',
    'global_flux_view_fluxnorm',
    'local_flux_view_fluxnorm',
    # 'global_flux_odd_view',
    'local_flux_odd_view',
    'local_flux_odd_view_fluxnorm',
    # 'global_flux_even_view',
    'local_flux_even_view',
    'local_flux_even_view_fluxnorm',
    # 'local_flux_oddeven_view_diff',
    # 'local_flux_oddeven_view_diff_dir',
    # 'global_weak_secondary_view',
    'local_weak_secondary_view',
    'local_weak_secondary_view_fluxnorm',
    'global_centr_view',
    'local_centr_view',
    # 'global_centr_view_std_clip',
    # 'local_centr_view_std_clip',
    'global_centr_view_std_noclip',
    'local_centr_view_std_noclip',
    # 'global_centr_view_medind_std',
    # 'local_centr_view_medind_std',
    # 'global_centr_view_medcmaxn',
    # 'local_centr_view_medcmaxn',
    # 'global_centr_view_medcmaxn_dir',
    # 'local_centr_view_medcmaxn_dir',
    # 'global_centr_view_medn',
    # 'local_centr_view_medn',
    'global_centr_fdl_view',
    'local_centr_fdl_view',
    # 'global_centr_fdl_view_norm',
    # 'local_centr_fdl_view_norm',
]

# scalarParams = {
#     'tce_steff': 0,
#     'tce_slogg': 1,
#     'tce_smet': 2,
#     'tce_sradius': 3,
#     # 'wst_robstat': 4,
#     # 'wst_depth': 5,
#     # 'tce_bin_oedp_stat': 6,
#     'boot_fap': 7,
#     'tce_smass': 8,  # 4
#     'tce_sdens': 9,  # 5
#     'tce_cap_stat': 10,
#     'tce_hap_stat': 11,
#     'tce_rb_tcount0': 12
# }

scalarParams = [
    'tce_period',
    'tce_duration',
    'transit_depth',
    'tce_max_mult_ev',
    # 'tce_model_snr',
    # 'tce_maxmesd',
    'tce_ptemp',
    'tce_albedo',
    'tce_dicco_msky',
    'tce_dicco_msky_err',
    'tce_dikco_msky',
    'tce_dikco_msky_err',
    'tce_steff',
    'tce_slogg',
    'tce_smet',
    'tce_sradius',
    'boot_fap',
    'tce_smass',
    'tce_sdens',
    'tce_cap_stat',
    'tce_hap_stat',
    'tce_rb_tcount0',
]

scheme = (3, 6)
basename = 'views'  # basename for figures

for tfrecFile in tfrecFiles:

    tfrecord_dataset = tf.data.TFRecordDataset(tfrecFile)

    for string_record in tfrecord_dataset.as_numpy_iterator():

        example = tf.train.Example()
        example.ParseFromString(string_record)

        tceIdentifierTfrec = example.features.feature[tceIdentifier].int64_list.value[0]
        targetIdTfrec = example.features.feature['target_id'].int64_list.value[0]

        if len(rankingTbl.loc[(rankingTbl['target_id'] == targetIdTfrec) &
                              (rankingTbl[tceIdentifier] == tceIdentifierTfrec)]) == 0:
            continue
        # if tce != (targetIdTfrec, tceIdentifierTfrec):
        #     continue

        # scalarParamsTfrec = np.array(example.features.feature['scalar_params'].float_list.value)
        scalarParamsStr = ''
        for scalarParam_i, scalarParam in enumerate(scalarParams):
            if scalarParam in ['tce_rb_tcount0', 'tce_steff']:
                scalarParamNonNormalized = np.array(example.features.feature[scalarParam].int64_list.value)[0]
            else:
                scalarParamNonNormalized = np.array(example.features.feature[scalarParam].float_list.value)[0]
            scalarParamNormalized = np.array(example.features.feature['{}_norm'.format(scalarParam)].float_list.value)[0]
            if scalarParam_i % 5 == 0:
                scalarParamsStr += '\n'
            if scalarParam in ['boot_fap']:
                scalarParamsStr += '{}={:.4f}({:.4E}) '.format(scalarParam,
                                                               scalarParamNonNormalized,
                                                               scalarParamNormalized)
            elif scalarParam in ['tce_rb_tcount0', 'tce_steff', 'transit_depth', 'tce_max_mult_ev',
                                 'tce_model_snr', 'tce_ptemp']:
                scalarParamsStr += '{}={:.4f}({:.0f})  '.format(scalarParam,
                                                            scalarParamNormalized,
                                                            scalarParamNonNormalized)
            else:
                scalarParamsStr += '{}={:.4f}({:.4f})  '.format(scalarParam,
                                                                scalarParamNormalized,
                                                                scalarParamNonNormalized)

        labelTfrec = example.features.feature['label'].bytes_list.value[0].decode("utf-8")

        viewsDict = {}
        for view in views:
            viewsDict[view] = np.array(example.features.feature[view].float_list.value)

        f, ax = plt.subplots(scheme[0], scheme[1], figsize=(22, 12))
        k = 0
        views_list = list(viewsDict.keys())
        for i in range(scheme[0]):
            for j in range(scheme[1]):
                if k < len(views_list):
                    ax[i, j].plot(viewsDict[views_list[k]])
                    ax[i, j].scatter(np.arange(len(viewsDict[views_list[k]])), viewsDict[views_list[k]], s=10,
                                     color='k', alpha=0.2)
                    ax[i, j].set_title(views_list[k], pad=20)
                if i == scheme[0] - 1:
                    ax[i, j].set_xlabel('Bin number')
                if j == 0:
                    ax[i, j].set_ylabel('Amplitude')
                k += 1

        f.suptitle('TCE {}-{} {}\n{}'.format(targetIdTfrec, tceIdentifierTfrec, labelTfrec, scalarParamsStr))
        plt.subplots_adjust(top=0.86,
                            bottom=0.067,
                            left=0.055,
                            right=0.992,
                            hspace=0.442,
                            wspace=0.394)
        plt.savefig(os.path.join(plotDir, '{}_{}_{}_{}.png'.format(targetIdTfrec, tceIdentifierTfrec, labelTfrec,
                                                                   basename)))
        # aaaa
        plt.close()

#%% Add scalar features to the TFRecords

features = {
    'tce_dikco_msky': {'missing_value': 0, 'log_transform': False, 'log_transform_eps': np.nan,
                       'clip_factor': 20},
    'tce_dikco_msky_err': {'missing_value': -1, 'log_transform': False, 'log_transform_eps': np.nan,
                           'clip_factor': 20},
    'tce_dicco_msky': {'missing_value': 0, 'log_transform': False, 'log_transform_eps': np.nan,
                       'clip_factor': 20},
    'tce_dicco_msky_err': {'missing_value': -1, 'log_transform': False, 'log_transform_eps': np.nan,
                           'clip_factor': 20},
    'tce_max_mult_ev': {'missing_value': np.nan, 'log_transform': False, 'log_transform_eps': np.nan,
                        'clip_factor': 200},
    'tce_maxmes': {'missing_value': 0, 'log_transform': False, 'log_transform_eps': np.nan,
                   'clip_factor': 20},
    'tce_albedo': {'missing_value': 0, 'log_transform': False, 'log_transform_eps': np.nan,
                   'clip_factor': 20},
    'tce_ptemp': {'missing_value': 0, 'log_transform': False, 'log_transform_eps': np.nan,
                  'clip_factor': 20},
    'transit_depth': {'missing_value': np.nan, 'log_transform': False, 'log_transform_eps': np.nan,
                      'clip_factor': 20},
    'tce_depth_err': {'missing_value': -1, 'log_transform': False, 'log_transform_eps': np.nan,
                      'clip_factor': 20},
    'tce_duration': {'missing_value': np.nan, 'log_transform': False, 'log_transform_eps': np.nan,
                         'clip_factor': np.nan},
    'tce_duration_err': {'missing_value': -1, 'log_transform': False, 'log_transform_eps': np.nan,
                         'clip_factor': 20},
    'tce_period': {'missing_value': np.nan, 'log_transform': False, 'log_transform_eps': np.nan,
                     'clip_factor': np.nan},
    'tce_period_err': {'missing_value': -1, 'log_transform': False, 'log_transform_eps': np.nan,
                         'clip_factor': 20},
    'tce_steff': {'missing_value': np.nan, 'log_transform': False, 'log_transform_eps': np.nan,
                       'clip_factor': np.nan},
    'tce_sradius': {'missing_value': -1, 'log_transform': False, 'log_transform_eps': np.nan,
                       'clip_factor': np.nan},
    'tce_sdens': {'missing_value': np.nan, 'log_transform': False, 'log_transform_eps': np.nan,
                       'clip_factor': np.nan},
    'tce_slogg': {'missing_value': np.nan, 'log_transform': False, 'log_transform_eps': np.nan,
                       'clip_factor': np.nan},
    'tce_smet': {'missing_value': np.nan, 'log_transform': False, 'log_transform_eps': np.nan,
                       'clip_factor': np.nan},
    'tce_smass': {'missing_value': -1, 'log_transform': False, 'log_transform_eps': np.nan,
                       'clip_factor': np.nan},
    'tce_cap_stat': {'missing_value': np.nan, 'log_transform': False, 'log_transform_eps': np.nan,
                       'clip_factor': 20},
    'tce_hap_stat': {'missing_value': np.nan, 'log_transform': False, 'log_transform_eps': np.nan,
                       'clip_factor': 20},
    'boot_fap': {'missing_value': -1, 'log_transform': True, 'log_transform_eps': 1e-32,
                       'clip_factor': np.nan},
    'tce_rb_tcount0': {'missing_value': np.nan, 'log_transform': False, 'log_transform_eps': np.nan,
                       'clip_factor': np.nan},

}
featuresList = list(features.keys())

trainsetTbl = pd.read_csv('/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/train-val-test-sets/trainset.csv')
print('Number of TCEs in the training set: {}'.format(len(trainsetTbl)))

tceTbl = pd.read_csv('/data5/tess_project/Data/Ephemeris_tables/Kepler/Q1-Q17_DR25/q1_q17_dr25_tce_2020.09.28_10.36.22_stellar_koi_cfp_norobovetterlabels_renamedcols_nomissingval_rmcandandfpkois_norogues.csv')
print('Number of TCEs in the dataset: {}'.format(len(tceTbl)))

tceTbl['UID'] = tceTbl[['target_id', 'tce_plnt_num']].apply(lambda x: '{}-{}'.format(x['target_id'], x['tce_plnt_num']),
                                                            axis=1)
trainsetTbl['UID'] = trainsetTbl[['target_id', 'tce_plnt_num']].apply(lambda x:
                                                                      '{}-{}'.format(x['target_id'], x['tce_plnt_num']),
                                                                      axis=1)

# get only training set TCEs
trainTceTbl = tceTbl[tceTbl['UID'].isin(trainsetTbl['UID'])][featuresList]

assert len(trainTceTbl) == len(trainsetTbl)

# compute training statistics for standardization
for feature in features:

    # remove missing values
    if not np.isnan(features[feature]['missing_value']):
        values = trainTceTbl.loc[trainTceTbl[feature] != features[feature]['missing_value'], feature].values
    else:
        values = trainTceTbl[feature].values

    # log transform
    if features[feature]['log_transform']:
        values += features[feature]['log_transform_eps']
        values = np.log10(values)

    features[feature].update({
        'median': np.median(values),
        'mad_std': mad_std(values)
    })


def _standardize(row, normStats):
    """ Standardize parameters for a TCE.

    :param row: pandas Series, TCE parameters
    :param normStats: dict, normalization statistics
    :return:
        pandas Series with TCE parameters standardized
    """

    for feature in row.index:

        # replace missing values by central tendency estimate of the training set
        if row[feature] == normStats[feature]['missing_value']:
            row[feature] = normStats[feature]['median']

        else:

            # log transform the data
            if normStats[feature]['log_transform']:

                # add constant value to deal with zero for log transform
                if not np.isnan(normStats[feature]['log_transform_eps']):
                    row[feature] += normStats[feature]['log_transform_eps']

                row[feature] = np.log10(row[feature])

            # clipping the data between median +- clip_factor * mad_std
            if not np.isnan(normStats[feature]['clip_factor']):
                row[feature] = np.clip([row[feature]],
                                       normStats[feature]['median'] -
                                       normStats[feature]['clip_factor'] * normStats[feature]['mad_std'],
                                       normStats[feature]['median'] +
                                       normStats[feature]['clip_factor'] * normStats[feature]['mad_std']
                                       )[0]

        # standardization
        row[feature] -= normStats[feature]['median']
        row[feature] /= normStats[feature]['mad_std']

    return row


# standardize for all the data set
tceTbl[featuresList] = tceTbl[featuresList].apply(_standardize, args=(features,), axis=1)

# source TFRecords
srcTfrecDir = '/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/tfrecordskeplerdr25-dv_g301-l31_6tr_spline_nongapped_flux-loe-centroid-centroid_fdl-6stellar-bfap-ghost-rollingband_data/tfrecordskeplerdr25-dv_g301-l31_6tr_spline_nongapped_flux-loe-centroid-centroid_fdl-6stellar-bfap-ghost-rollingband_starshuffle_experiment-labels-norm'
srcTfrecFiles = [os.path.join(srcTfrecDir, file) for file in os.listdir(srcTfrecDir) if 'shard' in file]
srcTfrecFiles = [file for file in srcTfrecFiles if not file.endswith('.csv')]

# define TFRecords destination
destTfrecDir = srcTfrecDir + '_convscalars'
os.makedirs(destTfrecDir, exist_ok=True)

tceIdentifier = 'tce_plnt_num'

# add standardized features to the source TFRecords and write them to the destination
for srcTfrecFile in srcTfrecFiles:

    with tf.io.TFRecordWriter(os.path.join(destTfrecDir, srcTfrecFile.split('/')[-1])) as writer:

        tfrecord_dataset = tf.data.TFRecordDataset(srcTfrecFile)

        for string_i, string_record in enumerate(tfrecord_dataset.as_numpy_iterator()):

            example = tf.train.Example()
            example.ParseFromString(string_record)

            targetIdTfrec = example.features.feature['target_id'].int64_list.value[0]
            tceIdentifierTfrec = example.features.feature[tceIdentifier].int64_list.value[0]
            tceScalarParams = np.array(example.features.feature['scalar_params'].float_list.value)

            tceFound = tceTbl.loc[(tceTbl['target_id'] == targetIdTfrec) &
                                  (tceTbl['tce_plnt_num'] == tceIdentifierTfrec)]

            # for feature in featuresList:
            #     tceScalarParams = np.append(tceScalarParams, tceFound[feature])

            # example_util.set_float_feature(example, 'scalar_params', tceScalarParams, allow_overwrite=True)

            for feature in featuresList:
                example_util.set_float_feature(example, '{}_norm'.format(feature), tceFound[feature].values)

            writer.write(example.SerializeToString())
