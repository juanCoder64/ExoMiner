# 3rd party
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
import os
from astropy.stats import mad_std
from pathlib import Path
import logging
import multiprocessing
import itertools

# local
from src_preprocessing.tf_util import example_util
from src_preprocessing.utils_manipulate_tfrecords import create_shard, normalize_scalar_features, \
    normalize_timeseries_features

#%%

# # plot boxplots of centroid local and global views in the training set
# print('Generating boxplots...')
# f, ax = plt.subplots()
# ax.boxplot([centroidMat['global_centr_view'], centroidMat['local_centr_view']], bootstrap=None, meanline=True,
#            showmeans=True)
# ax.set_title('Non-normalized centroid time series')
# ax.set_yscale('log')
# ax.set_ylabel('Value')
# ax.set_xticklabels(['Global view', 'Local view'])
# f.savefig('/home/msaragoc/Downloads/hist_nonnormalized_centroids.png')
# # plt.show()

#%% Checking values for centroid views

# print(len(np.where(centroidMat['global_centr_view'], np.percentile(centroidMat['global_centr_view'], 75))))

badTces = []
thr = 1000  # 5844.65576171875  # 99.9 percentile

label = 'AFP'
# centroidDict = {'global_centr_view': [], 'local_centr_view': []}

for tfrec_i, tfrecFile in enumerate(tfrecTrainFiles):

    print('Getting data from {} ({} %)'.format(tfrecFile.split('/')[-1], tfrec_i / len(tfrecTrainFiles) * 100))

    # iterate through the shard
    record_iterator = tf.python_io.tf_record_iterator(path=tfrecFile)

    for string_i, string_record in enumerate(record_iterator):

        example = tf.train.Example()
        example.ParseFromString(string_record)

        tceLabel = example.features.feature['label'].bytes_list.value[0].decode("utf-8")

        if tceLabel != label:
            continue

        tceIdentifierTfrec = example.features.feature[tceIdentifier].int64_list.value[0]
        targetIdTfrec = example.features.feature['target_id'].int64_list.value[0]

        # get centroid time series data

        timeSeriesTce = np.array(example.features.feature['global_centr_view'].float_list.value)
        # centroidDict['global_centr_view'].append(np.min(timeSeriesTce))

        if np.any(np.array(timeSeriesTce) > thr):
            badTces.append((targetIdTfrec, tceIdentifierTfrec))

        # timeSeriesTce = np.array(example.features.feature['local_centr_view'].float_list.value)
        # centroidDict['local_centr_view'].append(np.min(timeSeriesTce))

print(len(badTces))
print(badTces)

# numOutliers = {}
# for timeSeries in centroidDict:
#     q75 = np.percentile(centroidDict[timeSeries], 75)
#     q25 = np.percentile(centroidDict[timeSeries], 25)
#
#     iqr = q75 - q25
#
#     numOutliers[timeSeries] = len(np.where(centroidDict[timeSeries] > 1.5 * iqr)[0])
#
# # plot boxplots of centroid local and global views in the training set
# print('Generating boxplots...')
# f, ax = plt.subplots()
# ax.boxplot([centroidDict['global_centr_view'], centroidDict['local_centr_view']], bootstrap=None, meanline=True,
#            showmeans=True)
# ax.set_title('Non-normalized centroid time series\n{} Num outliers = {}'.format(label, numOutliers[timeSeries]))
# ax.set_yscale('log')
# ax.set_ylabel('Median value')
# ax.set_xticklabels(['Global view', 'Local view'])
# f.savefig('/home/msaragoc/Downloads/hist_nonnormalized_centroids_{}.png'.format(label))
# plt.show()

#%% Plot minimum value for odd and even local views for a given dataset

# rankingTbl = pd.read_csv('/home/msaragoc/Projects/Kepler-TESS_exoplanet/Kepler_planet_finder/results_ensemble/'
#                          'keplerdr25_g2001-l201_spline_gapped_glfluxbohb_norobovetterkois_starshuffle_glflux-loe/'
#                          'ensemble_ranked_predictions_testset')
tceTbl = pd.read_csv('/data5/tess_project/Data/Ephemeris_tables/Kepler/Q1-Q17_DR25/'
                     'q1_q17_dr25_tce_2020.04.15_23.19.10_cumkoi_2020.02.21_shuffledstar_noroguetces_noRobobvetterKOIs.csv')

tceTbl['odd_min_val'] = np.nan
tceTbl['even_min_val'] = np.nan

for targetId, tceId in oddevenMinTce:
    tceTbl.loc[(tceTbl['target_id'] == targetId) &
               (tceTbl['tce_plnt_num'] == tceId), ['odd_min_val', 'even_min_val']] = oddevenMinTce[targetId, tceId]

dispositions = ['PC', 'AFP', 'NTP']
plotTbls = {disposition: None for disposition in dispositions}
for disposition in dispositions:
    plotTbls[disposition] = tceTbl.loc[tceTbl['label'] == disposition]
    # plotTbls[disposition] = rankingTbl.loc[tceTbl['original_label'] == disposition]

f, ax = plt.subplots()
# ax.scatter(plotTbls['PC']['odd_min_val'].values, plotTbls['PC']['even_min_val'].values, label='PC', s=15, c='g')
# ax.scatter(plotTbls['AFP']['odd_min_val'].values, plotTbls['AFP']['even_min_val'].values, label='AFP', s=15, c='y')
ax.scatter(plotTbls['NTP']['odd_min_val'].values, plotTbls['NTP']['even_min_val'].values, label='NTP', s=15, c='r')
ax.set_ylabel('Even view min value')
ax.set_xlabel('Odd view min value')
ax.set_title('Q1-Q17 DR25 \nNon-rogue TCEs and no Robovetter KOIs dataset')
ax.legend()
# ax.set_yscale('log')
# ax.set_xscale('log')

#%% Plot oot-std value for local weak secondary flux view for a given dataset

rankingTbl = pd.read_csv('/home/msaragoc/Projects/Kepler-TESS_exoplanet/Kepler_planet_finder/results_ensemble/'
                         'keplerdr25_g2001-l201_spline_gapped_glfluxbohb_norobovetterkois_starshuffle_glflux-loe/'
                         'ensemble_ranked_predictions_testset')
# tceTbl = pd.read_csv('/data5/tess_project/Data/Ephemeris_tables/Kepler/Q1-Q17_DR25/'
#                      'q1_q17_dr25_tce_2020.04.15_23.19.10_cumkoi_2020.02.21_shuffledstar_noroguetces_noRobobvetterKOIs.csv')

rankingTbl['lwks_oot-std'] = np.nan

for targetId, tceId in oddevenMinTce:
    rankingTbl.loc[(rankingTbl['target_id'] == targetId) &
                   (rankingTbl['tce_plnt_num'] == tceId), 'lwks_oot'] = lwksViewStdTce[targetId, tceId]

dispositions = ['PC', 'AFP', 'NTP']
plotTbls = {disposition: None for disposition in dispositions}
for disposition in dispositions:
    # plotTbls[disposition] = tceTbl.loc[tceTbl['label'] == disposition]
    plotTbls[disposition] = rankingTbl.loc[rankingTbl['original_label'] == disposition]

f, ax = plt.subplots()
ax.scatter(plotTbls['PC']['lwks_oot'].values, plotTbls['PC']['score'].values, label='PC', s=15, c='g')
ax.scatter(plotTbls['AFP']['lwks_oot'].values, plotTbls['AFP']['score'].values, label='AFP', s=15, c='y')
ax.scatter(plotTbls['NTP']['lwks_oot'].values, plotTbls['NTP']['score'].values, label='NTP', s=15, c='r')
ax.set_ylabel('Score')
ax.set_xlabel('Local weak secondary oot-std')
ax.set_title('Test set')
ax.legend()
# ax.set_xscale('log')
ax.set_ylim([0, 1])
# ax.set_xlim([0, 1])

#%% Iterate over TFRecords to see where we have a Data Loss Error

tfrecDir = '/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/tfrecordskeplerdr25_g2001-l201_spline_nongapped_flux-centroid_selfnormalized-oddeven-wks-scalar_tps_globalbinwidthaslocal_data/tfrecordskeplerdr25_tps_g2001-l201_gbal_spline_nongapped_flux-centroid-oddeven-wks-scalar_starshuffle_experiment-labels-norm'

tfrecFiles = [os.path.join(tfrecDir, file) for file in os.listdir(tfrecDir) if 'shard' in file]

for tfrecFile in tfrecFiles:

    record_iterator = tf.python_io.tf_record_iterator(path=tfrecFile)

    for string_i, string_record in enumerate(record_iterator):

        a = 1

#%% Correcting local centr medcmaxn_dir by dividing by max(abs) since it was dividied by max

tceIdentifier = 'tce_plnt_num'
srcTfrecDir = '/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/tfrecordskeplerdr25_nontces_g2001-l201_gbal_spline_nongapped_flux-centroid-oddeven-wks-scalar_data/tfrecordskeplerdr25_nontces_g2001-l201_gbal_spline_nongapped_flux-centroid-oddeven-wks-scalar'
srcTfrecFiles = [os.path.join(srcTfrecDir, file) for file in os.listdir(srcTfrecDir) if 'shard' in file]
srcTfrecFiles = [file for file in srcTfrecFiles if not file.endswith('.csv')]
destTfrecDir = '/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/tfrecordskeplerdr25_nontces_g2001-l201_gbal_spline_nongapped_flux-centroid-oddeven-wks-scalar_data/tfrecordskeplerdr25_nontces_g2001-l201_gbal_spline_nongapped_flux-centroid-oddeven-wks-scalar_correct'
os.makedirs(destTfrecDir, exist_ok=True)

for srcTfrecFile in srcTfrecFiles:

    with tf.python_io.TFRecordWriter(os.path.join(destTfrecDir, srcTfrecFile.split('/')[-1])) as writer:

        record_iterator = tf.python_io.tf_record_iterator(path=srcTfrecFile)

        for string_i, string_record in enumerate(record_iterator):

            example = tf.train.Example()
            example.ParseFromString(string_record)

            loc_centr = np.array(example.features.feature['local_centr_view'].float_list.value)

            loc_centr -= np.median(loc_centr)
            loc_centr_medc_maxabsn = loc_centr / np.max(np.abs(loc_centr))
            example_util.set_float_feature(example, 'local_centr_view_medcmaxn_dir', loc_centr_medc_maxabsn,
                                           allow_overwrite=True)

            writer.write(example.SerializeToString())

#%% Adding the normalized rolling band diagnostic to the TFRecords

tceIdentifier = 'tce_plnt_num'
srcTfrecDir = '/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/tfrecordskeplerdr25_g2001-l201_spline_nongapped_flux-centroid_selfnormalized-oddeven-wks-scalar_globalbinwidthaslocal_data/tfrecordskeplerdr25_g2001-l201_spline_nongapped_flux-centroid_selfnormalized-oddeven-wks-scalar_globalbinwidthaslocal_starshuffle_experiment-labels-norm_rollingband_newcentrnorm_ghostclip'
srcTfrecFiles = [os.path.join(srcTfrecDir, file) for file in os.listdir(srcTfrecDir) if 'shard' in file]
destTfrecDir = '/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/tfrecordskeplerdr25_g2001-l201_spline_nongapped_flux-centroid_selfnormalized-oddeven-wks-scalar_globalbinwidthaslocal_data/tfrecordskeplerdr25_tce1_g2001-l201_spline_nongapped_flux-centroid_selfnormalized-oddeven-wks-scalar_globalbinwidthaslocal_starshuffle_experiment-labels-norm_rollingband_newcentrnorm_ghostclip'
os.makedirs(destTfrecDir, exist_ok=True)

# training set TCE table
trainTceTbl = pd.read_csv('/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/tfrecordskeplerdr25_g2001-l201_spline_gapped_flux-centroid_selfnormalized-oddeven-wks-scalar_data/'
                          'tfrecordskeplerdr25_g2001-l201_spline_gapped_flux-centroid_selfnormalized-oddeven-wks-scalar_starshuffle_experiment/trainset.csv')

# TCE table
tceTbl = pd.read_csv('/data5/tess_project/Data/Ephemeris_tables/Kepler/Q1-Q17_DR25/'
                     'q1_q17_dr25_tce_2020.04.15_23.19.10_cumkoi_2020.02.21_shuffledstar_noroguetces_noRobovetterKOIs.csv')

# compute training set statistics for the rolling band diagnostic
tce_rb_tcount0Arr = np.zeros(len(trainTceTbl), dtype='float')
for tce_i, tce in trainTceTbl.iterrows():
    tce_rb_tcount0Arr[tce_i] = tceTbl.loc[(tceTbl['target_id'] == tce['target_id']) &
                                          (tceTbl[tceIdentifier] == tce[tceIdentifier])]['tce_rb_tcount0']
normStats = {
    'tce_rb_tcount0': {'median': np.median(tce_rb_tcount0Arr),
                       'std': stats.mad_std(tce_rb_tcount0Arr)}
}

# normalization stats for ghost diagnostic
normStatsDir = '/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/tfrecordskeplerdr25_g2001-l201_spline_nongapped_flux-centroid_selfnormalized-oddeven-wks-scalar_globalbinwidthaslocal_data/tfrecordskeplerdr25_g2001-l201_spline_nongapped_flux-centroid_selfnormalized-oddeven-wks-scalar_globalbinwidthaslocal_starshuffle_experiment/'
# scalar parameters normalization statistics
normStatsScalar = np.load(os.path.join(normStatsDir, 'train_scalarparam_norm_stats.npy'), allow_pickle=True).item()

for srcTfrecFile in srcTfrecFiles:

    with tf.python_io.TFRecordWriter(os.path.join(destTfrecDir, srcTfrecFile.split('/')[-1])) as writer:

        record_iterator = tf.python_io.tf_record_iterator(path=srcTfrecFile)

        for string_i, string_record in enumerate(record_iterator):

            example = tf.train.Example()
            example.ParseFromString(string_record)

            # get rolling band diagnostic from TCE table for this TCE; normalize it using training set stats;
            # add to the example
            tceIdentifierTfrec = example.features.feature[tceIdentifier].int64_list.value[0]
            targetIdTfrec = example.features.feature['target_id'].int64_list.value[0]

            if tceIdentifierTfrec != 1:
                continue

            # tce_rb_tcount0Tfrec = tceTbl.loc[(tceTbl['target_id'] == targetIdTfrec) &
            #                                  (tceTbl[tceIdentifier] == tceIdentifierTfrec)]['tce_rb_tcount0']
            #
            # assert len(tce_rb_tcount0Tfrec) > 0
            #
            # tce_rb_tcount0Tfrec = (tce_rb_tcount0Tfrec - normStats['tce_rb_tcount0']['median']) / \
            #                       normStats['tce_rb_tcount0']['std']
            #
            # tceScalarParams = np.array(example.features.feature['scalar_params'].float_list.value)
            #
            # tceScalarParams = np.append(tceScalarParams, tce_rb_tcount0Tfrec.values[0])
            #
            # # clip ghost diagnostic values to 20 * sigma
            # # cap
            # tceScalarParams[-3] = np.clip(tceScalarParams[-3],
            #                               -20 * normStatsScalar['std'][-2],
            #                               20 * normStatsScalar['std'][-2])
            # # hap
            # tceScalarParams[-2] = np.clip(tceScalarParams[-2],
            #                               -20 * normStatsScalar['std'][-1],
            #                               20 * normStatsScalar['std'][-1])
            #
            # example_util.set_float_feature(example, 'scalar_params', tceScalarParams, allow_overwrite=True)
            #
            # # create new global and local centroid views
            # # 1) absolute and then max normalization
            # # 2) max(abs) normalization
            # glob_centr_medcmaxn = np.array(example.features.feature['global_centr_view_medcmaxn'].float_list.value)
            # loc_centr_medcmaxn = np.array(example.features.feature['local_centr_view_medcmaxn'].float_list.value)
            #
            # # 1)
            # glob_centr_medc_abs_maxn = np.abs(glob_centr_medcmaxn) / np.max(np.abs(glob_centr_medcmaxn))
            # example_util.set_float_feature(example, 'global_centr_view_medcmaxn', glob_centr_medc_abs_maxn,
            #                                allow_overwrite=True)
            # loc_centr_medc_abs_maxn = np.abs(loc_centr_medcmaxn) / np.max(np.abs(loc_centr_medcmaxn))
            # example_util.set_float_feature(example, 'local_centr_view_medcmaxn', loc_centr_medc_abs_maxn,
            #                                allow_overwrite=True)
            #
            # # 2)
            # glob_centr_medc_maxabsn = glob_centr_medcmaxn / np.max(np.abs(glob_centr_medcmaxn))
            # example_util.set_float_feature(example, 'global_centr_view_medcmaxn_dir', glob_centr_medc_maxabsn)
            # loc_centr_medc_maxabsn = loc_centr_medcmaxn / np.max(np.abs(loc_centr_medcmaxn))
            # example_util.set_float_feature(example, 'local_centr_view_medcmaxn_dir', loc_centr_medc_maxabsn)

            writer.write(example.SerializeToString())


#%% Correcting name for local flux odd-even diff dir

tceIdentifier = 'tce_plnt_num'
srcTfrecDir = '/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/tfrecordskeplerdr25_g2001-l201_gbal_spline_nongapped_flux-centroid-oddeven-wks-6stellar-ghost-bfap-rollingband_data/tfrecordskeplerdr25_g2001-l201_gbal_spline_nongapped_flux-centroid-oddeven-wks-6stellar-ghost-bfap-rollingband_starshuffle_experiment-labels-norm'
srcTfrecFiles = [os.path.join(srcTfrecDir, file) for file in os.listdir(srcTfrecDir) if 'shard' in file]
srcTfrecFiles = [file for file in srcTfrecFiles if not file.endswith('.csv')]
destTfrecDir = srcTfrecDir + '_correct'
os.makedirs(destTfrecDir, exist_ok=True)

for srcTfrecFile in srcTfrecFiles:

    with tf.python_io.TFRecordWriter(os.path.join(destTfrecDir, srcTfrecFile.split('/')[-1])) as writer:

        record_iterator = tf.python_io.tf_record_iterator(path=srcTfrecFile)

        for string_i, string_record in enumerate(record_iterator):

            example = tf.train.Example()
            example.ParseFromString(string_record)

            loc_oe_diff_dir = np.array(example.features.feature['local_flux_even_view_diff_dir'].float_list.value)

            example_util.set_float_feature(example, 'local_flux_oddeven_view_diff_dir', loc_oe_diff_dir)

            writer.write(example.SerializeToString())

#%% Getting only TCEs with tce_plnt_num = 1

tceIdentifier = 'tce_plnt_num'
srcTfrecDir = '/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/tfrecordskeplerdr25_g2001-l201_spline_nongapped_flux-centroid-oddeven-wks-6stellar-bfap-ghost-rollingband_data/tfrecordskeplerdr25-dv_g2001-l201_spline_nongapped_flux-centroid-oddeven-wks-6stellar-bfap-ghost-rollingband_starshuffle_experiment-labels-norm'
srcTfrecFiles = [os.path.join(srcTfrecDir, file) for file in os.listdir(srcTfrecDir) if 'shard' in file]
srcTfrecFiles = [file for file in srcTfrecFiles if not file.endswith('.csv')]
destTfrecDir = '/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/tfrecordskeplerdr25_g2001-l201_spline_nongapped_flux-centroid-oddeven-wks-6stellar-bfap-ghost-rollingband_data/tfrecordskeplerdr25-dv_g2001-l201_spline_nongapped_flux-centroid-oddeven-wks-6stellar-bfap-ghost-rollingband_starshuffle_experiment-labels-norm_tce1'
os.makedirs(destTfrecDir, exist_ok=True)

for srcTfrecFile in srcTfrecFiles:

    with tf.io.TFRecordWriter(os.path.join(destTfrecDir, srcTfrecFile.split('/')[-1])) as writer:

        # record_iterator = tf.python_io.tf_record_iterator(path=srcTfrecFile)
        tfrecord_dataset = tf.data.TFRecordDataset(srcTfrecFile)

        for string_i, string_record in enumerate(tfrecord_dataset.as_numpy_iterator()):
        # for string_i, string_record in enumerate(record_iterator):

            example = tf.train.Example()
            example.ParseFromString(string_record)

            tceIdentifierTfrec = example.features.feature[tceIdentifier].int64_list.value[0]

            if tceIdentifierTfrec != 1:
                continue

            writer.write(example.SerializeToString())

#%% Remove Possible Planets from dataset

tceTbl = pd.read_csv('/data5/tess_project/Data/Ephemeris_tables/Kepler/Q1-Q17_DR25/'
                     'q1_q17_dr25_tce_2020.09.28_10.36.22_stellar_koi_cfp_norobovetterlabels_renamedcols_nomissingval_'
                     'rmcandandfpkois_norogues.csv')

tceIdentifier = 'tce_plnt_num'
srcTfrecDir = '/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/tfrecordskeplerdr25-tps_g301-l31_6tr_spline_gapped1_flux-loe-lwks-centroid-centroidfdl-6stellar-stdts_correctprimarygapping_confirmedkoiperiod_data/tfrecordskeplerdr25-tps_g301-l31_6tr_spline_gapped1_flux-loe-lwks-centroid-centroidfdl-6stellar-stdts_correctprimarygapping_confirmedkoiperiod_starshuffle_experiment-labels-normdv'
srcTfrecFiles = [os.path.join(srcTfrecDir, file) for file in os.listdir(srcTfrecDir) if 'shard' in file]
srcTfrecFiles = [file for file in srcTfrecFiles if not file.endswith('.csv')]
destTfrecDir = srcTfrecDir + '_nopps'
os.makedirs(destTfrecDir, exist_ok=True)

tcesRemoved = {'train': 0, 'val': 0, 'test': 0}
for srcTfrecFile in srcTfrecFiles:

    with tf.io.TFRecordWriter(os.path.join(destTfrecDir, srcTfrecFile.split('/')[-1])) as writer:

        tfrecord_dataset = tf.data.TFRecordDataset(srcTfrecFile)

        for string_i, string_record in enumerate(tfrecord_dataset.as_numpy_iterator()):

            example = tf.train.Example()
            example.ParseFromString(string_record)

            targetIdTfrec = example.features.feature['target_id'].int64_list.value[0]
            tceIdentifierTfrec = example.features.feature[tceIdentifier].int64_list.value[0]

            tceFound = tceTbl.loc[(tceTbl['target_id'] == targetIdTfrec) &
                                  (tceTbl['tce_plnt_num'] == tceIdentifierTfrec)]

            # possible planet KOI is not a confirmed KOI
            if tceFound['fpwg_disp_status'].values[0] == 'POSSIBLE PLANET' and \
                    tceFound['koi_disposition'].values[0] != 'CONFIRMED':
                tcesRemoved[srcTfrecFile.split('/')[-1].split('-')[0]] += 1
                continue

            writer.write(example.SerializeToString())

print('Number of TCEs removed:', tcesRemoved)
print('Possible Planet KOIs removed from the dataset.')

#%% Count TCEs from dataset

tceTbl = pd.read_csv('/data5/tess_project/Data/Ephemeris_tables/Kepler/Q1-Q17_DR25/'
                     'q1_q17_dr25_tce_2020.04.15_23.19.10_cumkoi_2020.02.21_shuffledstar_noroguetces_noRobovetterKOIs.csv')

tceIdentifier = 'tce_plnt_num'
srcTfrecDir = '/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/tfrecordskeplerdr25-dv_g2001-l201_spline_nongapped_flux-centroid-6stellar_shalluevand_fdl_data/tfrecordskeplerdr25-dv_g2001-l201_spline_nongapped_flux-centroid-6stellar_shalluevand_fdl'

tcesCount = {'PC': 0, 'NTP': 0, 'AFP': 0}
for srcTfrecFile in srcTfrecFiles:

    tfrecord_dataset = tf.data.TFRecordDataset(srcTfrecFile)

    for string_i, string_record in enumerate(tfrecord_dataset.as_numpy_iterator()):

        example = tf.train.Example()
        example.ParseFromString(string_record)

        targetIdTfrec = example.features.feature['target_id'].int64_list.value[0]
        tceIdentifierTfrec = example.features.feature[tceIdentifier].int64_list.value[0]
        labelTfrec = example.features.feature['label'].bytes_list.value[0].decode("utf-8")

        tcesCount[labelTfrec] += 1

print(tcesCount)
print(np.sum(list(tcesCount.values())))

#%% Add tce_dikco_msky, tce_dicco_msky and tce_max_mult_ev to the TFRecords

features = ['tce_dikco_msky', 'tce_dicco_msky', 'tce_max_mult_ev', 'tce_maxmes']

trainsetTbl = pd.read_csv('/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/train-val-test-sets/trainset.csv')
print('Number of TCEs in the training set: {}'.format(len(trainsetTbl)))

tceTbl = pd.read_csv('/data5/tess_project/Data/Ephemeris_tables/Kepler/Q1-Q17_DR25/'
                     'q1_q17_dr25_tce_2020.09.15_15.12.12_stellar_koi_cfp_norobovetterlabels_renamedcols_'
                     'rmcandandfpkois_norogues.csv')
print('Number of TCEs in the dataset: {}'.format(len(tceTbl)))

tceTbl['UID'] = tceTbl[['target_id', 'tce_plnt_num']].apply(lambda x: '{}-{}'.format(x['target_id'], x['tce_plnt_num']),
                                                            axis=1)
trainsetTbl['UID'] = trainsetTbl[['target_id', 'tce_plnt_num']].apply(lambda x:
                                                                      '{}-{}'.format(x['target_id'], x['tce_plnt_num']),
                                                                      axis=1)

# get only training set TCEs
trainTceTbl = tceTbl[tceTbl['UID'].isin(trainsetTbl['UID'])][features]

assert len(trainTceTbl) == len(trainsetTbl)

# compute training statistics for standardization
statsFeatures = {feature: {'median': np.median(trainTceTbl[feature]),
                           'mad_std': mad_std(trainTceTbl[feature])
                           } for feature in features}


def _standardize_and_clip(row, normStats):
    for feature in row.index:
        row[feature] -= normStats[feature]['median']
        row[feature] /= normStats[feature]['mad_std']
        row[feature] = np.clip(row[feature], a_min=0, a_max=20 * normStats[feature]['mad_std'])

    return row

# standardize for all the data set
tceTbl[features] = tceTbl[features].apply(_standardize_and_clip, args=(statsFeatures,), axis=1)

# add standardized features to the TFRecords
srcTfrecDir = '/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/tfrecordskeplerdr25-dv_g2001-l201_spline_' \
              'nongapped_flux-loe-lwks-centroid-centroid_fdl-6stellar-bfap-ghost-rollingband_data/' \
              'tfrecordskeplerdr25-dv_g2001-l201_spline_nongapped_flux-loe-lwks-centroid-centroid_fdl-6stellar-bfap-' \
              'ghost-rollingband_starshuffle_experiment-labels-norm_withcentrmedind_std'
srcTfrecFiles = [os.path.join(srcTfrecDir, file) for file in os.listdir(srcTfrecDir) if 'shard' in file]
srcTfrecFiles = [file for file in srcTfrecFiles if not file.endswith('.csv')]
destTfrecDir = srcTfrecDir + '_diffimg_koot_coff-mes-wksmaxmes'
os.makedirs(destTfrecDir, exist_ok=True)
tceIdentifier = 'tce_plnt_num'
for srcTfrecFile in srcTfrecFiles:

    with tf.io.TFRecordWriter(os.path.join(destTfrecDir, srcTfrecFile.split('/')[-1])) as writer:

        tfrecord_dataset = tf.data.TFRecordDataset(srcTfrecFile)

        for string_i, string_record in enumerate(tfrecord_dataset.as_numpy_iterator()):

            example = tf.train.Example()
            example.ParseFromString(string_record)

            targetIdTfrec = example.features.feature['target_id'].int64_list.value[0]
            tceIdentifierTfrec = example.features.feature[tceIdentifier].int64_list.value[0]
            tceScalarParams = np.array(example.features.feature['scalar_params'].float_list.value)

            tceFound = tceTbl.loc[(tceTbl['target_id'] == targetIdTfrec) &
                                  (tceTbl['tce_plnt_num'] == tceIdentifierTfrec)]

            for feature in features:
                tceScalarParams = np.append(tceScalarParams, tceFound[feature])

            example_util.set_float_feature(example, 'scalar_params', tceScalarParams, allow_overwrite=True)

            writer.write(example.SerializeToString())

#%% Plot processed data (views, scalar parameters...)

plt.switch_backend('agg')

tfrecDir = '/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/tfrecordskeplerdr25-dv_g301-l31_6tr_spline_nongapped_flux-loe-lwks-centroid-centroid_fdl-6stellar-bfap-ghost-rollingband-stdtimeseries_secsymphase_wksnorm_maxflux-wks_correctprimarygapping_data/tfrecordskeplerdr25-dv_g301-l31_6tr_spline_nongapped_flux-loe-lwks-centroid-centroidfdl-6stellar-bfap-ghost-rollingband_secsymphase_wksnorm_maxflux-wks_correctprimarygapping_starshuffle_experiment-labels-norm_nopps_ptempstat-albedostat-wstdepth-fwmstat'
# plotDir = '/home/msaragoc/Projects/Kepler-TESS_exoplanet/Data/tfrecords/Kepler/Q1-Q17_DR25/' \
#           'test_set_misclassified_configK_nopps_CertifiedFPs_10-26-2020'
plotDir = os.path.join(tfrecDir, 'plot_strange_secondary_11-20-2020')  # , 'Confirmed_testset')
os.makedirs(plotDir, exist_ok=True)
tfrecFiles = [os.path.join(tfrecDir, file) for file in os.listdir(tfrecDir) if 'shard' in file]

# rankingTbl = pd.read_csv('/home/msaragoc/Projects/Kepler-TESS_exoplanet/Analysis/misclassified_analysis/configK_nopps_secsymphase_11-2-2020/ranking_Confirmed KOI_testset.csv')[-14:]
tce_list = ['9651668-1', '6922244-1', '11446443-1', '10874614-1', '10619192-1']  # (8416523, 2)

tceIdentifier = 'tce_plnt_num'
views = [
    'global_flux_view',
    'local_flux_view',
    'global_flux_view_fluxnorm',
    'local_flux_view_fluxnorm',
    # 'global_flux_odd_view',
    'local_flux_odd_view',
    'local_flux_odd_view_fluxnorm',
    # 'global_flux_even_view',
    'local_flux_even_view',
    'local_flux_even_view_fluxnorm',
    # 'local_flux_oddeven_view_diff',
    # 'local_flux_oddeven_view_diff_dir',
    # 'global_weak_secondary_view',
    'local_weak_secondary_view',
    'local_weak_secondary_view_selfnorm',
    'local_weak_secondary_view_fluxnorm',
    'local_weak_secondary_view_max_flux-wks_norm',
    'global_centr_view',
    'local_centr_view',
    # 'global_centr_view_std_clip',
    # 'local_centr_view_std_clip',
    'global_centr_view_std_noclip',
    'local_centr_view_std_noclip',
    # 'global_centr_view_medind_std',
    # 'local_centr_view_medind_std',
    # 'global_centr_view_medcmaxn',
    # 'local_centr_view_medcmaxn',
    # 'global_centr_view_medcmaxn_dir',
    # 'local_centr_view_medcmaxn_dir',
    # 'global_centr_view_medn',
    # 'local_centr_view_medn',
    # 'global_centr_fdl_view',
    # 'local_centr_fdl_view',
    # 'global_centr_fdl_view_norm',
    # 'local_centr_fdl_view_norm',
]

# scalarParams = {
#     'tce_steff': 0,
#     'tce_slogg': 1,
#     'tce_smet': 2,
#     'tce_sradius': 3,
#     # 'wst_robstat': 4,
#     # 'wst_depth': 5,
#     # 'tce_bin_oedp_stat': 6,
#     'boot_fap': 7,
#     'tce_smass': 8,  # 4
#     'tce_sdens': 9,  # 5
#     'tce_cap_stat': 10,
#     'tce_hap_stat': 11,
#     'tce_rb_tcount0': 12
# }

scalarParams = [
    'tce_period',
    'tce_duration',
    'transit_depth',
    'tce_max_mult_ev',
    # 'tce_model_snr',
    # 'tce_maxmesd',
    'wst_depth',
    'tce_maxmes',
    # 'tce_ptemp',
    'tce_ptemp_stat',
    # 'tce_albedo',
    'tce_albedo_stat',
    'tce_fwm_stat',
    'tce_dicco_msky',
    'tce_dicco_msky_err',
    'tce_dikco_msky',
    'tce_dikco_msky_err',
    'tce_steff',
    'tce_slogg',
    'tce_smet',
    'tce_sradius',
    'boot_fap',
    'tce_smass',
    'tce_sdens',
    'tce_cap_stat',
    'tce_hap_stat',
    'tce_rb_tcount0',
]

scheme = (3, 6)
basename = 'views'  # basename for figures

for tfrecFile in tfrecFiles:

    tfrecord_dataset = tf.data.TFRecordDataset(tfrecFile)

    for string_record in tfrecord_dataset.as_numpy_iterator():

        example = tf.train.Example()
        example.ParseFromString(string_record)

        tceIdentifierTfrec = example.features.feature[tceIdentifier].int64_list.value[0]
        targetIdTfrec = example.features.feature['target_id'].int64_list.value[0]

        # if len(rankingTbl.loc[(rankingTbl['target_id'] == targetIdTfrec) &
        #                       (rankingTbl[tceIdentifier] == tceIdentifierTfrec)]) == 0:
        #     continue
        # if tce != (targetIdTfrec, tceIdentifierTfrec):
        #     continue
        if f'{targetIdTfrec}-{tceIdentifierTfrec}' not in tce_list:
            continue

        # scalarParamsTfrec = np.array(example.features.feature['scalar_params'].float_list.value)
        scalarParamsStr = ''
        for scalarParam_i, scalarParam in enumerate(scalarParams):
            if scalarParam in ['tce_rb_tcount0', 'tce_steff']:  # , 'tce_steff']:
                scalarParamNonNormalized = np.array(example.features.feature[scalarParam].int64_list.value)[0]
            else:
                scalarParamNonNormalized = np.array(example.features.feature[scalarParam].float_list.value)[0]
            scalarParamNormalized = np.array(example.features.feature['{}_norm'.format(scalarParam)].float_list.value)[0]
            if scalarParam_i % 5 == 0:
                scalarParamsStr += '\n'
            if scalarParam in ['boot_fap']:
                scalarParamsStr += '{}={:.4f}({:.4E}) '.format(scalarParam,
                                                               scalarParamNormalized,
                                                               scalarParamNonNormalized)
            elif scalarParam in ['tce_rb_tcount0', 'tce_steff', 'transit_depth', 'tce_max_mult_ev',
                                 'tce_model_snr', 'tce_ptemp']:
                scalarParamsStr += '{}={:.4f}({:.0f})  '.format(scalarParam,
                                                            scalarParamNormalized,
                                                            scalarParamNonNormalized)
            else:
                scalarParamsStr += '{}={:.4f}({:.4f})  '.format(scalarParam,
                                                                scalarParamNormalized,
                                                                scalarParamNonNormalized)

        labelTfrec = example.features.feature['label'].bytes_list.value[0].decode("utf-8")

        viewsDict = {}
        for view in views:
            viewsDict[view] = np.array(example.features.feature[view].float_list.value)

        f, ax = plt.subplots(scheme[0], scheme[1], figsize=(22, 12))
        k = 0
        views_list = list(viewsDict.keys())
        for i in range(scheme[0]):
            for j in range(scheme[1]):
                if k < len(views_list):
                    ax[i, j].plot(viewsDict[views_list[k]])
                    ax[i, j].scatter(np.arange(len(viewsDict[views_list[k]])), viewsDict[views_list[k]], s=10,
                                     color='k', alpha=0.2)
                    ax[i, j].set_title(views_list[k], pad=20)
                if i == scheme[0] - 1:
                    ax[i, j].set_xlabel('Bin number')
                if j == 0:
                    ax[i, j].set_ylabel('Amplitude')
                k += 1

        f.suptitle('TCE {}-{} {}\n{}'.format(targetIdTfrec, tceIdentifierTfrec, labelTfrec, scalarParamsStr))
        plt.subplots_adjust(top=0.82,
                            bottom=0.067,
                            left=0.055,
                            right=0.992,
                            hspace=0.442,
                            wspace=0.394)
        plt.savefig(os.path.join(plotDir, '{}_{}_{}_{}.png'.format(targetIdTfrec, tceIdentifierTfrec, labelTfrec,
                                                                   basename)))
        # aaaa
        plt.close()

#%% Add scalar features to the TFRecords

features = {
    # 'tce_dikco_msky': {'missing_value': 0, 'log_transform': False, 'log_transform_eps': np.nan,
    #                    'clip_factor': 20},
    # 'tce_dikco_msky_err': {'missing_value': -1, 'log_transform': False, 'log_transform_eps': np.nan,
    #                        'clip_factor': 20},
    # 'tce_dicco_msky': {'missing_value': 0, 'log_transform': False, 'log_transform_eps': np.nan,
    #                    'clip_factor': 20},
    # 'tce_dicco_msky_err': {'missing_value': -1, 'log_transform': False, 'log_transform_eps': np.nan,
    #                        'clip_factor': 20},
    'tce_fwm_stat': {'missing_value': np.nan, 'log_transform': False, 'log_transform_eps': np.nan, 'clip_factor': 20},
    # 'tce_max_mult_ev': {'missing_value': np.nan, 'log_transform': False, 'log_transform_eps': np.nan,
    #                     'clip_factor': 200},
    # 'tce_maxmes': {'missing_value': 0, 'log_transform': False, 'log_transform_eps': np.nan,
    #                'clip_factor': 20},
    # 'tce_albedo': {'missing_value': 0, 'log_transform': False, 'log_transform_eps': np.nan,
    #                'clip_factor': 20},
    # 'tce_albedo_stat': {'missing_value': 0, 'log_transform': False, 'log_transform_eps': np.nan, 'clip_factor': 20},
    # 'tce_ptemp': {'missing_value': 0, 'log_transform': False, 'log_transform_eps': np.nan,
    #               'clip_factor': 20},
    # 'tce_ptemp_stat': {'missing_value': 0, 'log_transform': False, 'log_transform_eps': np.nan, 'clip_factor': 20},
    'wst_depth': {'missing_value': 0, 'log_transform': False, 'log_transform_eps': np.nan, 'clip_factor': 20,
                  'replace_value': 0},
    # 'transit_depth': {'missing_value': np.nan, 'log_transform': False, 'log_transform_eps': np.nan,
    #                   'clip_factor': 20},
    # 'tce_depth_err': {'missing_value': -1, 'log_transform': False, 'log_transform_eps': np.nan,
    #                   'clip_factor': 20},
    # 'tce_duration': {'missing_value': np.nan, 'log_transform': False, 'log_transform_eps': np.nan,
    #                      'clip_factor': np.nan},
    # 'tce_duration_err': {'missing_value': -1, 'log_transform': False, 'log_transform_eps': np.nan,
    #                      'clip_factor': 20},
    # 'tce_period': {'missing_value': np.nan, 'log_transform': False, 'log_transform_eps': np.nan,
    #                  'clip_factor': np.nan},
    # 'tce_period_err': {'missing_value': -1, 'log_transform': False, 'log_transform_eps': np.nan,
    #                      'clip_factor': 20},
    # 'tce_steff': {'missing_value': np.nan, 'log_transform': False, 'log_transform_eps': np.nan,
    #                    'clip_factor': np.nan},
    # 'tce_sradius': {'missing_value': -1, 'log_transform': False, 'log_transform_eps': np.nan,
    #                    'clip_factor': np.nan},
    # 'tce_sdens': {'missing_value': np.nan, 'log_transform': False, 'log_transform_eps': np.nan,
    #                    'clip_factor': np.nan},
    # 'tce_slogg': {'missing_value': np.nan, 'log_transform': False, 'log_transform_eps': np.nan,
    #                    'clip_factor': np.nan},
    # 'tce_smet': {'missing_value': np.nan, 'log_transform': False, 'log_transform_eps': np.nan,
    #                    'clip_factor': np.nan},
    # 'tce_smass': {'missing_value': -1, 'log_transform': False, 'log_transform_eps': np.nan,
    #                    'clip_factor': np.nan},
    # 'tce_cap_stat': {'missing_value': np.nan, 'log_transform': False, 'log_transform_eps': np.nan,
    #                    'clip_factor': 20},
    # 'tce_hap_stat': {'missing_value': np.nan, 'log_transform': False, 'log_transform_eps': np.nan,
    #                    'clip_factor': 20},
    # 'boot_fap': {'missing_value': -1, 'log_transform': True, 'log_transform_eps': 1e-32,
    #                    'clip_factor': np.nan},
    # 'tce_rb_tcount0': {'missing_value': np.nan, 'log_transform': False, 'log_transform_eps': np.nan,
    #                    'clip_factor': np.nan},
}
featuresList = list(features.keys())

trainsetTbl = pd.read_csv('/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/train-val-test-sets/trainset.csv')
print('Number of TCEs in the training set: {}'.format(len(trainsetTbl)))

tceTbl = pd.read_csv('/data5/tess_project/Data/Ephemeris_tables/Kepler/Q1-Q17_DR25/q1_q17_dr25_tce_2020.09.28_10.36.22_stellar_koi_cfp_norobovetterlabels_renamedcols_nomissingval_rmcandandfpkois_norogues.csv')
print('Number of TCEs in the dataset: {}'.format(len(tceTbl)))

tceTbl['UID'] = tceTbl[['target_id', 'tce_plnt_num']].apply(lambda x: '{}-{}'.format(x['target_id'], x['tce_plnt_num']),
                                                            axis=1)
trainsetTbl['UID'] = trainsetTbl[['target_id', 'tce_plnt_num']].apply(lambda x:
                                                                      '{}-{}'.format(x['target_id'], x['tce_plnt_num']),
                                                                      axis=1)

# get only training set TCEs
trainTceTbl = tceTbl[tceTbl['UID'].isin(trainsetTbl['UID'])][featuresList]

assert len(trainTceTbl) == len(trainsetTbl)

# compute training statistics for standardization
for feature in features:

    # remove missing values
    if not np.isnan(features[feature]['missing_value']):
        if feature == 'wst_depth':
            values = trainTceTbl.loc[trainTceTbl[feature] >= features[feature]['missing_value'], feature].values
        else:
            values = trainTceTbl.loc[trainTceTbl[feature] != features[feature]['missing_value'], feature].values
    else:
        values = trainTceTbl[feature].values

    # log transform
    if features[feature]['log_transform']:
        values += features[feature]['log_transform_eps']
        values = np.log10(values)

    features[feature].update({
        'median': np.median(values),
        'mad_std': mad_std(values)
    })


def _standardize(row, normStats):
    """ Standardize parameters for a TCE.

    :param row: pandas Series, TCE parameters
    :param normStats: dict, normalization statistics
    :return:
        pandas Series with TCE parameters standardized
    """

    for feature in row.index:

        # TODO: make these checks more flexible and clearer
        # replace missing values
        if feature == 'wst_depth' and row[feature] <= normStats[feature]['missing_value']:  # special case for wst_depth
            row[feature] = normStats[feature]['replace_value']
        elif row[feature] == normStats[feature]['missing_value']:  # by central tendency estimate of the training set
            row[feature] = normStats[feature]['median']
        else:
            # log transform the data
            if normStats[feature]['log_transform']:

                # add constant value to deal with zero for log transform
                if not np.isnan(normStats[feature]['log_transform_eps']):
                    row[feature] += normStats[feature]['log_transform_eps']

                row[feature] = np.log10(row[feature])

            # clipping the data between median +- clip_factor * mad_std
            if not np.isnan(normStats[feature]['clip_factor']):
                row[feature] = np.clip([row[feature]],
                                       normStats[feature]['median'] -
                                       normStats[feature]['clip_factor'] * normStats[feature]['mad_std'],
                                       normStats[feature]['median'] +
                                       normStats[feature]['clip_factor'] * normStats[feature]['mad_std']
                                       )[0]

        # standardization
        row[feature] -= normStats[feature]['median']
        row[feature] /= normStats[feature]['mad_std']

    return row


# standardize for all the data set
tceTbl[featuresList] = tceTbl[featuresList].apply(_standardize, args=(features,), axis=1)

# source TFRecords
srcTfrecDir = '/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/tfrecordskeplerdr25-dv_g301-l31_6tr_spline_nongapped_flux-loe-lwks-centroid-centroid_fdl-6stellar-bfap-ghost-rollingband-stdtimeseries_secsymphase_wksnorm_maxflux-wks_correctprimarygapping_data/tfrecordskeplerdr25-dv_g301-l31_6tr_spline_nongapped_flux-loe-lwks-centroid-centroid_fdl-6stellar-bfap-ghost-rollingband-stdtimeseries_secsymphase_wksnorm_maxflux-wks_correctprimarygapping_starshuffle_experiment-labels-norm_nopps_ptemp_stat-albedo_stat'
srcTfrecFiles = [os.path.join(srcTfrecDir, file) for file in os.listdir(srcTfrecDir) if 'shard' in file]
srcTfrecFiles = [file for file in srcTfrecFiles if not file.endswith('.csv')]

# define TFRecords destination
# destTfrecDir = srcTfrecDir + '_wst_depth-fwm_stat'
destTfrecDir = '/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/tfrecordskeplerdr25-dv_g301-l31_6tr_spline_nongapped_flux-loe-lwks-centroid-centroid_fdl-6stellar-bfap-ghost-rollingband-stdtimeseries_secsymphase_wksnorm_maxflux-wks_correctprimarygapping_data/tfrecordskeplerdr25-dv_g301-l31_6tr_spline_nongapped_flux-loe-lwks-centroid-centroidfdl-6stellar-bfap-ghost-rollingband_secsymphase_wksnorm_maxflux-wks_correctprimarygapping_starshuffle_experiment-labels-norm_nopps_ptempstat-albedostat-wstdepth-fwmstat'
os.makedirs(destTfrecDir, exist_ok=True)

tceIdentifier = 'tce_plnt_num'

# add standardized features to the source TFRecords and write them to the destination
for srcTfrecFile in srcTfrecFiles:

    with tf.io.TFRecordWriter(os.path.join(destTfrecDir, srcTfrecFile.split('/')[-1])) as writer:

        tfrecord_dataset = tf.data.TFRecordDataset(srcTfrecFile)

        for string_i, string_record in enumerate(tfrecord_dataset.as_numpy_iterator()):

            example = tf.train.Example()
            example.ParseFromString(string_record)

            targetIdTfrec = example.features.feature['target_id'].int64_list.value[0]
            tceIdentifierTfrec = example.features.feature[tceIdentifier].int64_list.value[0]
            tceScalarParams = np.array(example.features.feature['scalar_params'].float_list.value)

            tceFound = tceTbl.loc[(tceTbl['target_id'] == targetIdTfrec) &
                                  (tceTbl['tce_plnt_num'] == tceIdentifierTfrec)]

            # for feature in featuresList:
            #     tceScalarParams = np.append(tceScalarParams, tceFound[feature])

            # example_util.set_float_feature(example, 'scalar_params', tceScalarParams, allow_overwrite=True)

            for feature in featuresList:
                example_util.set_float_feature(example, '{}_norm'.format(feature), tceFound[feature].values,
                                               allow_overwrite=True)

            writer.write(example.SerializeToString())

#%% Get wst_depth and fwm_stat scalar features from the dataset

tfrecdir = Path('/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/tfrecordskeplerdr25-dv_g301-l31_6tr_spline_nongapped_flux-loe-lwks-centroid-centroid_fdl-6stellar-bfap-ghost-rollingband-stdtimeseries_secsymphase_wksnorm_maxflux-wks_correctprimarygapping_data/tfrecordskeplerdr25-dv_g301-l31_6tr_spline_nongapped_flux-loe-lwks-centroid-centroidfdl-6stellar-bfap-ghost-rollingband_secsymphase_wksnorm_maxflux-wks_correctprimarygapping_starshuffle_experiment-labels-norm_nopps_ptempstat-albedostat-wstdepth-fwmstat')
tfrecfiles = [el for el in list(tfrecdir.iterdir()) if el.is_file()]

scalar_params = {
    'wst_depth_norm': [],
    'tce_fwm_stat_norm': [],
    'transit_depth_norm': []
                 }

# add standardized features to the source TFRecords and write them to the destination
for tfrecfile in tfrecfiles:

    tfrecord_dataset = tf.data.TFRecordDataset(str(tfrecfile))

    for string_i, string_record in enumerate(tfrecord_dataset.as_numpy_iterator()):

        example = tf.train.Example()
        example.ParseFromString(string_record)

        for scalar_param in scalar_params:
            scalar_param_ex = example.features.feature[scalar_param].float_list.value[0]
            scalar_params[scalar_param].append(scalar_param_ex)

#%% And then plot their histograms

plotdir = tfrecdir / 'plot_features'
plotdir.mkdir(exist_ok=True)

scalar_params_hist = {
    'wst_depth_norm': {
        'bins': np.linspace(-20, 20, 50, endpoint=True),
        'log': False,
        'stats': {
            'median': np.nan,
            'mean': np.nan,
            'std_rob': np.nan,
            'std': np.nan,
            'min': np.nan,
            'max': np.nan
        }
    },
    'tce_fwm_stat_norm': {
        'bins': np.linspace(-20, 20, 50, endpoint=True),
        'log': False,
        'stats': {
            'median': np.nan,
            'mean': np.nan,
            'std_rob': np.nan,
            'std': np.nan,
            'min': np.nan,
            'max': np.nan
        }
    },
    'transit_depth_norm': {
        'bins': np.linspace(-20, 20, 50, endpoint=True),
        'log': False,
        'stats': {
            'median': np.nan,
            'mean': np.nan,
            'std_rob': np.nan,
            'std': np.nan,
            'min': np.nan,
            'max': np.nan
        }
    }
}

for scalar_param in scalar_params:
    scalar_params_hist[scalar_param]['stats']['median'] = np.median(scalar_params[scalar_param])
    scalar_params_hist[scalar_param]['stats']['mean'] = np.mean(scalar_params[scalar_param])
    scalar_params_hist[scalar_param]['stats']['std_rob'] = mad_std(scalar_params[scalar_param])
    scalar_params_hist[scalar_param]['stats']['std'] = np.std(scalar_params[scalar_param], ddof=1)
    scalar_params_hist[scalar_param]['stats']['min'] = np.min(scalar_params[scalar_param])
    scalar_params_hist[scalar_param]['stats']['max'] = np.max(scalar_params[scalar_param])

for scalar_param in scalar_params:

    f, ax = plt.subplots()
    ax.hist(scalar_params[scalar_param],
            bins=scalar_params_hist[scalar_param]['bins'], edgecolor='k')
    ax.axvline(x= scalar_params_hist[scalar_param]['stats']['median'], c='y', label='median')
    ax.axvline(x= scalar_params_hist[scalar_param]['stats']['median'] +
                  scalar_params_hist[scalar_param]['stats']['std_rob'], c='g', label='med+std_rob')
    ax.axvline(x= scalar_params_hist[scalar_param]['stats']['median'] -
                  scalar_params_hist[scalar_param]['stats']['std_rob'], c='b', label='med-std_rob')
    ax.set_ylabel('Counts')
    ax.set_xlabel('{}'.format(scalar_param))
    ax.set_title('Mean: {:.2f} | Median: {:.2f} |\n Std: {:.2f} | Std Rob: {:.2f} | Min: {:.2f} '
                 '|Max: {:.2f}'.format(scalar_params_hist[scalar_param]['stats']['mean'],
                                       scalar_params_hist[scalar_param]['stats']['median'],
                                       scalar_params_hist[scalar_param]['stats']['std'],
                                       scalar_params_hist[scalar_param]['stats']['std_rob'],
                                       scalar_params_hist[scalar_param]['stats']['min'],
                                       scalar_params_hist[scalar_param]['stats']['max']
                                       )
                 )
    ax.set_xlim([scalar_params_hist[scalar_param]['bins'][0], scalar_params_hist[scalar_param]['bins'][-1]])
    ax.legend()
    if scalar_params_hist[scalar_param]['log']:
        ax.set_yscale('log')
    f.savefig(plotdir / f'{scalar_param}.png')
    plt.close()

#%% add normalized secondary max MES, albedo stat, planet eff temp stat, and secondary depth features to the TFRecords
# already normalized

features = {
    'tce_maxmes': {'missing_value': 0, 'log_transform': False, 'log_transform_eps': np.nan,
                   'clip_factor': 20},
    # 'tce_albedo': {'missing_value': 0, 'log_transform': False, 'log_transform_eps': np.nan,
    #                'clip_factor': 20},
    # 'tce_albedo_err': {'missing_value': -1, 'log_transform': False, 'log_transform_eps': np.nan,
    #                'clip_factor': 20},
    'tce_albedo_stat': {'missing_value': 0, 'log_transform': False, 'log_transform_eps': np.nan, 'clip_factor': 20},
    # 'tce_ptemp': {'missing_value': 0, 'log_transform': False, 'log_transform_eps': np.nan,
    #               'clip_factor': 20},
    # 'tce_ptemp_err': {'missing_value': 0, 'log_transform': False, 'log_transform_eps': np.nan,
    #               'clip_factor': 20},
    # 'tce_prad': {'missing_value': 0, 'log_transform': False, 'log_transform_eps': np.nan,
    #               'clip_factor': 20},
    # 'tce_prad_err': {'missing_value': 0, 'log_transform': False, 'log_transform_eps': np.nan,
    #                   'clip_factor': 20},
    'tce_ptemp_stat': {'missing_value': 0, 'log_transform': False, 'log_transform_eps': np.nan, 'clip_factor': 20},
    'wst_depth': {'missing_value': 0, 'log_transform': False, 'log_transform_eps': np.nan, 'clip_factor': 20,
                  'replace_value': 0},
}

featuresList = list(features.keys())

trainsetTbl = pd.read_csv('/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/train-val-test-sets/trainset.csv')
print('Number of TCEs in the training set: {}'.format(len(trainsetTbl)))

tceTbl = pd.read_csv('/data5/tess_project/Data/Ephemeris_tables/Kepler/Q1-Q17_DR25/q1_q17_dr25_tce_2020.09.28_10.36.22_stellar_koi_cfp_norobovetterlabels_renamedcols_nomissingval_symsecphase_confirmedkoiperiod_sec.csv')
# remove rogue TCEs
tceTbl = tceTbl.loc[tceTbl['tce_rogue_flag'] == 0]
# remove PP KOIs
tceTbl = tceTbl.loc[~((tceTbl['fpwg_disp_status'] == 'POSSIBLE PLANET') & (tceTbl['koi_disposition'] != 'CONFIRMED'))]
print('Number of TCEs in the dataset: {}'.format(len(tceTbl)))

sec_match_tbl_fp = "/home/msaragoc/Projects/Kepler-TESS_exoplanet/Analysis/wks_tce_match/match_tces_['max_period_diff', 'max_phase_diff_rel']_[0.005, 0.01]_23-10-20_12:54_final.csv"
sec_match_tbl = pd.read_csv(sec_match_tbl_fp)

tceTbl['UID'] = tceTbl[['target_id', 'tce_plnt_num']].apply(lambda x: '{}-{}'.format(x['target_id'], x['tce_plnt_num']),
                                                            axis=1)
trainsetTbl['UID'] = trainsetTbl[['target_id', 'tce_plnt_num']].apply(lambda x:
                                                                      '{}-{}'.format(x['target_id'], x['tce_plnt_num']),
                                                                      axis=1)

# get only training set TCEs
trainTceTbl = tceTbl[tceTbl['UID'].isin(trainsetTbl['UID'])][featuresList]

# assert len(trainTceTbl) == len(trainsetTbl)

# compute training statistics for standardization
for feature in ['tce_maxmes', 'wst_depth', 'tce_albedo_stat', 'tce_ptemp_stat']:

    # remove missing values
    if not np.isnan(features[feature]['missing_value']):
        if feature == 'wst_depth':
            values = trainTceTbl.loc[trainTceTbl[feature] >= features[feature]['missing_value'], feature].values
        else:
            values = trainTceTbl.loc[trainTceTbl[feature] != features[feature]['missing_value'], feature].values
    else:
        values = trainTceTbl[feature].values

    # log transform
    if features[feature]['log_transform']:
        values += features[feature]['log_transform_eps']
        values = np.log10(values)

    features[feature].update({
        'median': np.median(values),
        'mad_std': mad_std(values)
    })


def _standardize(row, normStats):
    """ Standardize parameters for a TCE.

    :param row: pandas Series, TCE parameters
    :param normStats: dict, normalization statistics
    :return:
        pandas Series with TCE parameters standardized
    """

    for feature in row.index:

        # TODO: make these checks more flexible and clearer
        # replace missing values
        if feature == 'wst_depth' and row[feature] <= normStats[feature]['missing_value']:  # special case for wst_depth
            row[feature] = normStats[feature]['replace_value']
        elif row[feature] == normStats[feature]['missing_value']:  # by central tendency estimate of the training set

            row[feature] = normStats[feature]['median']
        else:
            # log transform the data
            if normStats[feature]['log_transform']:

                # add constant value to deal with zero for log transform
                if not np.isnan(normStats[feature]['log_transform_eps']):
                    row[feature] += normStats[feature]['log_transform_eps']

                row[feature] = np.log10(row[feature])

            # clipping the data between median +- clip_factor * mad_std
            if not np.isnan(normStats[feature]['clip_factor']):
                row[feature] = np.clip([row[feature]],
                                       normStats[feature]['median'] -
                                       normStats[feature]['clip_factor'] * normStats[feature]['mad_std'],
                                       normStats[feature]['median'] +
                                       normStats[feature]['clip_factor'] * normStats[feature]['mad_std']
                                       )[0]

        # standardization
        row[feature] -= normStats[feature]['median']
        row[feature] /= normStats[feature]['mad_std']

    return row


tceTbl_nonnorm = tceTbl.copy(deep=True)

# standardize for all the data set
tceTbl[featuresList] = tceTbl[featuresList].apply(_standardize, args=(features,), axis=1)

tfrec_src_dir_fp = '/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/tfrecordskeplerdr25-dv_g301-l31_6tr_spline_nongapped_flux-loe-lwks-centroid-centroidfdl-6stellar-bfap-ghost-rollband-stdts_secsymphase_correctprimarygapping_confirmedkoiperiod_data/tfrecordskeplerdr25-dv_g301-l31_6tr_spline_nongapped_flux-loe-lwks-centroid-centroidfdl-6stellar-bfap-ghost-rollband-stdts_secsymphase_correctprimarygapping_confirmedkoiperiod_starshuffle_experiment-labels-norm_nopps'

tfrec_src_dir = Path(tfrec_src_dir_fp)

src_tfrecs = [file_path for file_path in tfrec_src_dir.iterdir() if 'shard' in file_path.stem]

# define TFRecords destination
tfrec_dest_dir = tfrec_src_dir.parent / (tfrec_src_dir.stem + '_secparams')
tfrec_dest_dir.mkdir(exist_ok=True)

tce_identifier = 'tce_plnt_num'

# add standardized features to the source TFRecords and write them to the destination
for src_tfrec in src_tfrecs:

    with tf.io.TFRecordWriter(str(tfrec_dest_dir / src_tfrec.stem)) as writer:

        tfrecord_dataset = tf.data.TFRecordDataset(str(src_tfrec))

        for string_i, string_record in enumerate(tfrecord_dataset.as_numpy_iterator()):

            example = tf.train.Example()
            example.ParseFromString(string_record)

            target_id = example.features.feature['target_id'].int64_list.value[0]
            tce_id = example.features.feature[tce_identifier].int64_list.value[0]

            tceFound = tceTbl.loc[(tceTbl['target_id'] == target_id) &
                                  (tceTbl['tce_plnt_num'] == tce_id)]

            tceFound_nonnorm = tceTbl_nonnorm.loc[(tceTbl_nonnorm['target_id'] == target_id) &
                                                 (tceTbl_nonnorm['tce_plnt_num'] == tce_id)]

            for feature in featuresList:
                example_util.set_float_feature(example, '{}'.format(feature), tceFound_nonnorm[feature].values,
                                               allow_overwrite=True)
                example_util.set_float_feature(example, '{}_norm'.format(feature), tceFound[feature].values,
                                               allow_overwrite=True)

            writer.write(example.SerializeToString())

#%% check TFRecords

tfrec_src_dir_fp = '/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/tfrecordskeplerdr25-dv_g301-l31_6tr_spline_nongapped_flux-loe-lwks-centroid-centroidfdl-6stellar-bfap-ghost-rollband-stdts_secsymphase_correctprimarygapping_confirmedkoiperiod_data/tfrecordskeplerdr25-dv_g301-l31_6tr_spline_nongapped_flux-loe-lwks-centroid-centroidfdl-6stellar-bfap-ghost-rollband-stdts_secsymphase_correctprimarygapping_confirmedkoiperiod_starshuffle_experiment-labels-norm_nopps_secparams'

tfrec_src_dir = Path(tfrec_src_dir_fp)

src_tfrecs = [file_path for file_path in tfrec_src_dir.iterdir() if 'shard' in file_path.stem]

tce_identifier = 'tce_plnt_num'

search_tces = ['3098194-2', '1434660-5', '8937762-1']

for src_tfrec in src_tfrecs:

    tfrecord_dataset = tf.data.TFRecordDataset(str(src_tfrec))

    for string_i, string_record in enumerate(tfrecord_dataset.as_numpy_iterator()):

        example = tf.train.Example()
        example.ParseFromString(string_record)

        target_id = example.features.feature['target_id'].int64_list.value[0]
        tce_id = example.features.feature[tce_identifier].int64_list.value[0]

        if f'{target_id}-{tce_id}' in search_tces:
            print(f'TCE {target_id}-{tce_id}')
            for feature in ['tce_maxmes', 'wst_depth', 'tce_albedo_stat', 'tce_ptemp_stat']:
                print(f'{feature}: {example.features.feature[feature].float_list.value[0]}|'
                      f'{example.features.feature[f"{feature}_norm"].float_list.value[0]}')

#%% ADD NORMALIZED PLANET RADIUS AND ORBITAL PERIOD

features = {
    'tce_period': {'missing_value': np.nan, 'log_transform': False, 'log_transform_eps': np.nan,
                   'clip_factor': np.nan},
    'tce_prad': {'missing_value': np.nan, 'log_transform': False, 'log_transform_eps': np.nan,
                 'clip_factor': 20},
}

featuresList = list(features.keys())

trainsetTbl = pd.read_csv('/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/train-val-test-sets/trainset.csv')
print('Number of TCEs in the training set: {}'.format(len(trainsetTbl)))

tceTbl = pd.read_csv('/data5/tess_project/Data/Ephemeris_tables/Kepler/Q1-Q17_DR25/q1_q17_dr25_tce_2020.09.28_10.36.22_stellar_koi_cfp_norobovetterlabels_renamedcols_nomissingval_symsecphase_confirmedkoiperiod_sec.csv')
# remove rogue TCEs
tceTbl = tceTbl.loc[tceTbl['tce_rogue_flag'] == 0]
# remove PP KOIs
tceTbl = tceTbl.loc[~((tceTbl['fpwg_disp_status'] == 'POSSIBLE PLANET') & (tceTbl['koi_disposition'] != 'CONFIRMED'))]
print('Number of TCEs in the dataset: {}'.format(len(tceTbl)))

tceTbl['UID'] = tceTbl[['target_id', 'tce_plnt_num']].apply(lambda x: '{}-{}'.format(x['target_id'], x['tce_plnt_num']),
                                                            axis=1)
trainsetTbl['UID'] = trainsetTbl[['target_id', 'tce_plnt_num']].apply(lambda x:
                                                                      '{}-{}'.format(x['target_id'], x['tce_plnt_num']),
                                                                      axis=1)

# get only training set TCEs
trainTceTbl = tceTbl[tceTbl['UID'].isin(trainsetTbl['UID'])][featuresList]

# assert len(trainTceTbl) == len(trainsetTbl)

# compute training statistics for standardization
for feature in features:

    # remove missing values
    if not np.isnan(features[feature]['missing_value']):
        if feature == 'wst_depth':
            values = trainTceTbl.loc[trainTceTbl[feature] >= features[feature]['missing_value'], feature].values
        else:
            values = trainTceTbl.loc[trainTceTbl[feature] != features[feature]['missing_value'], feature].values
    else:
        values = trainTceTbl[feature].values

    # log transform
    if features[feature]['log_transform']:
        values += features[feature]['log_transform_eps']
        values = np.log10(values)

    features[feature].update({
        'median': np.median(values),
        'mad_std': mad_std(values)
    })


def _standardize(row, normStats):
    """ Standardize parameters for a TCE.

    :param row: pandas Series, TCE parameters
    :param normStats: dict, normalization statistics
    :return:
        pandas Series with TCE parameters standardized
    """

    for feature in row.index:

        # TODO: make these checks more flexible and clearer
        # replace missing values
        if feature == 'wst_depth' and row[feature] <= normStats[feature]['missing_value']:  # special case for wst_depth
            row[feature] = normStats[feature]['replace_value']
        elif row[feature] == normStats[feature]['missing_value']:  # by central tendency estimate of the training set

            row[feature] = normStats[feature]['median']
        else:
            # log transform the data
            if normStats[feature]['log_transform']:

                # add constant value to deal with zero for log transform
                if not np.isnan(normStats[feature]['log_transform_eps']):
                    row[feature] += normStats[feature]['log_transform_eps']

                row[feature] = np.log10(row[feature])

            # clipping the data between median +- clip_factor * mad_std
            if not np.isnan(normStats[feature]['clip_factor']):
                row[feature] = np.clip([row[feature]],
                                       normStats[feature]['median'] -
                                       normStats[feature]['clip_factor'] * normStats[feature]['mad_std'],
                                       normStats[feature]['median'] +
                                       normStats[feature]['clip_factor'] * normStats[feature]['mad_std']
                                       )[0]

        # standardization
        row[feature] -= normStats[feature]['median']
        row[feature] /= normStats[feature]['mad_std']

    return row


tceTbl_nonnorm = tceTbl.copy(deep=True)

# standardize for all the data set
tceTbl[featuresList] = tceTbl[featuresList].apply(_standardize, args=(features,), axis=1)

tfrec_src_dir_fp = '/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/tfrecordskeplerdr25-dv_g301-l31_6tr_spline_nongapped_flux-loe-lwks-centroid-centroidfdl-6stellar-bfap-ghost-rollband-stdts_secsymphase_correctprimarygapping_confirmedkoiperiod_data/tfrecordskeplerdr25-dv_g301-l31_6tr_spline_nongapped_flux-loe-lwks-centroid-centroidfdl-6stellar-bfap-ghost-rollband-stdts_secsymphase_correctprimarygapping_confirmedkoiperiod_starshuffle_experiment-labels-norm_nopps_secparams'

tfrec_src_dir = Path(tfrec_src_dir_fp)

src_tfrecs = [file_path for file_path in tfrec_src_dir.iterdir() if 'shard' in file_path.stem]

# define TFRecords destination
tfrec_dest_dir = tfrec_src_dir.parent / (tfrec_src_dir.stem + '_prad_period')
tfrec_dest_dir.mkdir(exist_ok=True)

tce_identifier = 'tce_plnt_num'

# add standardized features to the source TFRecords and write them to the destination
for src_tfrec in src_tfrecs:

    with tf.io.TFRecordWriter(str(tfrec_dest_dir / src_tfrec.stem)) as writer:

        tfrecord_dataset = tf.data.TFRecordDataset(str(src_tfrec))

        for string_i, string_record in enumerate(tfrecord_dataset.as_numpy_iterator()):

            example = tf.train.Example()
            example.ParseFromString(string_record)

            target_id = example.features.feature['target_id'].int64_list.value[0]
            tce_id = example.features.feature[tce_identifier].int64_list.value[0]

            tceFound = tceTbl.loc[(tceTbl['target_id'] == target_id) &
                                  (tceTbl['tce_plnt_num'] == tce_id)]

            for feature in featuresList:
                # example_util.set_float_feature(example, '{}'.format(feature), tceFound_nonnorm[feature].values,
                #                                allow_overwrite=True)
                example_util.set_float_feature(example, '{}_norm'.format(feature), tceFound[feature].values,
                                               allow_overwrite=True)

            writer.write(example.SerializeToString())

#%% Generate TFRecords for the TCEs that are not part of the dataset built to train and evaluate our models

# set up logger
logger = logging.getLogger(name='reate_tfrecords_predict_tcesnotused')
logger_handler = logging.FileHandler(filename='/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/' \
              'tfrecordskeplerdr25-dv_g301-l31_6tr_spline_nongapped_flux-loe-lwks-centroid-centroidfdl-6stellar-bfap-ghost-rollband-stdts_secsymphase_correctprimarygapping_confirmedkoiperiod_data/'
                                              'create_tfrecords_predict_tcesnotused.log', mode='w')
logger_formatter = logging.Formatter('%(asctime)s - %(message)s')
logger.setLevel(logging.INFO)
logger_handler.setFormatter(logger_formatter)
logger.addHandler(logger_handler)
logger.info(f'Starting run...')

tce_tbl_fp = '/data5/tess_project/Data/Ephemeris_tables/Kepler/Q1-Q17_DR25/'\
             'q1_q17_dr25_tce_2020.09.28_10.36.22_stellar_koi_cfp_norobovetterlabels_renamedcols_nomissingval_symsecphase_confirmedkoiperiod_sec.csv'
tce_tbl = pd.read_csv(tce_tbl_fp)
tce_tbl['tce_duration'] = tce_tbl['tce_duration'] / 24
logger.info(f'Loaded TCE table {tce_tbl_fp}')
logger.info(f'Total number of TCEs available: {len(tce_tbl)}')

tce_tbl['UID'] = tce_tbl[['target_id', 'tce_plnt_num']].apply(lambda x: '{}-{}'.format(x['target_id'],
                                                                                       x['tce_plnt_num']),
                                                              axis=1)

trainset_tbl_fp = '/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/train-val-test-sets/trainset.csv'
trainset_tbl = pd.read_csv(trainset_tbl_fp)
logger.info(f'Loaded TCE training set table {trainset_tbl_fp}')
logger.info(f'The training set has {len(trainset_tbl)} TCEs.')
trainset_tbl['UID'] = trainset_tbl[['target_id', 'tce_plnt_num']].apply(lambda x: '{}-{}'.format(x['target_id'],
                                                                                                 x['tce_plnt_num']),
                                                                        axis=1)

# get only training set TCEs
trainset_tce_tbl = tce_tbl[tce_tbl['UID'].isin(trainset_tbl['UID'])]

# remove rogue TCEs
tce_tbl = tce_tbl.loc[tce_tbl['tce_rogue_flag'] == 0]
logger.info(f'Number of TCEs after removing rogue TCEs: {len(tce_tbl)}')
# remove CONFIRMED KOIs
tce_tbl = tce_tbl.loc[tce_tbl['koi_disposition'] != 'CONFIRMED']
logger.info(f'Number of TCEs after removing Confirmed KOI TCEs: {len(tce_tbl)}')
# remove CFP and CFA KOIs
tce_tbl = tce_tbl.loc[~((tce_tbl['fpwg_disp_status'] == 'CERTIFIED FP') |
                        (tce_tbl['fpwg_disp_status'] == 'CERTIFIED FA'))]
logger.info(f'Number of TCEs after removing CFP and CFA KOI TCEs: {len(tce_tbl)}')
# remove non-KOIs
tce_tbl = tce_tbl.loc[~(tce_tbl['kepoi_name'].isna())]
logger.info(f'Number of TCEs after removing non-KOI TCEs: {len(tce_tbl)}')

logger.info(f'Number of TCEs in the dataset: {len(tce_tbl)}')
logger.info(f'{tce_tbl["koi_disposition"].value_counts()}')
logger.info(f'{tce_tbl["fpwg_disp_status"].value_counts()}')

# CREATE TFRECORDS ONLY FOR THIS SUBSET OF TCES
numTces = len(tce_tbl)
dataset = 'predict'

tceIdentifier = 'tce_plnt_num'  # TCE identifier

# defined number of examples per shard
numTcesPerShard = min(100, numTces)
numShards = int(numTces / numTcesPerShard)

logger.info(f'Number of shards: {numShards}')

# create pairs of shard filename and shard TCE table
shardFilenameTuples = []
shardFilenameTuples.extend(list(itertools.product([dataset], range(numShards), [numShards - 1])))
shardFilenames = ['{}-shard-{:05d}-of-{:05d}'.format(*shardFilenameTuple) for shardFilenameTuple in shardFilenameTuples]

shardTuples = []
for shardFilename in shardFilenames:

    shardFilenameSplit = shardFilename.split('-')
    dataset, shard_i = shardFilenameSplit[0], int(shardFilenameSplit[2])

    if shard_i < numShards - 1:
        shardTuples.append((shardFilename,
                            tce_tbl[shard_i * numTcesPerShard:(shard_i + 1) * numTcesPerShard]))
    else:
        shardTuples.append((shardFilename,
                            tce_tbl[shard_i * numTcesPerShard:]))

checkNumTcesInDatasets = 0
for shardTuple in shardTuples:
    checkNumTcesInDatasets += len(shardTuple[1])
logger.info(f'Confirming total number of TCEs in shards: {checkNumTcesInDatasets}')

srcTfrecDir = '/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/' \
              'tfrecordskeplerdr25-dv_g301-l31_6tr_spline_nongapped_flux-loe-lwks-centroid-centroidfdl-6stellar-bfap-ghost-rollband-stdts_secsymphase_correctprimarygapping_confirmedkoiperiod_data/' \
              'tfrecordskeplerdr25-dv_g301-l31_6tr_spline_nongapped_flux-loe-lwks-centroid-centroidfdl-6stellar-bfap-ghost-rollband-stdts_secsymphase_correctprimarygapping_confirmedkoiperiod'
srcTbl = pd.read_csv(os.path.join(srcTfrecDir, 'merged_shards.csv'), index_col=0)
logger.info(f'Using as source TFRecords: {srcTfrecDir}')
destTfrecDir = srcTfrecDir + '_predict_tcesnotused'
logger.info(f'Destination TFRecord directory: {destTfrecDir}')
os.makedirs(destTfrecDir, exist_ok=True)

logger.info('Creating new TFRecords...')
omitMissing = True
nProcesses = 15
pool = multiprocessing.Pool(processes=nProcesses)
jobs = [shardTuple + (srcTbl, srcTfrecDir, destTfrecDir, tceIdentifier, omitMissing) for shardTuple in shardTuples]
async_results = [pool.apply_async(create_shard, job) for job in jobs]
pool.close()
for async_result in async_results:
    async_result.get()
logger.info('New TFRecords created.')

logger.info('Checking the number of examples in the newly created TFRecords.')
# check the number of Examples in the TFRecord shards and that each TCE example for a given dataset is in the TFRecords
tfrecFiles = [os.path.join(destTfrecDir, file) for file in os.listdir(destTfrecDir) if 'shard' in file]
countExamples = []  # total number of examples
tceIdentifier = 'tce_plnt_num'
for tfrecFile in tfrecFiles:

    countExamplesShard = 0

    # iterate through the source shard
    tfrecord_dataset = tf.data.TFRecordDataset(tfrecFile)

    for string_record in tfrecord_dataset.as_numpy_iterator():

        example = tf.train.Example()
        example.ParseFromString(string_record)

        tceIdentifierTfrec = example.features.feature[tceIdentifier].int64_list.value[0]
        targetIdTfrec = example.features.feature['target_id'].int64_list.value[0]

        foundTce = tce_tbl.loc[(tce_tbl['target_id'] == targetIdTfrec) &
                               (tce_tbl[tceIdentifier] == tceIdentifierTfrec)]

        if len(foundTce) == 0:
            raise ValueError(f'TCE {targetIdTfrec}-{tceIdentifierTfrec}')

        countExamplesShard += 1
    countExamples.append(countExamplesShard)

# ADD SCALAR FEATURES TO THE TFRECORDS
# tce_prad, tce_period, tce_maxmes, tce_albedo_stat, tce_ptemp_stat, wst_depth

logger.info('### Adding scalar features to the TFRecords ###')
features = {
    # secondary related features
    'tce_maxmes': {'missing_value': 0, 'log_transform': False, 'log_transform_eps': np.nan, 'clip_factor': 20},
    'tce_albedo_stat': {'missing_value': 0, 'log_transform': False, 'log_transform_eps': np.nan, 'clip_factor': 20},
    'tce_ptemp_stat': {'missing_value': 0, 'log_transform': False, 'log_transform_eps': np.nan, 'clip_factor': 20},
    'wst_depth': {'missing_value': 0, 'log_transform': False, 'log_transform_eps': np.nan, 'clip_factor': 20,
                  'replace_value': 0},
    # stellar parameters
    'tce_steff': {'missing_value': np.nan, 'log_transform': False, 'log_transform_eps': np.nan,
                  'clip_factor': np.nan, 'dtype': 'int'},
    'tce_slogg': {'missing_value': np.nan, 'log_transform': False, 'log_transform_eps': np.nan,
                  'clip_factor': np.nan, 'dtype': 'float'},
    'tce_smet': {'missing_value': np.nan, 'log_transform': False, 'log_transform_eps': np.nan,
                 'clip_factor': np.nan, 'dtype': 'float'},
    'tce_sradius': {'missing_value': np.nan, 'log_transform': False, 'log_transform_eps': np.nan,
                    'clip_factor': np.nan, 'dtype': 'float'},
    'tce_smass': {'missing_value': np.nan, 'log_transform': False, 'log_transform_eps': np.nan,
                  'clip_factor': np.nan, 'dtype': 'float'},
    'tce_sdens': {'missing_value': np.nan, 'log_transform': False, 'log_transform_eps': np.nan,
                  'clip_factor': np.nan, 'dtype': 'float'},
    # other diagnostics
    'boot_fap': {'missing_value': -1, 'log_transform': True, 'log_transform_eps': 1e-32,
                 'clip_factor': np.nan, 'dtype': 'float'},
    'tce_cap_stat': {'missing_value': np.nan, 'log_transform': False, 'log_transform_eps': np.nan,
                     'clip_factor': 20, 'dtype': 'float'},
    'tce_hap_stat': {'missing_value': np.nan, 'log_transform': False, 'log_transform_eps': np.nan,
                     'clip_factor': 20, 'dtype': 'float'},
    'tce_rb_tcount0': {'missing_value': np.nan, 'log_transform': False, 'log_transform_eps': np.nan,
                       'clip_factor': np.nan, 'dtype': 'int'},
    # centroid
    'tce_fwm_stat': {'missing_value': np.nan, 'log_transform': False, 'log_transform_eps': np.nan, 'clip_factor': 20,
                     'dtype': 'float'},
    'tce_dikco_msky': {'missing_value': np.nan, 'log_transform': False, 'log_transform_eps': np.nan,
                       'clip_factor': np.nan, 'dtype': 'float'},
    'tce_dicco_msky': {'missing_value': np.nan, 'log_transform': False, 'log_transform_eps': np.nan,
                       'clip_factor': np.nan, 'dtype': 'float'},
    'tce_dikco_msky_err': {'missing_value': np.nan, 'log_transform': False, 'log_transform_eps': np.nan,
                           'clip_factor': np.nan, 'dtype': 'float'},
    'tce_dicco_msky_err': {'missing_value': np.nan, 'log_transform': False, 'log_transform_eps': np.nan,
                           'clip_factor': np.nan, 'dtype': 'float'},
    # flux
    'transit_depth': {'missing_value': np.nan, 'log_transform': False, 'log_transform_eps': np.nan,
                      'clip_factor': 20, 'dtype': 'float'},
    'tce_depth_err': {'missing_value': np.nan, 'log_transform': False, 'log_transform_eps': np.nan,
                      'clip_factor': np.nan, 'dtype': 'float'},
    'tce_duration': {'missing_value': np.nan, 'log_transform': False, 'log_transform_eps': np.nan,
                     'clip_factor': np.nan, 'dtype': 'float'},
    'tce_duration_err': {'missing_value': np.nan, 'log_transform': False, 'log_transform_eps': np.nan,
                         'clip_factor': np.nan, 'dtype': 'float'},
    'tce_period': {'missing_value': np.nan, 'log_transform': False, 'log_transform_eps': np.nan,
                   'clip_factor': np.nan, 'dtype': 'float'},
    'tce_period_err': {'missing_value': np.nan, 'log_transform': False, 'log_transform_eps': np.nan,
                       'clip_factor': np.nan, 'dtype': 'float'},
    'tce_max_mult_ev': {'missing_value': np.nan, 'log_transform': False, 'log_transform_eps': np.nan,
                        'clip_factor': np.nan, 'dtype': 'float'},
    'tce_prad': {'missing_value': np.nan, 'log_transform': False, 'log_transform_eps': np.nan,
                 'clip_factor': 20}
}
logger.info(f'Features to be added (even if they are already there - some might need to be recomputed, such as stats '
            f'related to the secondary): {features}')

# compute training statistics for standardization
logger.info(f'Computing training statistics for normalizing the features...')
for feature in features:

    # remove missing values
    if not np.isnan(features[feature]['missing_value']):
        if feature == 'wst_depth':
            values = trainset_tce_tbl.loc[trainset_tce_tbl[feature] >=
                                          features[feature]['missing_value'], feature].values
        else:
            values = trainset_tce_tbl.loc[trainset_tce_tbl[feature] !=
                                          features[feature]['missing_value'], feature].values
    else:
        values = trainset_tce_tbl[feature].values

    # log transform
    if features[feature]['log_transform']:
        values += features[feature]['log_transform_eps']
        values = np.log10(values)

    features[feature].update({
        'median': np.median(values),
        'mad_std': mad_std(values)
    })
logger.info(f'Normalization statistics: {features}')

tce_tbl_nonnorm = tce_tbl.copy(deep=True)

logger.info(f'Normalizing scalar features in TCE table {tce_tbl_fp}...')
# standardize for all the data set
features_list = list(features.keys())
tce_tbl[features_list] = tce_tbl[features_list].apply(normalize_scalar_features, args=(features,), axis=1)

tfrec_src_dir_fp = '/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/tfrecordskeplerdr25-dv_g301-l31_6tr_spline_nongapped_flux-loe-lwks-centroid-centroidfdl-6stellar-bfap-ghost-rollband-stdts_secsymphase_correctprimarygapping_confirmedkoiperiod_data/tfrecordskeplerdr25-dv_g301-l31_6tr_spline_nongapped_flux-loe-lwks-centroid-centroidfdl-6stellar-bfap-ghost-rollband-stdts_secsymphase_correctprimarygapping_confirmedkoiperiod_predict_tcesnotused'
tfrec_src_dir = Path(tfrec_src_dir_fp)
src_tfrecs = [file_path for file_path in tfrec_src_dir.iterdir() if 'shard' in file_path.stem]

# define TFRecords destination
tfrec_dest_dir = tfrec_src_dir.parent / (tfrec_src_dir.stem + '_scalarnorm')
tfrec_dest_dir.mkdir(exist_ok=True)

tce_identifier = 'tce_plnt_num'

logger.info(f'Adding scalar features and their normalized versions to {tfrec_dest_dir}')


# add standardized features to the source TFRecords and write them to the destination
def _add_scalar_features_to_tfrecords(src_tfrecs, tfrec_dest_dir, tce_tbl, tce_tbl_nonnorm):

    for src_tfrec in src_tfrecs:

        with tf.io.TFRecordWriter(str(tfrec_dest_dir / src_tfrec.stem)) as writer:

            tfrecord_dataset = tf.data.TFRecordDataset(str(src_tfrec))

            for string_i, string_record in enumerate(tfrecord_dataset.as_numpy_iterator()):

                example = tf.train.Example()
                example.ParseFromString(string_record)

                target_id = example.features.feature['target_id'].int64_list.value[0]
                tce_id = example.features.feature[tce_identifier].int64_list.value[0]

                tceFound = tce_tbl.loc[(tce_tbl['target_id'] == target_id) &
                                      (tce_tbl['tce_plnt_num'] == tce_id)]
                tceFound_nonnorm = tce_tbl_nonnorm.loc[(tce_tbl_nonnorm['target_id'] == target_id) &
                                                       (tce_tbl_nonnorm['tce_plnt_num'] == tce_id)]

                for feature in features:
                    example_util.set_float_feature(example, f'{feature}', tceFound_nonnorm[feature].values,
                                                   allow_overwrite=True)
                    example_util.set_float_feature(example, f'{feature}_norm', tceFound[feature].values,
                                                   allow_overwrite=True)

                writer.write(example.SerializeToString())


_add_scalar_features_to_tfrecords(src_tfrecs, tfrec_dest_dir, tce_tbl, tce_tbl_nonnorm)
p = multiprocessing.Process(target=_add_scalar_features_to_tfrecords,
                            args=(src_tfrecs, tfrec_dest_dir, tce_tbl, tce_tbl_nonnorm))
p.start()
p.join()
logger.info('Finished normalizing scalar features.')

logger.info('### Normalizing time series features ###')
# normalize time series features

srcTfrecDir = '/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/' \
              'tfrecordskeplerdr25-dv_g301-l31_6tr_spline_nongapped_flux-loe-lwks-centroid-centroidfdl-6stellar-bfap-ghost-rollband-stdts_secsymphase_correctprimarygapping_confirmedkoiperiod_data/' \
              'tfrecordskeplerdr25-dv_g301-l31_6tr_spline_nongapped_flux-loe-lwks-centroid-centroidfdl-6stellar-bfap-ghost-rollband-stdts_secsymphase_correctprimarygapping_confirmedkoiperiod_predict_tcesnotused_scalarnorm'
destTfrecDir = srcTfrecDir + '_tsnorm'
logger.info(f'Adding normalized time series features to {destTfrecDir}')
os.makedirs(destTfrecDir, exist_ok=True)
srcTfrecFiles = [os.path.join(srcTfrecDir, file) for file in os.listdir(srcTfrecDir) if 'shard' in file]
srcTfrecFiles = [file for file in srcTfrecFiles if not file.endswith('.csv')]

auxParams = {
    'nr_transit_durations': 2 * 2.5 + 1,  # 2 * 4 + 1,  # number of transit durations (2*n+1, n on each side of the transit)
    'num_bins_loc': 31,  # 201
    'num_bins_glob': 301,  # 2001
}
logger.info(f'Auxiliary parameters for normalizing the time series: {auxParams}')

normStatsDir = '/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/' \
               'tfrecordskeplerdr25-dv_g301-l31_6tr_spline_nongapped_flux-loe-lwks-centroid-centroidfdl-6stellar-bfap-ghost-rollband-stdts_secsymphase_correctprimarygapping_confirmedkoiperiod_data/' \
               'tfrecordskeplerdr25-dv_g301-l31_6tr_spline_nongapped_flux-loe-lwks-centroid-centroidfdl-6stellar-bfap-ghost-rollband-stdts_secsymphase_correctprimarygapping_confirmedkoiperiod_starshuffle_experiment'
logger.info(f'Loading normalization statistics from {normStatsDir}')
# load normalization statistics
normStats = {
    'fdl_centroid': np.load(os.path.join(normStatsDir, 'train_fdlcentroid_norm_stats.npy'), allow_pickle=True).item(),
    'centroid': np.load(os.path.join(normStatsDir, 'train_centroid_norm_stats.npy'), allow_pickle=True).item()
}
logger.info(f'Normalization statistics: {normStats}')

logger.info(f'Normalizing time series features in TFRecords {destTfrecDir}')
nProcesses = 15
pool = multiprocessing.Pool(processes=nProcesses)
jobs = [(destTfrecDir, file, normStats, auxParams) for file in srcTfrecFiles]
async_results = [pool.apply_async(normalize_timeseries_features, job) for job in jobs]
pool.close()
for async_result in async_results:
    async_result.get()
logger.info('Normalization finished.')

#%% create TFRecord shards for CV

tfrec_dir_root = Path('/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/'
                      'tfrecordskeplerdr25-dv_g301-l31_6tr_spline_nongapped_flux-loe-lwks-centroid-centroidfdl-6stellar'
                      '-bfap-ghost-rollband-stdts_secsymphase_correctprimarygapping_confirmedkoiperiod_data/')
# source TFRecord directory
src_tfrec_dataset = 'tfrecordskeplerdr25-dv_g301-l31_6tr_spline_nongapped_flux-loe-lwks-centroid-' \
                    'centroidfdl-6stellar-bfap-ghost-rollband-stdts_secsymphase_correctprimarygapping_' \
                    'confirmedkoiperiod'
src_tfrec_dir = tfrec_dir_root / src_tfrec_dataset
src_tfrec_fps = [file for file in src_tfrec_dir.iterdir() if 'shard' in file.stem and not file.suffix == '.csv']

# destination TFRecord directory
dest_tfrec_dir = tfrec_dir_root / f'{src_tfrec_dataset}_newsplit'
dest_tfrec_dir.mkdir(exist_ok=True)

# dataset TCE tables
dataset_tbls_dir = Path('/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/train-val-test-sets/'
                        'split_12-19-2020_10-22')
dataset_tbls = {'train': pd.read_csv(dataset_tbls_dir / 'trainset.csv'),
                'val': pd.read_csv(dataset_tbls_dir / 'valset.csv'),
                'test': pd.read_csv(dataset_tbls_dir / 'testset.csv')
                }

n_shards = 10
print(f'Total number of shards: {n_shards}')
n_total_tces = np.sum([len(dataset_tbls[tbl]) for tbl in dataset_tbls])
print(f'Total number of TCEs: {n_total_tces}')
n_tces_per_shard = int(np.ceil(n_total_tces / n_shards))  # 85  # number of TCEs per shard
print(f'Number of TCES per shard: {n_tces_per_shard}')

shard_i = 0
# iterate over the TCE dataset tables
for dataset in dataset_tbls:

    current_tce_i = 0
    while True:
        if current_tce_i == len(dataset_tbls[dataset]) - 1:
            break

        print(f'Current TCE i: {current_tce_i} ({len(dataset_tbls[dataset]) - 1})')
        # write examples in a new TFRecord shard
        n_tces_in_shard = 0
        tfrec_new_fp = dest_tfrec_dir / f'{dataset}-shard-{f"{shard_i}".zfill(4)}'
        with tf.io.TFRecordWriter(str(tfrec_new_fp)) as writer:

            if n_tces_in_shard % 50 == 0:
                print(f'Writing examples {n_tces_per_shard}) to TFRecord {tfrec_new_fp}...')

            # iterate over the TCEs in each dataset table
            aux_tbl = dataset_tbls[dataset][current_tce_i:]
            for tce_i, tce in aux_tbl.iterrows():

                if tce_i + 1 % 50 == 0:
                    print(f'Iterating over dataset {dataset} ({tce_i + 1} out of {len(dataset_tbls[dataset])})...')

                tce_found_flag = False

                # look for TCE in the source TFRecords
                for src_tfrec in src_tfrec_fps:
                    tfrecord_dataset = tf.data.TFRecordDataset(str(src_tfrec))
                    for string_i, string_record in enumerate(tfrecord_dataset.as_numpy_iterator()):
                        example = tf.train.Example()
                        example.ParseFromString(string_record)

                        target_id = example.features.feature['target_id'].int64_list.value[0]
                        tce_id = example.features.feature['tce_plnt_num'].int64_list.value[0]

                        tce_found = dataset_tbls[dataset].loc[(dataset_tbls[dataset]['target_id'] == target_id) &
                                                              (dataset_tbls[dataset]['tce_plnt_num'] == tce_id)]

                        if len(tce_found) > 0:  # add TCE found to the TFRecord shard
                            writer.write(example.SerializeToString())
                            n_tces_in_shard += 1
                            tce_found_flag = True
                            break

                    if tce_found_flag:
                        break

                if n_tces_in_shard == n_tces_per_shard:
                    break

            # shard completed or dataset iterately completely
            if n_tces_in_shard == n_tces_per_shard or tce_i == len(dataset_tbls[dataset]) - 1:
                shard_i += 1
                current_tce_i = tce_i
                print(f'End Current TCE i: {current_tce_i} ({len(dataset_tbls[dataset]) - 1})\nNumber of TCEs in shard: {n_tces_in_shard}')
                writer.close()

#%% Get only TCEs-1 from a dataset

src_tfrec_fp = '/data5/tess_project/Data/tfrecords/Kepler/Q1-Q17_DR25/'\
               'tfrecordskeplerdr25-dv_g301-l31_6tr_spline_nongapped_flux-loe-lwks-centroid-centroidfdl-6stellar-'\
               'bfap-ghost-rollband-stdts_secsymphase_correctprimarygapping_confirmedkoiperiod_data/'\
               'tfrecordskeplerdr25-dv_g301-l31_6tr_spline_nongapped_flux-loe-lwks-centroid-centroidfdl-6stellar-'\
               'bfap-ghost-rollband-stdts_secsymphase_correctprimarygapping_confirmedkoiperiod_starshuffle_'\
               'experiment-labels-norm_nopps_secparams_prad_period'

tfrec_src_dir = Path(src_tfrec_fp)
src_tfrecs = [file_path for file_path in tfrec_src_dir.iterdir() if 'shard' in file_path.stem]

# define TFRecords destination
tfrec_dest_dir = tfrec_src_dir.parent / (tfrec_src_dir.stem + '_tces1')
tfrec_dest_dir.mkdir(exist_ok=True)

# set up logger
logger = logging.getLogger(name='create_tfrecords_tces1')
logger_handler = logging.FileHandler(filename=tfrec_dest_dir / 'create_tfrecords_tces1.log', mode='w')
logger_formatter = logging.Formatter('%(asctime)s - %(message)s')
logger.setLevel(logging.INFO)
logger_handler.setFormatter(logger_formatter)
logger.addHandler(logger_handler)
logger.info(f'Starting run...')

tces_stats = {dataset: {'removed': 0, 'kept': 0} for dataset in ['train', 'val', 'test']}
for src_tfrec in src_tfrecs:

    with tf.io.TFRecordWriter(str(tfrec_dest_dir / src_tfrec.name)) as writer:

        tfrecord_dataset = tf.data.TFRecordDataset(str(src_tfrec))

        for string_i, string_record in enumerate(tfrecord_dataset.as_numpy_iterator()):

            example = tf.train.Example()
            example.ParseFromString(string_record)

            tceid = example.features.feature['tce_plnt_num'].int64_list.value[0]

            if tceid != 1:
                tces_stats[src_tfrec.name.split('-')[0]]['removed'] += 1
                continue

            tces_stats[src_tfrec.name.split('-')[0]]['kept'] += 1
            writer.write(example.SerializeToString())

print(f'Number of TCEs removed/kept: {tces_stats}')
logger.info(f'Number of TCEs removed/kept:\n{tces_stats}')
