# Conduct preprocessing run that creates a TFRecord dataset for CV; uses GNU parallel to generate multiple processes,
# each one conducting the preprocessing of a subset of the examples.
#PBS -S /bin/bash
#PBS -N preprocess_data_cv_exoplnt
#PBS -l walltime=12:00:00
# PBS -l select=1:ncpus=10:model=rom_ait
# PBS -q debug
#PBS -l select=1:ncpus=10:mem=360g:model=sky_gpu
#PBS -l place=free:excl
#PBS -q dsg_gpu@pbspl4
#PBS -o /home6/msaragoc/jobs/Kepler-TESS_exoplanet/job_preprocess_data_cv.out
#PBS -e /home6/msaragoc/jobs/Kepler-TESS_exoplanet/job_preprocess_data_cv.err
#PBS -W group_list=a1509
#PBS -m bea

export PYTHONPATH=/home6/msaragoc/work_dir/Kepler-TESS_exoplanet/codebase/

source "$HOME"/.bashrc

conda activate exoplnt_dl_tf2_13

# create output directory for preprocessing results
OUTPUT_DIR=/nobackupp19/msaragoc/work_dir/Kepler-TESS_exoplanet/data/tfrecords/TESS/tfrecords_tess_spoc_2min_s1-s67_8-21-2024_1038_data/cv_tfrecords_tess_spoc_2min_s1-s67_8-27-2024_1300/tfrecords/eval_normalized
mkdir -p $OUTPUT_DIR

# script file path
SCRIPT_FP=/nobackupp19/msaragoc/work_dir/Kepler-TESS_exoplanet/codebase/src_cv/create_cv_dataset/preprocess_cv_folds_trecord_dataset.py
# config file path
CONFIG_FP=/nobackupp19/msaragoc/work_dir/Kepler-TESS_exoplanet/codebase/src_cv/create_cv_dataset/config_preprocess_cv_folds_tfrecord_dataset.yaml
# job script for running preprocessing pipeline
PREPROCESS_SH_SCRIPT=/nobackupp19/msaragoc/work_dir/Kepler-TESS_exoplanet/codebase/src_cv/create_cv_dataset/preprocessing_job.sh

# number of total jobs; CV iterations
NUM_TOTAL_JOBS=10
# number of jobs run simultaneously
NUM_JOBS_PARALLEL=10

# run with GNU parallel
seq 0 $((NUM_TOTAL_JOBS - 1)) | parallel -j $NUM_JOBS_PARALLEL "$PREPROCESS_SH_SCRIPT {} $OUTPUT_DIR $SCRIPT_FP $CONFIG_FP $NUM_TOTAL_JOBS"
