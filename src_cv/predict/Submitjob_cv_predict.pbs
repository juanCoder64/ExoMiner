# Run CV predict experiment in parallel (multiple iterations at the same time) on a single GPU node with multiple GPUs
# and using a combination of GNU parallel and job array
#PBS -S /bin/bash
#PBS -N cv_predict_exoplanet
#PBS -l walltime=24:00:00
# V100 GPU nodes -------
#PBS -l select=1:ncpus=36:ngpus=4:mem=360g:model=sky_gpu
# place the chunk wherever it is possible for the requested resources; share resources with other people
#PBS -l place=free:excl
#PBS -q dsg_gpu@pbspl4
# non-GPU nodes ----------
# PBS -l select=1:ncpus=40:model=cas_ait
# PBS -q debug
# Cabeus A100 GPU nodes -------
# PBS -l select=1:ncpus=16:mem=128g:ngpus=1:model=mil_a100
# PBS -l place=scatter:shared
# PBS -q p_gpu_normal@pbspl4
# GH nodes --------
# PBS -lselect=1:ncpus=72:ngpus=1:model=gh200
# PBS -q dsggh_gpu@pbs05a
#PBS -o /home6/msaragoc/jobs/Kepler-TESS_exoplanet/job_cv_predict_exoplnt_mgpus_singlenode.out
#PBS -e /home6/msaragoc/jobs/Kepler-TESS_exoplanet/job_cv_predict_exoplnt_mgpus_singlenode.err
#PBS -W group_list=a1509
# PBS -W group_list=s2857
#PBS -m bea
#PBS -J 0-2

# PBS_ARRAY_INDEX=0
# echo $TMPDIR
export TMPDIR=/var/tmp/pbs.34282.pbs05a.gh.nas.nasa.gov
mkdir -p "$TMPDIR"

# initialize conda and activate conda environment
module use -a /swbuild/analytix/tools/modulefiles
# module load miniconda3/gh2
# source activate exoplnt_dl_gh_nfs
# source activate exoplnt_dl_gh
# source activate tf2_18_gh
module load miniconda3/v4
source activate exoplnt_dl_tf2_13
# source activate exoplnt_dl_tf2_16_nfs

# export LD_LIBRARY_PATH=$CONDA_PREFIX/lib:$LD_LIBRARY_PATH

# set path to codebase root directory
export PYTHONPATH=/home6/msaragoc/work_dir/Kepler-TESS_exoplanet/codebase/

# script for running inference for a single CV iteration
RUN_SH_SCRIPT=$PYTHONPATH/src_cv/predict/run_predict_cv_iter_modular.sh
# config yaml file used to run inference
CONFIG_FP=$PYTHONPATH/src_cv/predict/config_cv_predict.yaml
# config yaml file used to preprocess data for model inference
CONFIG_PREPROCESS_FP=$PYTHONPATH/src_cv/preprocessing/config_preprocess_cv_folds_predict_tfrecord_dataset.yaml

# output directory
OUTPUT_DIR=/u/msaragoc/work_dir/Kepler-TESS_exoplanet/experiments/tess_spoc_ffi/cv_exominernew_focal-loss-alpha0.98-gamma2_tess-spoc-ffi-s36-s72_6-2-2025_1725_predictunks_6-3-2025_1648/
mkdir -p $OUTPUT_DIR

PBS_SCRIPT_FP=$(realpath $0)
cp $PBS_SCRIPT_FP $OUTPUT_DIR/pbs_job.txt

# copy codebase git commit hash
COMMIT_HASH=$(git -C $PYTHONPATH rev-parse HEAD)
echo "Git hash commit: $COMMIT_HASH"  >> $JOB_FP

# root directory for model file paths
MODELS_CV_ROOT_DIR=/u/msaragoc/work_dir/Kepler-TESS_exoplanet/experiments/tess_spoc_ffi/cv_exominernew_focal-loss-alpha0.98-gamma2_tess-spoc-ffi-s36-s72_6-2-2025_1725/
N_CV_ITERS=5  # number of CV folds/iterations

# whether to preprocess data before running inference
PREPROCESS_DATA=true

# whether to delete preprocessed data after running inference
DELETE_PREPROCESSED_DATA=true

# number of GPUs to be used by this job array
N_GPUS_TOTAL=4

# number of total jobs per job in job array
NUM_TOTAL_JOBS=4
# number of jobs run simultaneously
NUM_JOBS_PARALLEL=4

# run with GNU parallel
# run CV sh script
seq 0 $((NUM_TOTAL_JOBS - 1)) | parallel -j $NUM_JOBS_PARALLEL "$RUN_SH_SCRIPT {} $PBS_ARRAY_INDEX $CONFIG_FP $OUTPUT_DIR $NUM_TOTAL_JOBS $N_GPUS_TOTAL $MODELS_CV_ROOT_DIR" "$N_CV_ITERS" "$PREPROCESS_DATA" $CONFIG_PREPROCESS_FP "$DELETE_PREPROCESSED_DATA"

# testing multi-node setup...
# seq 0 $((NUM_TOTAL_JOBS - 1)) | parallel -j $NUM_JOBS_PARALLEL --sshloginfile "$RUN_SH_SCRIPT {} $PBS_ARRAY_INDEX $CONFIG_FP $OUTPUT_DIR $NUM_TOTAL_JOBS $N_GPUS_TOTAL $MODELS_CV_ROOT_DIR" "$N_CV_ITERS" "$PREPROCESS_DATA" $CONFIG_PREPROCESS_FP "$DELETE_PREPROCESSED_DATA"
