# Train multiple models in parallel on a single GPU node with multiple GPUs and using a combination of GNU parallel and
# job array
#PBS -S /bin/bash
#PBS -N train_exoplnt
#PBS -l walltime=08:00:00
#PBS -l select=1:ncpus=4:ngpus=1:mem=30g:model=sky_gpu
# place the chunk wherever it is possible for the requested resources; share resources with other people
#PBS -l place=pack:shared
#PBS -q dsg_gpu@pbspl4
#PBS -o /home6/msaragoc/jobs/Kepler-TESS_exoplanet/job_train_exoplnt_singlegpu.out
#PBS -e /home6/msaragoc/jobs/Kepler-TESS_exoplanet/job_train_exoplnt_singlegpu.err
#PBS -W group_list=a1509
#PBS -m bea

# script file path
# SCRIPT_FP=/home6/msaragoc/work_dir/Kepler-TESS_exoplanet/codebase/src/train_model.py
SCRIPT_FP=/home6/msaragoc/work_dir/Kepler-TESS_exoplanet/codebase/src_cv/cv.py
# config file path
# CONFIG_FP=/home6/msaragoc/work_dir/Kepler-TESS_exoplanet/codebase/src/config_train.yaml
CONFIG_FP=/home6/msaragoc/work_dir/Kepler-TESS_exoplanet/scripts_unchanged/config_cv_train_singlegpu_seq_simple.yaml
# job script for running the Python application to train the model
# TRAIN_SH_SCRIPT=/home6/msaragoc/work_dir/Kepler-TESS_exoplanet/codebase/src/train_model.sh
TRAIN_SH_SCRIPT=/home6/msaragoc/work_dir/Kepler-TESS_exoplanet/codebase/src_cv/run_cv_iter.sh
# output directory
OUTPUT_DIR=/home6/msaragoc/work_dir/Kepler-TESS_exoplanet/experiments/cv_kepler_q1q17dr25_11-30-2023_1038
mkdir -p $OUTPUT_DIR

# RANK=0
# N_GPUS_TOTAL=4

$TRAIN_SH_SCRIPT $RANK $RANK $SCRIPT_FP $CONFIG_FP $OUTPUT_DIR $N_GPUS_TOTAL