# Conduct HPO run multi-GPU and multi-node; using MPIEXEC to evaluate multiple configurations (one per GPU) at the same
# time.
#PBS -S /bin/bash
#PBS -N hpo_exoplanet
#PBS -l walltime=72:00:00
#PBS -l select=4:model=sky_gpu:ngpus=2:mpiprocs=2:ncpus=2:mem=50g
#PBS -l place=vscatter:excl
#PBS -q dsg_gpu@pbspl4
#PBS -o /home6/msaragoc/jobs/Kepler-TESS_exoplanet/job_hpo_mgpus.out
#PBS -e /home6/msaragoc/jobs/Kepler-TESS_exoplanet/job_hpo_mgpus.err
#PBS -W group_list=a1509
#PBS -m bea

module list

# source activate /home6/msaragoc/work_dir/tools/python_envs/exoplnt_dl
source activate exoplnt_dl

cd /home6/msaragoc/work_dir/Kepler-TESS_exoplanet/codebase

mpiexec python src_hpo/hpo_keras.py  &>  /home6/msaragoc/work_dir/Kepler-TESS_exoplanet/hpo_configs/hpo.txt
