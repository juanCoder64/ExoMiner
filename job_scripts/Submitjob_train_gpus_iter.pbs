# Train ensemble of models (multi-GPU and multi-node); Uses MPIEXEC and job arrays; Number of trained models needs to
# be less or equal to mpiprocs*n_jobs_array.
#PBS -S /bin/bash
#PBS -N train_exoplnt
#PBS -l walltime=08:00:00
#PBS -l select=1:ncpus=36:mpiprocs=4:model=sky_gpu:ngpus=4:mem=360g
#PBS -l place=scatter:exclhost
# #PBS -l select=1:ncpus=36:mpiprocs=4:model=sky_gpu
# #PBS -l select=10:ncpus=16:mpiprocs=1:model=san_gpu
#PBS -q dsg_gpu@pbspl4
# #PBS -q v100
# #PBS -q k40
#PBS -o /home6/msaragoc/jobs/Kepler-TESS_exoplanet/job_train_exoplnt_mgpus.out
#PBS -e /home6/msaragoc/jobs/Kepler-TESS_exoplanet/job_train_exoplnt_mgpus.err
#PBS -W group_list=a1509
#PBS -m bea
# #PBS -J 0-9
#PBS -J 0-2

# module load cuda/10.1
# module load tensorflow/1.6
# module load mpi-sgi/mpt
# module unload mpi-hpe/mpt.2.17r13

module list

source activate exoplnt_dl
# source activate /home6/msaragoc/work_dir/tools/python_envs/exoplnt_dl

cd /home6/msaragoc/work_dir/Kepler-TESS_exoplanet/codebase/

mpiexec python src/train_keras.py --job_idx=$PBS_ARRAY_INDEX &> /home6/msaragoc/work_dir/Kepler-TESS_exoplanet/trained_models/train_$PBS_ARRAY_INDEX.txt
# without job arrays
# mpiexec python src/train_keras.py &> /home6/msaragoc/work_dir/Kepler-TESS_exoplanet/trained_models/train.txt
