# Train ensemble of models (multi-GPU and multi-node); Uses MPIEXEC and job arrays; Number of trained models needs to
# be less or equal to mpiprocs*n_jobs_array; Configuration filepath is received as environment variable to the PBS
# script; Allows for running multiple configurations sequentially.
#PBS -S /bin/bash
#PBS -N shap_exoplnt_train
#PBS -l walltime=01:00:00
#PBS -l select=1:ncpus=36:mpiprocs=4:model=sky_gpu:ngpus=4:mem=360g
#PBS -l place=scatter:exclhost
# #PBS -l select=1:ncpus=36:mpiprocs=4:model=sky_gpu
# #PBS -l select=10:ncpus=16:mpiprocs=1:model=san_gpu
#PBS -q dsg_gpu@pbspl4
# #PBS -q v100
# #PBS -q k40
#PBS -o /home6/msaragoc/jobs/Kepler-TESS_exoplanet/job_shap_exoplnt_train.out
#PBS -e /home6/msaragoc/jobs/Kepler-TESS_exoplanet/job_shap_exoplnt_train.err
#PBS -W group_list=a1509
#PBS -m bea
# #PBS -J 0-9
#PBS -J 0-2

module list

source activate exoplnt_dl
# source activate /home6/msaragoc/work_dir/tools/python_envs/exoplnt_dl

cd /home6/msaragoc/work_dir/Kepler-TESS_exoplanet/codebase/

CONFIG_FILE_DIR=/home6/msaragoc/work_dir/Kepler-TESS_exoplanet/experiments/shap/shap_1-11-2022/runs_configs/
TRAIN_SCRIPT=/home6/msaragoc/work_dir/Kepler-TESS_exoplanet/codebase/src/train_keras.py
PYOUT_DIR=/home6/msaragoc/work_dir/Kepler-TESS_exoplanet/experiments/shap/shap_1-11-2022/log_py/

echo Training "$SHAP_CONFIG_RUN"; mpiexec python "$TRAIN_SCRIPT" --config_file="$CONFIG_FILE_DIR$SHAP_CONFIG_RUN"_train.yaml --job_idx=$PBS_ARRAY_INDEX &> "$PYOUT_DIR$SHAP_CONFIG_RUN"_train"$PBS_ARRAY_INDEX"_log_py.txt;
