# PBS job script example for requesting resources in a Rome Aitken non-GPU node for applications that do not involve GPU
# usage E.g., preprocessing data without CUDA applications
#PBS -S /bin/bash
# job name
#PBS -N job_name
# request job time hh:mm:ss (different queues will have different constraints on max walltime)
#PBS -l walltime=02:00:00
# requesting 1 aitken rome node, all 128 cpus in that node
# for more info see https://www.nas.nasa.gov/hecc/support/kb/preparing-to-run-on-aitken-rome-nodes_657.html
#PBS -l select=1:ncpus=128:model=rom_ait
# SET QUEUE
# available queues: debug for debugging (< 2h wall time); devel for development; normal; long; low
# for more info see https://www.nas.nasa.gov/hecc/support/kb/pbs-job-queue-structure_187.html
#PBS -q normal
# set path to job output (not application output necessarily!)
#PBS -o path/to/job_output_file.out
# set path to job error (not application error necessarily!)
#PBS -e /path/to/job_error_file.err
# set charge to group
#PBS -W group_list=a1509
#PBS -m bea


#### code to be run in your job starts here ####

# initiate conda
source ~/.bashrc

# activate conda environment
conda activate conda_env_name

# export path to codebase
export PYTHONPATH=/path/to/codebase

SCRIPT_FP=/path/to/the/script
python $SCRIPT_FP

# output directory for preprocessing run
OUTPUT_DIR=/path/to/output/dir
# create directory
mkdir -p $OUTPUT_DIR

### WITHOUT EXTERNAL PARALLELIZATION ###
python $SCRIPT_FP &> $OUTPUT_DIR/python_output_{}.log

### WITH EXTERNAL PARALLELIZATION THROUGH GNU PARALLEL SINGLE NODE ###

# total number of jobs
NUM_TOTAL_RUNS=$((1 * 128))
# number of simultaneous jobs
NUM_RUNS_PER_NODE=128

# Using gnu parallel, start $NUM_TOTAL_RUNS processes, $NUM_RUNS_PER_NODE processes per node. Each process runs a
# Python application that handles a subset of data (~1/128)
seq 0 $(($NUM_TOTAL_RUNS - 1)) | parallel -j $NUM_RUNS_PER_NODE -u "python -u $SCRIPT_FP --rank={} --n_runs=$NUM_TOTAL_RUNS --output_dir=$OUTPUT_DIR > $OUTPUT_DIR/python_output_{}.log"
