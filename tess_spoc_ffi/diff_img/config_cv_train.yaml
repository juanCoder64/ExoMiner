rank: null
train_parallel: true  # set to true to run CV iterations in parallel (one per GPU); only useful when number of GPUs > 1
rnd_seed: 2  # random seed used to select the validation fold in each CV iteration
paths:
  # experiment directory; results are saved here
  experiment_root_dir: null
  # YAML file containing a list of CV iterations, where each CV iteration is a dictionary with the TFRecord filepaths
  # for each training, validation, and test set folds combination (i.e., {'train': ['/path/to/train_shard-xxxx', ...],
  # 'val': ['/path/to/val_shard-xxxx', ...], 'test': ['/path/to/test_shard-xxxx']}
  cv_folds: /u/msaragoc/work_dir/Kepler-TESS_exoplanet/data/tfrecords/TESS/tfrecords_tess_spoc_ffi_s36-s72_multisector_s56-s69_1-3-2025_1157_data/cv_tfrecords_tess_spoc_ffi_s36-s72_multisector_s56-s69_1-6-2025_1132/tfrecords/eval_with_2mindata_transferlearning/cv_2min_ffi_combined/cv_iterations.yaml
  # HPO run configuration directory; get configurations from an HPO run; comment to not use any
  hpo_dir: null

val_from_train: false  # if true, the validation fold is chosen randomly from the training split. The training split must contain more than one fold
num_val_folds: null  # assuming `val_from_train` is true, this variable sets the number of training folds used for validation

trainable_layers_fp: null  # /nobackupp19/msaragoc/work_dir/Kepler-TESS_exoplanet/codebase/tess_spoc_ffi/trainable_layers.yaml
fine_tuning_hyperparameters_fp: null  # /nobackupp19/msaragoc/work_dir/Kepler-TESS_exoplanet/codebase/tess_spoc_ffi/fine_tuning_hyperparameters.yaml

generate_csv_pred: true  # generates a prediction ranking for each of the specified datasets

data_fields: # scalar data from TFRecords to add to prediction table
  - uid
  - target_id
  - tce_plnt_num
  - label
  - tce_period
  - tce_duration
  - tce_time0bk
  - tce_depth
  - ruwe
  - tce_prad
  - tce_max_mult_ev
  - tce_model_snr
  - obs_type  # tess spoc ffi
  - tce_dikco_msky
  - tce_dikco_msky_err

#  uid: 'string'
#  target_id: 'int_scalar'
#  tce_plnt_num: 'int_scalar'
#  label: 'string'
##  label_id: 'int_scalar'
#
##  odd_even_flag: 'string'
##  sec_flag: 'string'
##  centroid_flag: 'string'
##  not_transit_like_flag: 'string'
#
#  tce_period: 'float_scalar'
#  tce_duration: 'float_scalar'
#  tce_time0bk: 'float_scalar'
#  tce_depth: 'float_scalar'
#  ruwe: 'float_scalar'
#  tce_prad: 'float_scalar'
#  tce_max_mult_ev: 'float_scalar'
#  tce_model_snr: 'float_scalar'
#
##  tce_dikco_msky_rat: 'float_scalar'
##  tce_dicco_msky_rat: 'float_scalar'
#
#  obs_type: string  # tess spoc ffi

# set general architecture of the model based on implementations under models.models_keras.py
model_architecture: ExoMinerJointLocalFlux  # ExoMinerPlusPlus

config:
  # input features
  diff_img_branch:
    imgs:
      - diff_imgs_std_trainset
      - oot_imgs_std_trainset
      #       - diff_imgs_stdnorm
      #       - oot_imgs_stdnorm
      - target_imgs
#      - snr_imgs_std_trainset
#      - neighbor_imgs_std_trainset
    imgs_scalars:
      - quality
    scalars:
      - mag_shift_norm
#      - tce_dikco_msky_norm
#      - tce_dikco_msky_err_norm
      #       - tce_dicco_msky_norm
      #       - tce_dicco_msky_err_norm
      # - tce_dikco_msky_rat_norm


  # difference image hyperparameters ---------
  num_diffimg_conv_blocks: 3
  diffimg_conv_ls_per_block: 3
  init_diffimg_conv_filters: 2
  kernel_size_diffimg: 3  # (1, k , k)
  pool_size_diffimg: 2  # (1, k, k)
  num_fc_diff_units: 3

  # shared conv hyperparameters ----
  num_fc_conv_units: 3
  dropout_rate_fc_conv: 0.1211390996398814

  # fc block hyperparameters
  init_fc_neurons: 512
  dropout_rate: 0.021491238286347598
  num_fc_layers: 4
  decay_rate: null  # for l2-regularization

  # shared hyperparameters --------
  non_lin_fn: prelu
  weight_initializer: null
  kernel_stride: 1
  pool_stride: 1

  # optimization hyperparameters ------
  optimizer: 'Adam'
  lr: 4.176171931455995e-05
  #  'batch_size': 64
  force_softmax: false
  #  use_kepler_ce: false

  # loss hyperparameters
  multi_class: false  # switch to multiclass classification

  # other hyperparameters -----
  batch_norm: false  # apply batch norm before FC block to all concatenated output features extracted from the branches

features_set: # each key-value pair is feature_name: {'dim': feature_dim, 'dtype': feature_dtype}

#  label: { 'dim': [ 1, ], 'dtype': string }
#  obs_type: { 'dim': [1, ], 'dtype': string }
#  obs_type_int: { 'dim': [1, ], 'dtype': int }

  # centroid related features
##  global_centr_view_std_noclip: { 'dim': [ 301, 1 ], 'dtype': float }
  local_centr_view_std_noclip: { 'dim': [ 31, 1 ], 'dtype': float }
  local_centr_view_std_noclip_var: { 'dim': [ 31, 1 ], 'dtype': float }
#  tce_fwm_stat_norm: { 'dim': [ 1, ], 'dtype': float }

 # stellar parameters
  tce_sdens_norm: { 'dim': [ 1, ], 'dtype': float }
  tce_steff_norm: { 'dim': [ 1, ], 'dtype': float }
  tce_smet_norm: { 'dim': [ 1, ], 'dtype': float }
  tce_slogg_norm: { 'dim': [ 1, ], 'dtype': float }
  tce_smass_norm: { 'dim': [ 1, ], 'dtype': float }
  tce_sradius_norm: { 'dim': [ 1, ], 'dtype': float }
  # mag_norm: { 'dim': [ 1, ], 'dtype': float }
  # mag_cat: { 'dim': [ 1, ], 'dtype': int }
  # mag_cat_norm: { 'dim': [ 1, ], 'dtype': float }
  ruwe_norm: { 'dim': [ 1, ], 'dtype': float }
  mag_shift_norm: { 'dim': [ 1, ], 'dtype': float }

  # difference image
  diff_imgs_std_trainset: { 'dim': [ 5, 33, 33 ], 'dtype': float }
  oot_imgs_std_trainset: { 'dim': [ 5, 33, 33 ], 'dtype': float }
#  #  diff_imgs_stdnorm: { 'dim': [ 5, 33, 33 ], 'dtype': float }
#  #  oot_imgs_stdnorm: { 'dim': [ 5, 33, 33 ], 'dtype': float }
  target_imgs: { 'dim': [ 5, 33, 33 ], 'dtype': float }
#  snr_imgs_std_trainset: { 'dim': [ 5, 33, 33 ], 'dtype': float }
#  neighbor_imgs_std_trainset: { 'dim': [ 5, 33, 33 ], 'dtype': float }

  quality: { 'dim': [ 5, 1], 'dtype': float }
  #  tce_dicco_msky_norm: { 'dim': [ 1, ], 'dtype': float }
  #  tce_dicco_msky_err_norm: { 'dim': [ 1, ], 'dtype': float }
  tce_dikco_msky_norm: { 'dim': [ 1, ], 'dtype': float }
  tce_dikco_msky_err_norm: { 'dim': [ 1, ], 'dtype': float }
  # tce_dikco_msky_rat_norm: { 'dim': [ 1, ], 'dtype': float }

# maps features' names to features names expected by the model
feature_map: null
#  feature_name: feature_name_model

training:
  data_augmentation: false  # perform online data augmentation
  online_preprocessing_params: # online data augmentation parameters
    'num_bins_global': 301
    'num_bins_local': 31
    'num_transit_dur': 5
  n_epochs: 300  # number of epochs used to train each model
  opt_metric: auc_pr  # metric shown in the epoch plots besides loss for the training, validation and test sets
  batch_size: 32
  category_weights: null  # category weights used in weighted loss; set to null for non-weighted loss
  #    PC: 1.0
  #    AFP: 0.5
  #    NTP: 1.0
  sample_weights: false  # use sample weights defined in the data set
  shuffle_buffer_size: 85000  # should be larger than size of the training set to allow perfect sampling

label_field_name: tce_dikco_msky  # label  # name of the label field in the TFRecord that is going to be used as the label

task: regression  # either `regression` or `classification`

evaluation:
  batch_size: 32
inference:
  batch_size: 32

callbacks:
  train:
    early_stopping:
      monitor: val_loss  # val_auc_pr  # val_auc_pr  # which metric used to monitor for early stopping
      min_delta: 0
      patience: 50
      verbose: 1
      mode: min  # maximize/minimize monitored metric in early stopping
      baseline: null
      restore_best_weights: true  # get model from epoch that had the best performance according to monitored metric
    tensorboard:
      histogram_freq: 1
      write_graph: true
      write_images: false
      update_freq: epoch
      profile_batch: 2
      embeddings_metadata: null
      embeddings_freq: 0

label_map:  # maps label to a label id
#  # Kepler
#  PC: 1
#  AFP: 0
#  NTP: 0
  # TESS
  KP: 1
  CP: 1
  FP: 2
#  FA: 0
  EB: 1
#  B: 0
#  J: 0
  NTP: 2
  NEB: 0
  NPC: 0
  BD: 2
  # Kepler Sim
#  INJ1: 1
#  INJ2: 0
#  INJ3: 0
#  SCR1: 0
#  SCR2: 0
#  INV: 0

datasets:
  - train
  - val
  - test

plot_model: true
write_model_summary: true
verbose_model: 2  # for fit, eval, predict functions
verbose: true  # general

metrics:
  'clf_thr': 0.5  # classification threshold
  'num_thr': 1000  # number of thresholds used to compute some metrics
