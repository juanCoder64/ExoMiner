# Run CV experiment in parallel (multiple iterations at the same time) on a single GPU node with multiple GPUs and
# using a combination of GNU parallel and job array
#PBS -S /bin/bash
#PBS -N cv_exoplanet
#PBS -l walltime=72:00:00
# DSG_GPU V100 GPU nodes
# PBS -l select=1:ncpus=36:ngpus=4:mem=360g:model=sky_gpu
# place the chunk wherever it is possible for the requested resources; share resources with other people
# PBS -l place=free:excl
# PBS -q dsg_gpu@pbspl4
# Cabeus A100 GPU nodes
# PBS -l select=1:ncpus=16:mem=128g:ngpus=1:model=mil_a100
# PBS -l place=scatter:shared
# PBS -q p_gpu_normal@pbspl4
# Grace Hopper GPU nodes
#PBS -lselect=1:ncpus=72:model=gra_h100:mem=480g
#PBS -q alpha_test
#PBS -o /home6/msaragoc/jobs/Kepler-TESS_exoplanet/job_train_exoplnt_mgpus_singlenode.out
#PBS -e /home6/msaragoc/jobs/Kepler-TESS_exoplanet/job_train_exoplnt_mgpus_singlenode.err
# PBS -W group_list=a1509
# PBS -W group_list=s2857
#PBS -m bea
#PBS -J 0-9

# PBS_ARRAY_INDEX=0

# initialize conda and activate conda environment
module use -a /swbuild/analytix/tools/modulefiles
# non-GH conda
module load miniconda3/v4
source activate exoplnt_dl_tf2_13
# GH conda (aarch64)
module load miniconda3/gh2
source activate exoplnt_dl_gh

# set path to codebase root directory
export PYTHONPATH=/home6/msaragoc/work_dir/Kepler-TESS_exoplanet/codebase/

# config files paths
CONFIG_FP_2MIN=/home6/msaragoc/work_dir/Kepler-TESS_exoplanet/codebase/tess_spoc_ffi/config_cv_train_2min.yaml
CONFIG_FP_FFI=/home6/msaragoc/work_dir/Kepler-TESS_exoplanet/codebase/tess_spoc_ffi/config_cv_train_ffi.yaml

# job script for running the Python application
RUN_SH_SCRIPT=/nobackupp19/msaragoc/work_dir/Kepler-TESS_exoplanet/codebase/tess_spoc_ffi/run_cv_iter_modular.sh
# output directory
OUTPUT_DIR=/home6/msaragoc/work_dir/Kepler-TESS_exoplanet/experiments/tess_spoc_ffi/cv_tess-spoc-ffi_s36-s72_multisector_s56-s69_with2mindata_finetuning_lr10f_3-6-2025_0938
mkdir -p $OUTPUT_DIR

N_CV_ITERS=10  # number of CV folds/iterations
N_MODELS_PER_CV_ITER=10  # number of models to train per CV iteration

# number of GPUs to be used by this job array
N_GPUS_TOTAL=1

# number of total jobs per job in job array
NUM_TOTAL_JOBS=4
# number of jobs run simultaneously
NUM_JOBS_PARALLEL=4

# run CV sh script
$RUN_SH_SCRIPT 0 $PBS_ARRAY_INDEX $CONFIG_FP_2MIN $CONFIG_FP_FFI $OUTPUT_DIR $N_GPUS_TOTAL $N_MODELS_PER_CV_ITER
# run CV sh script with GNU parallel
# seq 0 $((NUM_TOTAL_JOBS - 1)) | parallel -j $NUM_JOBS_PARALLEL "$RUN_SH_SCRIPT {} $PBS_ARRAY_INDEX $CONFIG_FP_2MIN $CONFIG_FP_FFI $OUTPUT_DIR $N_GPUS_TOTAL $N_CV_ITERS $N_GPUS_PER_NODE $N_MODELS_PER_CV_ITER"
