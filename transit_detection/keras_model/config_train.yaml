rank: null
rnd_seed: 42  # random seed used to select the validation fold in each CV iteration
paths:
  # experiment directory; results are saved here
  experiment_root_dir: null
  # YAML file containing a list of CV iterations, where each CV iteration is a dictionary with the TFRecord filepaths
  # for each training, validation, and test set folds combination (i.e., {'train': ['/path/to/train_shard-xxxx', ...],
  # 'val': ['/path/to/val_shard-xxxx', ...], 'test': ['/path/to/test_shard-xxxx']}
  datasets_fps_yaml: null
  # HPO run configuration directory; get configurations from an HPO run; set to null to not use any
  hpo_dir: null

val_from_train: false  # if true, the validation fold is chosen randomly from the training split. The training split must contain more than one fold
generate_csv_pred: true  # generates a prediction ranking for each of the specified datasets

data_fields: # scalar data from TFRecords to add to ranking table
  uid: 'string'
  target_id: 'int_scalar'
  tce_plnt_num: 'int_scalar'

  disposition: 'float_scalar'
  # label: 'float_scalar'

  tce_period: 'float_scalar'
  tce_duration: 'float_scalar'
  tce_time0bk: 'float_scalar'

# set general architecture of the model based on implementations under models.models_keras.py
model_architecture: ExoMiner_TESS_Transit_Detection

config:

  global_branch_settings:
    # universal - conv_block/fc_conv_layer/fc_block
    non_lin_fn: prelu
    kernel_initializer: glorot_uniform
    bias_initializer: zeros
    # conv_block
    weight_initializer: 
    # fc_conv
    num_fc_conv_units: 4
    dropout_rate_fc_conv: 0.0

  flux_window_branch_settings:
    window:
      - flux
      
    n_blocks: 3
    kernel_size: 3 #1d conv
    kernel_stride : 1
    pool_size: 2 #1d max pooling
    pool_stride: 2
    init_conv_filters: 2
    conv_ls_per_block: 3

  diff_img_branch_settings: # null
    imgs:
      - oot_img
      - diff_img
      - snr_img
      - target_img

    n_blocks: 3
    kernel_size: !!python/tuple [3,3] #2d conv
    kernel_stride : !!python/tuple [1,1]
    pool_size: !!python/tuple [2,2] #2d max pooling
    pool_stride: !!python/tuple [2,2] 
    init_conv_filters: 2
    conv_ls_per_block: 3

  connect_segment_settings: # connect branches
    batch_norm: null

  fc_block_settings:
    init_fc_neurons: 32
    num_fc_layers: 2 # could try 1/2 vs 16 units with 2 layers & 4 neurons
    decay_rate: null
    dropout_rate: 0.0

  # dropout_rate: 0.0 #dropout rate for final fc block

  # pool_stride: 1
  # kernel_stride: 1

  optimizer: 'Adam' #null for sgd
  lr: 0.001 # standard for adam? TODO: update?



  # weight_initializer: null
  # force_softmax: false

  # decay_rate: null
  # batch_norm: false
  multi_class: false  # used by compile_model

features_set: # each key-value pair is feature_name: {'dim': feature_dim, 'dtype': feature_dtype}
  #flux windows
  flux: { 'dim': [ 100, ], 'dtype': float }

  # difference image - normalize later?
  oot_img: { 'dim': [33, 33], 'dtype': float}
  diff_img: { 'dim': [33, 33], 'dtype': float}
  snr_img: { 'dim': [33, 33], 'dtype': float}
  target_img: { 'dim': [33, 33], 'dtype': float}


# maps features' names to features names expected by the model
feature_map: null

training:
  data_augmentation: false  # perform online data augmentation
  online_preprocessing_params: null # online data augmentation parameters - not used for transit detection
#  n_models: 2  # number of models in the ensemble
  n_epochs: 300  # TODO: determine number of epochs? # number of epochs used to train each model
  opt_metric: auc_pr  # metric shown in the epoch plots besides loss for the training, validation and test sets
  batch_size: 32
  #  ce_weights: null
  filter_data: null  # deprecated; useless
  category_weights: null  # category weights used in weighted loss; set to null for non-weighted loss

  sample_weights: false  # use sample weights defined in the data set
  shuffle_buffer_size: 1000  # should be larger than size of the training set to allow perfect sampling



label_field_name: label  # name of the label field in the TFRecord that is going to be used as the label

evaluation:
  batch_size: 32
inference:
  batch_size: 32

callbacks:
  early_stopping:
    monitor: val_auc_pr  # val_auc_pr  # which metric used to monitor for early stopping
    min_delta: 0
    patience: 300
    verbose: 1
    mode: max  # maximize/minimize monitored metric in early stopping
    baseline: null
    restore_best_weights: true  # get model from epoch that had the best performance according to monitored metric
  backup_and_restore:
    backup_dir: "/nobackupp27/jochoa4/work_dir/job_runs/train_keras_model_min_snr_20/intermediate_model" # Path to backup directory used for saving intermediate models in case of interruption
    save_freq: epoch
    delete_checkpoint: true # Overwrite previous model @ each save_freq
  model_checkpoint:
    filepath: "/nobackupp27/jochoa4/work_dir/job_runs/train_keras_model_min_snr_20/model_checkpoints/model_epoch_{epoch:03d}.keras"
    save_freq: 'epoch'
    save_weights_only: false
    save_best_only: false
    verbose: 1
  csv_logger:
    filename: "/nobackupp27/jochoa4/work_dir/job_runs/train_keras_model_min_snr_20/training_metrics_log.csv"
    append: true # If training paused


  tensorboard:
    histogram_freq: 1
    write_graph: true
    write_images: false
    update_freq: epoch
    profile_batch: 2
    embeddings_metadata: null
    embeddings_freq: 0

label_map: # maps label to a label id
  0.0: 0
  1.0: 1

labels: [0, 1]

datasets_fps: # TODO: update fps
  # train: /Users/jochoa4/Desktop/test_tfrecord/test_shard_updated_0001-0001
  train: /nobackupp27/jochoa4/work_dir/data/datasets/TESS_exoplanet_dataset_11-12-2024_split_norm_v2_opt/tfrecords/train/opt_norm_train_shard_????-????
  # val: /Users/jochoa4/Desktop/test_tfrecord/test_shard_updated_0001-0013
  val: /nobackupp27/jochoa4/work_dir/data/datasets/TESS_exoplanet_dataset_11-12-2024_split_norm_v2_opt/tfrecords/val/opt_norm_val_shard_????-????
  test: /nobackupp27/jochoa4/work_dir/data/datasets/TESS_exoplanet_dataset_11-12-2024_split_norm_v2_opt/tfrecords/test/opt_norm_test_shard_????-????

datasets:
  - train
  - val
  - test

plot_model: true
write_model_summary: true
verbose_model: 2  # for fit, eval, predict functions
verbose: true  # general

metrics:
  'clf_thr': 0.5  # classification threshold
  'num_thr': 1000  # number of thresholds used to compute some metrics
