rank: null
rnd_seed: 42  # random seed used to select the validation fold in each CV iteration
paths:
  # experiment directory; results are saved here
  experiment_root_dir: null
  # YAML file containing a list of CV iterations, where each CV iteration is a dictionary with the TFRecord filepaths
  # for each training, validation, and test set folds combination (i.e., {'train': ['/path/to/train_shard-xxxx', ...],
  # 'val': ['/path/to/val_shard-xxxx', ...], 'test': ['/path/to/test_shard-xxxx']}
  datasets_fps_yaml: null
  # HPO run configuration directory; get configurations from an HPO run; set to null to not use any
  hpo_dir: null

val_from_train: false  # if true, the validation fold is chosen randomly from the training split. The training split must contain more than one fold
generate_csv_pred: true  # generates a prediction ranking for each of the specified datasets

data_fields: # scalar data from TFRecords to add to ranking table
  uid: 'string'
  target_id: 'int_scalar'
  tce_plnt_num: 'int_scalar'

  disposition: 'float_scalar'
  # label: 'float_scalar'

  tce_period: 'float_scalar'
  tce_duration: 'float_scalar'
  tce_time0bk: 'float_scalar'

# set general architecture of the model based on implementations under models.models_keras.py
model_architecture: ExoMiner_TESS_Transit_Detection

config:
  
  num_fc_conv_units : 1

  flux_window_branch:
    window:
      - flux
    # scalars: null

  diff_img_branch: # null
    imgs:
      - oot_img
      - diff_img
      - snr_img
      - target_img
    # imgs_scalars: null
    # scalars: null

  non_lin_fn: prelu

  dropout_rate_fc_conv: 0.0
  dropout_rate: 0.0 #dropout rate for final fc block

  init_fc_neurons: 512

  num_fc_layers: 1

  pool_stride: 1
  kernel_stride: 1

  optimizer: 'Adam'

  weight_initializer: null
  force_softmax: false

  decay_rate: null
  batch_norm: false
  multi_class: false  # switch to multiclass classification

features_set: # each key-value pair is feature_name: {'dim': feature_dim, 'dtype': feature_dtype}
  #flux windows
  flux: { 'dim': [ 100, ], 'dtype': float }

  # difference image - normalize later?
  oot_img: { 'dim': [33, 33], 'dtype': float}
  diff_img: { 'dim': [33, 33], 'dtype': float}
  snr_img: { 'dim': [33, 33], 'dtype': float}
  target_img: { 'dim': [33, 33], 'dtype': float}


# maps features' names to features names expected by the model
feature_map: null
#  feature_name: feature_name_model

training: null
#   data_augmentation: false  # perform online data augmentation
#   online_preprocessing_params: # online data augmentation parameters
#     'num_bins_global': 301
#     'num_bins_local': 31
#     'num_transit_dur': 5
# #  n_models: 2  # number of models in the ensemble
#   n_epochs: 2  # number of epochs used to train each model
#   #  use_kepler_ce: false
#   opt_metric: auc_pr  # metric shown in the epoch plots besides loss for the training, validation and test sets
#   batch_size: 32
#   #  ce_weights: null
#   filter_data: null  # deprecated; useless
#   category_weights: null  # category weights used in weighted loss; set to null for non-weighted loss
# #    '0': 68.5
# #    '1': 0.5
#   #    PC: 1.0
#   #    AFP: 0.5
#   #    NTP: 1.0
#   # charles was using these weights
# #  PC: 0.0002921
# #  AFP: 0.0002229
# #  NTP: 2.967e-05
# #  T-NTP: 2.967e-05
# #  T-EB: 0.0002229
# #  T-KP: 0.0002921
# #  T-FP: 0.0002229
# #  T-CP: 0.0002921
# #  T-FA: 0.0002229
#   sample_weights: false  # use sample weights defined in the data set
#   shuffle_buffer_size: 42000  # should be larger than size of the training set to allow perfect sampling

# label_field_name: label  # name of the label field in the TFRecord that is going to be used as the label

# evaluation:
#   batch_size: 32
# inference:
#   batch_size: 32

# callbacks:
#   early_stopping:
#     monitor: val_auc_pr  # val_auc_pr  # which metric used to monitor for early stopping
#     min_delta: 0
#     patience: 20
#     verbose: 1
#     mode: max  # maximize/minimize monitored metric in early stopping
#     baseline: null
#     restore_best_weights: true  # get model from epoch that had the best performance according to monitored metric
#   tensorboard:
#     histogram_freq: 1
#     write_graph: true
#     write_images: false
#     update_freq: epoch
#     profile_batch: 2
#     embeddings_metadata: null
#     embeddings_freq: 0

# label_map: null

# datasets:
#   - train
#   - val
#   - test

# plot_model: true
# write_model_summary: true
# verbose_model: 2  # for fit, eval, predict functions
# verbose: true  # general

# metrics:
#   'clf_thr': 0.5  # classification threshold
#   'num_thr': 1000  # number of thresholds used to compute some metrics
