# Conduct HPO run multi-GPU and multi-node; using MPIEXEC to evaluate multiple configurations (one per GPU) at the same
# time.

# initialize conda and activate conda environment
source activate exoplnt_dl_tf2_13
# source activate exoplnt_dl_gh

# set path to codebase root directory
export PYTHONPATH=codebase/

# config file path
CONFIG_FP=$PYTHONPATH/src_hpo/config_hpo.yaml
# job script for running the Python application
RUN_SH_SCRIPT=$PYTHONPATH/src_hpo/run_hpo_worker.sh
# output directory
OUTPUT_DIR=
mkdir -p $OUTPUT_DIR

# create main output job file
JOB_FP=$OUTPUT_DIR/output_job.txt

# copy PBS script
PBS_SCRIPT_FP=$(realpath $0)
cp $PBS_SCRIPT_FP $JOB_FP

# copy codebase git commit hash
COMMIT_HASH=$(git -C $PYTHONPATH rev-parse HEAD)
echo "Git hash commit: $COMMIT_HASH"  >> $JOB_FP

# number of GPUs per node
N_GPUS_TOTAL=4

# number of total jobs per job in job array
NUM_TOTAL_JOBS=4
# number of jobs run simultaneously
NUM_JOBS_PARALLEL=4

# run with GNU parallel
seq 0 $((NUM_TOTAL_JOBS - 1)) | parallel -j $NUM_JOBS_PARALLEL "$RUN_SH_SCRIPT {} $CONFIG_FP $OUTPUT_DIR $N_GPUS_TOTAL"
# seq 0 $((NUM_TOTAL_JOBS - 1)) | parallel -j $NUM_JOBS_PARALLEL -u --sshloginfile $PBS_NODEFILE "$RUN_SH_SCRIPT {} $CONFIG_FP $OUTPUT_DIR $N_GPUS_TOTAL"
