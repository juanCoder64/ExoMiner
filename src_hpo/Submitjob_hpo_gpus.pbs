# Conduct HPO run multi-GPU and multi-node; using MPIEXEC to evaluate multiple configurations (one per GPU) at the same
# time.
#PBS -S /bin/bash
#PBS -N hpo_exoplanet
#PBS -l walltime=72:00:00
# PBS -l select=1:ncpus=36:ngpus=4:mem=360g:model=sky_gpu
#PBS -l select=2:ncpus=36:ngpus=4:mem=360g:model=sky_gpu
# place the chunk wherever it is possible for the requested resources; share resources with other people
#PBS -l place=free:excl
#PBS -q dsg_gpu@pbspl4
#PBS -o /home6/msaragoc/jobs/Kepler-TESS_exoplanet/job_hpo_mgpus.out
#PBS -e /home6/msaragoc/jobs/Kepler-TESS_exoplanet/job_hpo_mgpus.err
#PBS -W group_list=a1509
#PBS -m bea

# config file path
CONFIG_FP=/home6/msaragoc/work_dir/Kepler-TESS_exoplanet/codebase/src_hpo/config_hpo.yaml
# job script for running the Python application
RUN_SH_SCRIPT=/home6/msaragoc/work_dir/Kepler-TESS_exoplanet/codebase/src_hpo/run_hpo_worker.sh
# output directory
OUTPUT_DIR=/home6/msaragoc/work_dir/Kepler-TESS_exoplanet/experiments/tess_paper/hpo_runs/hpo_run_keplerq1q17dr25_11-20-2024_1640
mkdir -p $OUTPUT_DIR

# number of GPUs per node
N_GPUS_TOTAL=4

# number of total jobs per job in job array
NUM_TOTAL_JOBS=8
# number of jobs run simultaneously
NUM_JOBS_PARALLEL=4

# run with GNU parallel
# seq 0 $((NUM_TOTAL_JOBS - 1)) | parallel -j $NUM_JOBS_PARALLEL "$RUN_SH_SCRIPT {} $CONFIG_FP $OUTPUT_DIR $N_GPUS_TOTAL"
seq 0 $((NUM_TOTAL_JOBS - 1)) | parallel -j $NUM_JOBS_PARALLEL -u --sshloginfile $PBS_NODEFILE "$RUN_SH_SCRIPT {} $CONFIG_FP $OUTPUT_DIR $N_GPUS_TOTAL"
