ngpus_per_node: 4  # number of GPUs per node
rank: null

paths:
  experiment_dir: /nobackup/cyates2/experiments/hpo/hpo_exp/hpo_merged_unfolded_7-5-2023
  tfrec_dir: /nobackup/cyates2/data/cvs/updt_diffimg_centr_2-8-2023/tfrecords/norm_data
  prev_run_dir: null  # warm-up from previous HPO run

optimizer: bohb  # types of hyperparameter optimizers available: 'random_search', 'bohb'
# if minimum and maximum budgets are set to the same value, then BOHB becomes BO (Bayesian optimization)
min_budget: 6  # 6  # budget in this case is number of epochs a given configuration is evaluated on
max_budget: 50  # 50
n_iterations: 400
eta: 2  # down-sampling rate, must be greater or equal to 2, hyperband parameter; check [1]
# BOHB and BO parameters
bohb_params:
  top_n_percent: 15
  num_samples: 64
  random_fraction: 0.333
  bandwidth_factor: 3
  min_bandwidth: 1.0e-3
hpo_loss: auc_pr  # metric used to evaluate the performance of a given configuration on the validation set
# number of models trained per configuration evaluated on a given budget
# used to decrease the variability due to random weights initialization
ensemble_n: 3
nic_name: ib0  # 'ib0' (infiniband, use in the HECC cluster), 'lo' (local on Linux systems); 'en0' (local on MacOS)

plot_model: true

config: # fixed parameters
# branches used in ExoMiner
  conv_branches:
    local_unfolded_flux:
      views:
        - unfolded_local_flux_view_fluxnorm
      scalars:
        - tce_num_transits_obs
    global_flux:
      views:
        - global_flux_view_fluxnorm
      scalars: null
    local_flux:
      views:
        - local_flux_view_fluxnorm
#        - local_flux_view_fluxnorm_var
      scalars:
        #        - transit_depth_norm
        - tce_max_mult_ev_norm
        - tce_max_sngle_ev_norm
        - tce_robstat_norm
        - tce_model_chisq_norm
    local_weak_secondary:
      views:
        #        - local_weak_secondary_view_fluxnorm
        - local_weak_secondary_view_selfnorm
      #        - local_weak_secondary_view_max_flux-wks_norm
      scalars:
        - tce_ptemp_stat_norm
        - tce_albedo_stat_norm
        - tce_maxmes_norm
        - tce_prad_norm
    #        - wst_depth_norm
    local_centroid:
      views:
        - local_centr_view_std_noclip
      #        - local_centr_fdl_view_norm
      scalars:
        #        - tce_fwm_stat_norm
        - tce_dikco_msky_norm
        - tce_dikco_msky_err_norm
        #        - tce_dicco_msky_norm
        #        - tce_dicco_msky_err_norm
        #        - mag_norm
#        - mag_cat
        - mag_cat_norm
        - ruwe_norm
#        - tce_ruwe_norm
#    global_centroid:
#      views:
#        #        - global_centr_fdl_view_norm
#        - global_centr_view_std_noclip
#      scalars: null
    local_odd_even:
      views:
        - local_flux_odd_view_fluxnorm
        - local_flux_even_view_fluxnorm
      scalars:
        - odd_se_oot_norm
        - even_se_oot_norm
  scalar_branches:
    stellar:
      - tce_sdens_norm
      - tce_steff_norm
      - tce_smet_norm
      - tce_slogg_norm
      - tce_smass_norm
      - tce_sradius_norm
    dv_tce_fit:
      - boot_fap_norm
      - tce_cap_stat_norm
      - tce_hap_stat_norm
      #          - tce_cap_hap_stat_diff_norm
      #      - tce_rb_tcount0n_norm
#      - tce_prad_norm
      - tce_period_norm
  transformer_branches: null
#      global:
#        - unfolded_global_flux_view_fluxnorm
#      local:
#        - unfolded_local_flux_view_fluxnorm

  #transformer hyperparameters
  #if true, will use transformer branches. Otherwise will use convolutional branches for unfolded flux data
  use_transformer: true
  num_transformer_blocks: 2
  head_size: 256
  num_heads: 4
  ff_dim: 4
  dropout_rate_transformer: 0.03
  #can either be bin_average_pooling, phase_average_pooling, or flat, or time distributed
  # output will be [301/31], [20], and [20 * 301/31] respectively
  # if time_encoding = true, shape of bin_average_pooling and flat increase by a factor of 3
  transformer_output: 'flat'
  #  concat_stats_to: folded-local #none #folded-local, unfolded-local, none
  time_encoding: true
  #  trans_pooling: none #max, avg, none
  #  trans_pool_size: 8
  #  trans_stride_size: 1
  #transformer head
  #num_units_fc_layers: [16, 8]
  global-num_units_transformer_fc_layers: [ 4 ] #[16, 16, 8 , 4] #[ 256, 64, 16, 4 ] #[512, 128, 32, 8 ] #[16, 12, 8] #[128, 32, 8]
  local-num_units_transformer_fc_layers: [ 4 ] #[16, 16, 8 , 4] #[ 64, 32, 16, 4 ] #[512, 128, 32, 8 ] #[16, 12, 8]
  dropout_rate_trans_fc: 0.03
  non_lin_fn: prelu
#  num_loc_conv_blocks: 2
  #  'num_glob_conv_blocks': 5
  #  'init_fc_neurons': 512
  #  'num_fc_layers': 4
  #  'pool_size_loc': 7
  #  'pool_size_glob': 5
  'pool_stride': 1
  #  'conv_ls_per_block': 2
  #  'init_conv_filters': 4
  #  'kernel_size': 5
  'kernel_stride': 1
  'optimizer': 'Adam'
  #  'lr': 1.0e-5
  #  'batch_size': 64
  #  'dropout_rate': 0.0
  #  'dropout_rate_fc_conv': 0.0
  # num_fc_conv_units: 4  # number of neurons in FC layer at the end of the convolutional branch
  weight_initializer: null
  force_softmax: false
  #  use_kepler_ce: false
  decay_rate: null
  batch_norm: false
  multi_class: false  # switch to multiclass classification


features_set: # each key-value pair is feature_name: {'dim': feature_dim, 'dtype': feature_dtype}
  global_flux_view_fluxnorm: { 'dim': [ 301, 1 ], 'dtype': float }
  local_flux_view_fluxnorm: { 'dim': [ 31, 1 ], 'dtype': float }
#  local_flux_view_fluxnorm_var: { 'dim': [ 31, 1 ], 'dtype': float }
  #  transit_depth_norm: { 'dim': [ 1, ], 'dtype': float }
  tce_max_mult_ev_norm: { 'dim': [ 1, ], 'dtype': float }
  tce_max_sngle_ev_norm: { 'dim': [ 1, ], 'dtype': float }
  tce_robstat_norm: { 'dim': [ 1, ], 'dtype': float }
  tce_model_chisq_norm: { 'dim': [ 1, ], 'dtype': float }
  # odd-even flux related features
  local_flux_odd_view_fluxnorm: { 'dim': [ 31, 1 ], 'dtype': float }
  local_flux_even_view_fluxnorm: { 'dim': [ 31, 1 ], 'dtype': float }
  #  tce_bin_oedp_stat_norm: {'dim': [1,], 'dtype': float}
  #  odd_std_oot_bin_norm: {'dim': [1,], 'dtype': float}
  #  even_std_oot_bin_norm: {'dim': [1,], 'dtype': float}
  odd_se_oot_norm: { 'dim': [ 1, ], 'dtype': float }
  even_se_oot_norm: { 'dim': [ 1, ], 'dtype': float }
  # centroid related features
  # global_centr_fdl_view_norm: {'dim': [301, 1], 'dtype': float}
  # local_centr_fdl_view_norm: {'dim': [31, 1], 'dtype': float}
#  global_centr_view_std_noclip: { 'dim': [ 301, 1 ], 'dtype': float }
  local_centr_view_std_noclip: { 'dim': [ 31, 1 ], 'dtype': float }
#  global_centr_view_adjscl_std_noclip: { 'dim': [ 301, 1 ], 'dtype': float }
#  local_centr_view_adjscl_std_noclip: { 'dim': [ 31, 1 ], 'dtype': float }
  #  tce_fwm_stat_norm: { 'dim': [ 1, ], 'dtype': float }
  tce_dikco_msky_norm: { 'dim': [ 1, ], 'dtype': float }
  tce_dikco_msky_err_norm: { 'dim': [ 1, ], 'dtype': float }
#  tce_dikco_msky_adjscl_norm: { 'dim': [ 1, ], 'dtype': float }
#  tce_dikco_msky_err_adjscl_shifted_norm: { 'dim': [ 1, ], 'dtype': float }
  #  mag_norm: { 'dim': [ 1, ], 'dtype': float }
#  mag_cat: { 'dim': [ 1, ], 'dtype': int }
  mag_cat_norm: { 'dim': [ 1, ], 'dtype': float }
  ruwe_norm: { 'dim': [ 1, ], 'dtype': float }
#  tce_ruwe_norm: { 'dim': [ 1, ], 'dtype': float }
  # secondary related features
  # local_weak_secondary_view_fluxnorm: {'dim': [31, 1], 'dtype': float}
  local_weak_secondary_view_selfnorm: { 'dim': [ 31, 1 ], 'dtype': float }
  #  local_weak_secondary_view_max_flux-wks_norm: { 'dim': [ 31, 1 ], 'dtype': float }
  tce_maxmes_norm: { 'dim': [ 1, ], 'dtype': float }
  #  wst_depth_norm: { 'dim': [ 1, ], 'dtype': float }
  tce_albedo_stat_norm: { 'dim': [ 1, ], 'dtype': float }
  tce_ptemp_stat_norm: { 'dim': [ 1, ], 'dtype': float }
  # other diagnostic parameters
  boot_fap_norm: { 'dim': [ 1, ], 'dtype': float }
  tce_cap_stat_norm: { 'dim': [ 1, ], 'dtype': float }
  tce_hap_stat_norm: { 'dim': [ 1, ], 'dtype': float }
  #    tce_cap_hap_stat_diff_norm: {'dim': [1,], 'dtype': float}
  #  tce_rb_tcount0n_norm: { 'dim': [ 1, ], 'dtype': float }
  #    tce_rb_tcount0_norm: { 'dim': [1,], 'dtype': float}
  # stellar parameters
  tce_sdens_norm: { 'dim': [ 1, ], 'dtype': float }
  tce_steff_norm: { 'dim': [ 1, ], 'dtype': float }
  tce_smet_norm: { 'dim': [ 1, ], 'dtype': float }
  tce_slogg_norm: { 'dim': [ 1, ], 'dtype': float }
  tce_smass_norm: { 'dim': [ 1, ], 'dtype': float }
  tce_sradius_norm: { 'dim': [ 1, ], 'dtype': float }
  # tce parameters
  tce_prad_norm: { 'dim': [ 1, ], 'dtype': float }
  tce_period_norm: { 'dim': [ 1, ], 'dtype': float }
  # unfolded time series
  unfolded_local_flux_view_fluxnorm: { 'dim': [ 20, 31 ], 'dtype': float }
  tce_num_transits_obs: { 'dim': [ 1, ], 'dtype': float }
#  uid: { 'dim': [ 1, ], 'dtype': str }
  # difference image
#  diff_img: { 'dim': [ 11, 11, 5 ], 'dtype': float }
#  oot_img: { 'dim': [ 11, 11, 5 ], 'dtype': float }
#  target_subpx_pos: { 'dim': [ 2, 5 ], 'dtype': float }

# maps features' names to features names expected by the model
feature_map: null
#  feature_name: feature_name_model
#  global_centr_view_adjscl_std_noclip: global_centr_view_std_noclip
#  local_centr_view_adjscl_std_noclip: local_centr_view_std_noclip
#  tce_dikco_msky_adjscl_norm: tce_dikco_msky_norm
#  tce_dikco_msky_err_adjscl_shifted_norm: tce_dikco_msky_err_norm

training:
  data_augmentation: false  # perform online data augmentation
  online_preprocessing_params: # online data augmentation parameters
    'num_bins_global': 301
    'num_bins_local': 31
    'num_transit_dur': 5
  use_kepler_ce: false
  opt_metric: auc_pr
  batch_size: 32
  ce_weights: null
  filter_data: null
  category_weights: null
#    PC: 1.0
#    AFP: 1.0
#    NTP: 1.0
  shuffle_buffer_size: 42000  # should be larger than size of the training set to allow perfect sampling

evaluation:
  batch_size: 32
inference:
  batch_size: 32

callbacks: null
#  early_stopping:
#    monitor: val_auc_pr  # which metric to monitor for early stopping
#    min_delta: 0
#    patience: 20
#    verbose: 1
#    mode: max  # maximize/minimize monitored metric in early stopping
#    baseline: null
#    restore_best_weights: true
#  tensorboard:
#    histogram_freq: 1
#    write_graph: true
#    write_images: false
#    update_freq: epoch
#    profile_batch: 2
#    embeddings_metadata: null
#    embeddings_freq: 0

label_map:
  # Kepler
  PC: 1
  AFP: 0  # [1,0,0]
  AFP2: 0 #  [1, 0, 0]
  NTP: 0
  # TESS
#  KP: 1
#  CP: 1
#  PC: 0
#  FP: 0
#  FA: 0
#  EB: 0
#  BEB: 0
#  APC: 0
#  O: 0
  K-PC: 1
  K-AFP: 0
  K-AFP2: 0
  K-NTP: 0
  T-KP: 1
  T-CP: 1
  T-FP: 0
  T-FA: 0
  T-EB: 0
  T-NTP: 0

metrics:
  'clf_thr': 0.5  # classification threshold
  'num_thr': 1000  # number of thresholds used to compute some metrics
#  'top_k_arr': # values of k for computing precision-at-k
#    'train': [ 100, 1000, 1818 ]
#    'val': [ 50, 150, 222 ]
#    'test': [ 50, 150, 251 ]
#  top_k_curve: # values of k for computing precision-at-k curve
#    train: { 'start': 25, 'stop': 1800, 'num': 100,  'endpoint': true, 'dtype': 'int' }
#    val: { 'start': 25, 'stop': 200, 'num': 10,  'endpoint': true, 'dtype': 'int' }
#    test: { 'start': 25, 'stop': 250, 'num': 10,  'endpoint': true, 'dtype': 'int' }

datasets: [ 'train', 'val', 'test' ]  # datasets - same name convention as used for the TFRecords
verbose: true
verbose_model: 2  # 'auto', 0, 1, or 2

configuration_space:
  hyperparameters: # hyper-parameters to be optimized (they will overwrite hyper-parameters set in config
#    num_transformer_blocks:
#      type: categorical
#      parameters: [1, 2, 3]
    lr: # name (should match name expected in model function)
      type: uniform_float  # type of hyper-parameter
      parameters:
        lower: 1.0e-6
        upper: 1.0e-1
        default_value: 1e-2
        log: true
#    optimizer:
#      type: categorical
#      parameters: [ 'Adam', 'SGD' ]
#    sgd_momentum:
#      type: uniform_float
#      parameters:
#        lower: 1.0e-3
#        upper: 0.99
#        default_value: 0.9
#        log: true

    #  batch_size:
    #    type: categorical
    #    parameters: [ 4, 8, 26, 32, 64, 128, 256 ]

    #    weight_initializer:
    #      type: categorical
    #      parameters: [ 'he', 'glorot' ]

    #    batch_norm:
    #      type: categorical
    #      parameters: [ true, false' ]

    #    non_lin_fn:
    #      type: categorical
    #      parameters: [ 'relu', 'prelu' ]

    num_loc_conv_blocks:
      type: uniform_integer
      parameters:
        lower: 1
        upper: 5
        default_value: 2
    num_glob_conv_blocks:
      type: uniform_integer
      parameters:
        lower: 1
        upper: 5
        default_value: 3
    conv_ls_per_block:
      type: uniform_integer
      parameters:
        lower: 1
        upper: 3
        default_value: 1
    init_conv_filters:
      type: uniform_integer
      parameters:
        lower: 2
        upper: 6
        default_value: 4
#    kernel_size:
#      type: uniform_integer
#      parameters:
#        lower: 1
#        upper: 8
#        default_value: 2
    kernel_size_glob:
      type: uniform_integer
      parameters:
        lower: 1
        upper: 8
        default_value: 2
    kernel_size_loc:
      type: uniform_integer
      parameters:
        lower: 1
        upper: 8
        default_value: 2
#    kernel_stride:
#      type: uniform_integer
#      parameters:
#        lower: 1
#        upper: 2
#        default_value: 1

    pool_size_loc:
      type: categorical
      parameters: [2, 4, 8, 16]
    pool_size_glob:
      type: categorical
      parameters: [2, 4, 8, 16]
#    pool_stride:
#      type: uniform_integer
#      parameters:
#        lower: 1
#        upper: 2
#        default_value: 1

    dropout_rate_fc_conv:
      type: uniform_float
      parameters:
        lower: 1.0e-3
        upper: 2.0e-1
        default_value: 0.2
        log: true
    num_fc_conv_units:
      type: uniform_integer
      parameters:
        lower: 1
        upper: 4
        default_value: 4
    num_fc_layers:
      type: uniform_integer
      parameters:
        lower: 1
        upper: 4
        default_value: 2
    init_fc_neurons:
      type: categorical
      parameters: [ 32, 64, 128, 256, 512 ]
    dropout_rate:
      type: uniform_float
      parameters:
        lower: 1.0e-3
        upper: 2.0e-1
        default_value: 0.2
        log: true
#    num_transformer_blocks:
#      type: categorical
#      parameters: [1, 2, 3]

  conditions: # {}  # replace by empty dictionary `{}` if not using any condition
    sgd_momentum:
      type: equal
      child: sgd_momentum
      parent: optimizer
      value: SGD
